{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 13s - loss: 1.8365 - accuracy: 0.3387 - 13s/epoch - 17ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.6912 - accuracy: 0.3967 - 9s/epoch - 11ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 1.6251 - accuracy: 0.4214 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 1.5760 - accuracy: 0.4403 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 1.5470 - accuracy: 0.4505 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.5157 - accuracy: 0.4590 - 7s/epoch - 10ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 7s - loss: 1.4928 - accuracy: 0.4674 - 7s/epoch - 9ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 1.4780 - accuracy: 0.4753 - 7s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 7s - loss: 1.4501 - accuracy: 0.4855 - 7s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 7s - loss: 1.4420 - accuracy: 0.4871 - 7s/epoch - 9ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 8s - loss: 1.4226 - accuracy: 0.4953 - 8s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 7s - loss: 1.4124 - accuracy: 0.4989 - 7s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 1.3931 - accuracy: 0.5059 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 1.3813 - accuracy: 0.5096 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 1.3693 - accuracy: 0.5155 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 1.3561 - accuracy: 0.5179 - 7s/epoch - 9ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 1.3430 - accuracy: 0.5223 - 7s/epoch - 9ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 1.3283 - accuracy: 0.5306 - 7s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 1.3158 - accuracy: 0.5336 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 1.3109 - accuracy: 0.5356 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 1.2999 - accuracy: 0.5381 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 1.2871 - accuracy: 0.5440 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 1.2737 - accuracy: 0.5473 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 1.2660 - accuracy: 0.5532 - 7s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 1.2537 - accuracy: 0.5541 - 6s/epoch - 8ms/step\n",
      "> layers=2, train=0.579, test=0.499\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.2884 - accuracy: 0.5444 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 1.1610 - accuracy: 0.5877 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.1284 - accuracy: 0.5994 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.1107 - accuracy: 0.6071 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.0948 - accuracy: 0.6122 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0817 - accuracy: 0.6166 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 1.0701 - accuracy: 0.6207 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0614 - accuracy: 0.6236 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0521 - accuracy: 0.6263 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.0464 - accuracy: 0.6282 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.0398 - accuracy: 0.6309 - 4s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.0352 - accuracy: 0.6323 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 1.0273 - accuracy: 0.6333 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.0219 - accuracy: 0.6379 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 1.0157 - accuracy: 0.6409 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 1.0104 - accuracy: 0.6410 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 1.0073 - accuracy: 0.6419 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 1.0006 - accuracy: 0.6454 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9960 - accuracy: 0.6467 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 0.9946 - accuracy: 0.6462 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 0.9895 - accuracy: 0.6483 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.9865 - accuracy: 0.6480 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 0.9832 - accuracy: 0.6517 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9794 - accuracy: 0.6524 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9758 - accuracy: 0.6539 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 789,258\n",
      "_________________________________________________________________\n",
      "> layers=3, train=0.663, test=0.528\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.1816 - accuracy: 0.5859 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0387 - accuracy: 0.6335 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0005 - accuracy: 0.6451 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9775 - accuracy: 0.6528 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9601 - accuracy: 0.6595 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9481 - accuracy: 0.6628 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9385 - accuracy: 0.6661 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9298 - accuracy: 0.6691 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.9211 - accuracy: 0.6720 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9136 - accuracy: 0.6754 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9063 - accuracy: 0.6766 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9000 - accuracy: 0.6800 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8926 - accuracy: 0.6824 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8877 - accuracy: 0.6840 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 0.8827 - accuracy: 0.6852 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 0.8753 - accuracy: 0.6895 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8719 - accuracy: 0.6872 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.8666 - accuracy: 0.6910 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8614 - accuracy: 0.6949 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8556 - accuracy: 0.6958 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8516 - accuracy: 0.6968 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8448 - accuracy: 0.6988 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8434 - accuracy: 0.7007 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8381 - accuracy: 0.7022 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 0.8329 - accuracy: 0.7033 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 920,842\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 855,050\n",
      "_________________________________________________________________\n",
      "> layers=4, train=0.717, test=0.517\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.0303 - accuracy: 0.6411 - 5s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.8940 - accuracy: 0.6813 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.8696 - accuracy: 0.6907 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.8529 - accuracy: 0.6969 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.8417 - accuracy: 0.6993 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.8359 - accuracy: 0.7035 - 4s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8247 - accuracy: 0.7050 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.8177 - accuracy: 0.7087 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8082 - accuracy: 0.7129 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8085 - accuracy: 0.7110 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.8038 - accuracy: 0.7123 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8009 - accuracy: 0.7143 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.7933 - accuracy: 0.7172 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.7934 - accuracy: 0.7175 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.7916 - accuracy: 0.7179 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.7813 - accuracy: 0.7200 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.7811 - accuracy: 0.7209 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.7804 - accuracy: 0.7203 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 0.7784 - accuracy: 0.7241 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.7713 - accuracy: 0.7241 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.7752 - accuracy: 0.7221 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.7655 - accuracy: 0.7261 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.7706 - accuracy: 0.7247 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.7609 - accuracy: 0.7293 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.7622 - accuracy: 0.7279 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,634\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 920,842\n",
      "_________________________________________________________________\n",
      "> layers=5, train=0.732, test=0.507\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 0.9458 - accuracy: 0.6812 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.8226 - accuracy: 0.7095 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.7957 - accuracy: 0.7193 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.7990 - accuracy: 0.7148 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.7823 - accuracy: 0.7226 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.7794 - accuracy: 0.7228 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.7787 - accuracy: 0.7232 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.7688 - accuracy: 0.7278 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.7675 - accuracy: 0.7280 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.7640 - accuracy: 0.7274 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.7613 - accuracy: 0.7290 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.7552 - accuracy: 0.7302 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.7656 - accuracy: 0.7289 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.7537 - accuracy: 0.7312 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.7565 - accuracy: 0.7298 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.7606 - accuracy: 0.7302 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.7513 - accuracy: 0.7337 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.7503 - accuracy: 0.7338 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.7499 - accuracy: 0.7342 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.7515 - accuracy: 0.7329 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.7463 - accuracy: 0.7346 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.7478 - accuracy: 0.7345 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.7468 - accuracy: 0.7341 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.7476 - accuracy: 0.7350 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.7501 - accuracy: 0.7334 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,052,426\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 986,634\n",
      "_________________________________________________________________\n",
      "> layers=6, train=0.742, test=0.508\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0905 - accuracy: 0.6884 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.8096 - accuracy: 0.7165 - 4s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.7987 - accuracy: 0.7203 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.7900 - accuracy: 0.7223 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.7779 - accuracy: 0.7258 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 0.7766 - accuracy: 0.7268 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.7766 - accuracy: 0.7247 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.7748 - accuracy: 0.7283 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.7731 - accuracy: 0.7285 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.7783 - accuracy: 0.7255 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.7757 - accuracy: 0.7274 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.7730 - accuracy: 0.7281 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.7714 - accuracy: 0.7297 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.7794 - accuracy: 0.7252 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.7727 - accuracy: 0.7280 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.7600 - accuracy: 0.7309 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.7630 - accuracy: 0.7312 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.7600 - accuracy: 0.7315 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.7509 - accuracy: 0.7353 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.7646 - accuracy: 0.7292 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.7627 - accuracy: 0.7309 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.7622 - accuracy: 0.7308 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.7642 - accuracy: 0.7304 - 6s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 0.7491 - accuracy: 0.7357 - 6s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.7641 - accuracy: 0.7307 - 6s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,218\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,052,426\n",
      "_________________________________________________________________\n",
      "> layers=7, train=0.714, test=0.495\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 2.4977 - accuracy: 0.5862 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.1640 - accuracy: 0.6371 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.1493 - accuracy: 0.6343 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 1.1390 - accuracy: 0.6382 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.1602 - accuracy: 0.6324 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.1366 - accuracy: 0.6430 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.1126 - accuracy: 0.6483 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.4485 - accuracy: 0.5197 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.3611 - accuracy: 0.5459 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 1.2968 - accuracy: 0.5787 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.1849 - accuracy: 0.6188 - 4s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 1.1815 - accuracy: 0.6249 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.1405 - accuracy: 0.6326 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 1.1702 - accuracy: 0.6275 - 4s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 1.1541 - accuracy: 0.6327 - 4s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 1.1554 - accuracy: 0.6331 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 1.1786 - accuracy: 0.6289 - 4s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 1.1465 - accuracy: 0.6373 - 4s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 1.1842 - accuracy: 0.6303 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 1.1567 - accuracy: 0.6316 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 1.1523 - accuracy: 0.6358 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 1.1635 - accuracy: 0.6349 - 4s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 1.1451 - accuracy: 0.6382 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.1611 - accuracy: 0.6320 - 4s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 1.1601 - accuracy: 0.6334 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,010\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,118,218\n",
      "_________________________________________________________________\n",
      "> layers=8, train=0.624, test=0.466\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 2.4848 - accuracy: 0.6037 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0951 - accuracy: 0.6456 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0556 - accuracy: 0.6544 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0418 - accuracy: 0.6574 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.0581 - accuracy: 0.6533 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.0695 - accuracy: 0.6536 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.0255 - accuracy: 0.6618 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 1.0331 - accuracy: 0.6607 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.0297 - accuracy: 0.6615 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.0372 - accuracy: 0.6584 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 1.0173 - accuracy: 0.6620 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 1.0171 - accuracy: 0.6639 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.0352 - accuracy: 0.6615 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 1.0297 - accuracy: 0.6613 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 1.0260 - accuracy: 0.6632 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 1.0069 - accuracy: 0.6612 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 1.0392 - accuracy: 0.6583 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 1.0219 - accuracy: 0.6625 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 1.0360 - accuracy: 0.6597 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 1.0406 - accuracy: 0.6586 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 1.0293 - accuracy: 0.6620 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 1.0191 - accuracy: 0.6636 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 1.0124 - accuracy: 0.6651 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 1.0214 - accuracy: 0.6646 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 1.0283 - accuracy: 0.6625 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,249,802\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,184,010\n",
      "_________________________________________________________________\n",
      "> layers=9, train=0.604, test=0.441\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 2.1537 - accuracy: 0.5825 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.2162 - accuracy: 0.6201 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 1.1685 - accuracy: 0.6286 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.1489 - accuracy: 0.6342 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.0998 - accuracy: 0.6462 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.1172 - accuracy: 0.6428 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.0936 - accuracy: 0.6481 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 1.1081 - accuracy: 0.6454 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 1.0804 - accuracy: 0.6506 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.0755 - accuracy: 0.6522 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 1.0905 - accuracy: 0.6486 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 1.0879 - accuracy: 0.6486 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.0705 - accuracy: 0.6506 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 1.0773 - accuracy: 0.6536 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 1.0822 - accuracy: 0.6513 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 1.0592 - accuracy: 0.6516 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 1.0573 - accuracy: 0.6529 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 1.0781 - accuracy: 0.6506 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 1.1199 - accuracy: 0.6376 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 1.0798 - accuracy: 0.6476 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 1.0637 - accuracy: 0.6508 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 1.0660 - accuracy: 0.6540 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 1.0926 - accuracy: 0.6456 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 1.0689 - accuracy: 0.6509 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 1.0790 - accuracy: 0.6502 - 6s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315,594\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,249,802\n",
      "_________________________________________________________________\n",
      "> layers=10, train=0.664, test=0.477\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.1950 - accuracy: 0.6462 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9634 - accuracy: 0.6659 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9538 - accuracy: 0.6688 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9356 - accuracy: 0.6753 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9268 - accuracy: 0.6772 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9231 - accuracy: 0.6789 - 4s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9187 - accuracy: 0.6782 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9204 - accuracy: 0.6787 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.9152 - accuracy: 0.6799 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9160 - accuracy: 0.6792 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9147 - accuracy: 0.6798 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9108 - accuracy: 0.6827 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9107 - accuracy: 0.6810 - 4s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.9119 - accuracy: 0.6786 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9094 - accuracy: 0.6805 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9069 - accuracy: 0.6812 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9086 - accuracy: 0.6805 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9069 - accuracy: 0.6833 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9039 - accuracy: 0.6832 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9055 - accuracy: 0.6826 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9043 - accuracy: 0.6826 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9027 - accuracy: 0.6828 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9031 - accuracy: 0.6836 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9033 - accuracy: 0.6841 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9037 - accuracy: 0.6831 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,381,386\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,315,594\n",
      "_________________________________________________________________\n",
      "> layers=11, train=0.689, test=0.487\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.1914 - accuracy: 0.6470 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.9436 - accuracy: 0.6728 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9253 - accuracy: 0.6767 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9154 - accuracy: 0.6797 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9153 - accuracy: 0.6782 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9124 - accuracy: 0.6809 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9102 - accuracy: 0.6796 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9064 - accuracy: 0.6818 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9078 - accuracy: 0.6823 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9038 - accuracy: 0.6836 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9020 - accuracy: 0.6825 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8968 - accuracy: 0.6845 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8939 - accuracy: 0.6846 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8977 - accuracy: 0.6840 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8949 - accuracy: 0.6852 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8987 - accuracy: 0.6828 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8917 - accuracy: 0.6872 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8954 - accuracy: 0.6858 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8880 - accuracy: 0.6864 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8895 - accuracy: 0.6855 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8878 - accuracy: 0.6877 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8852 - accuracy: 0.6870 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8843 - accuracy: 0.6874 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8861 - accuracy: 0.6865 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8870 - accuracy: 0.6872 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,447,178\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,381,386\n",
      "_________________________________________________________________\n",
      "> layers=12, train=0.692, test=0.488\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.1814 - accuracy: 0.6499 - 5s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9379 - accuracy: 0.6750 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9133 - accuracy: 0.6790 - 4s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9054 - accuracy: 0.6821 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9005 - accuracy: 0.6822 - 6s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8967 - accuracy: 0.6836 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8920 - accuracy: 0.6862 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.8906 - accuracy: 0.6860 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8892 - accuracy: 0.6870 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8852 - accuracy: 0.6887 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8873 - accuracy: 0.6869 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8833 - accuracy: 0.6877 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8873 - accuracy: 0.6860 - 4s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8835 - accuracy: 0.6884 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8807 - accuracy: 0.6902 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8832 - accuracy: 0.6888 - 4s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8779 - accuracy: 0.6900 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.8809 - accuracy: 0.6895 - 4s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8785 - accuracy: 0.6894 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8779 - accuracy: 0.6890 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8763 - accuracy: 0.6892 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8775 - accuracy: 0.6894 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.8753 - accuracy: 0.6899 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8758 - accuracy: 0.6897 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8718 - accuracy: 0.6921 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,512,970\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,447,178\n",
      "_________________________________________________________________\n",
      "> layers=13, train=0.696, test=0.488\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.1154 - accuracy: 0.6547 - 7s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9362 - accuracy: 0.6756 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9127 - accuracy: 0.6804 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9037 - accuracy: 0.6842 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8983 - accuracy: 0.6862 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8950 - accuracy: 0.6852 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8909 - accuracy: 0.6870 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8880 - accuracy: 0.6878 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8845 - accuracy: 0.6879 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.8825 - accuracy: 0.6882 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.8844 - accuracy: 0.6878 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.8836 - accuracy: 0.6888 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8798 - accuracy: 0.6893 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8808 - accuracy: 0.6895 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8822 - accuracy: 0.6897 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.8787 - accuracy: 0.6889 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.8822 - accuracy: 0.6879 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8789 - accuracy: 0.6906 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.8791 - accuracy: 0.6886 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.8819 - accuracy: 0.6886 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8789 - accuracy: 0.6894 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8748 - accuracy: 0.6898 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8772 - accuracy: 0.6910 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8761 - accuracy: 0.6894 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8754 - accuracy: 0.6917 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,578,762\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,512,970\n",
      "_________________________________________________________________\n",
      "> layers=14, train=0.683, test=0.481\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.1614 - accuracy: 0.6517 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9576 - accuracy: 0.6716 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9340 - accuracy: 0.6764 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9224 - accuracy: 0.6790 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9200 - accuracy: 0.6801 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9101 - accuracy: 0.6834 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9031 - accuracy: 0.6841 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8996 - accuracy: 0.6830 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8982 - accuracy: 0.6854 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.8965 - accuracy: 0.6848 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8952 - accuracy: 0.6844 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.8922 - accuracy: 0.6853 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8915 - accuracy: 0.6859 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8924 - accuracy: 0.6848 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8952 - accuracy: 0.6847 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.8920 - accuracy: 0.6871 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.8877 - accuracy: 0.6862 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8919 - accuracy: 0.6863 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.8854 - accuracy: 0.6870 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.8864 - accuracy: 0.6873 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.8853 - accuracy: 0.6863 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8829 - accuracy: 0.6889 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.8898 - accuracy: 0.6872 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8876 - accuracy: 0.6878 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8820 - accuracy: 0.6892 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,554\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,578,762\n",
      "_________________________________________________________________\n",
      "> layers=15, train=0.696, test=0.487\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.6266 - accuracy: 0.6285 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.0089 - accuracy: 0.6631 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9956 - accuracy: 0.6666 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9500 - accuracy: 0.6742 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9479 - accuracy: 0.6730 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9336 - accuracy: 0.6782 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9338 - accuracy: 0.6765 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9256 - accuracy: 0.6808 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9421 - accuracy: 0.6740 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9341 - accuracy: 0.6786 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9302 - accuracy: 0.6781 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9278 - accuracy: 0.6782 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9245 - accuracy: 0.6789 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9232 - accuracy: 0.6783 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9286 - accuracy: 0.6781 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9273 - accuracy: 0.6784 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9154 - accuracy: 0.6808 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9239 - accuracy: 0.6804 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9247 - accuracy: 0.6783 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9255 - accuracy: 0.6779 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9222 - accuracy: 0.6800 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9223 - accuracy: 0.6787 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9202 - accuracy: 0.6798 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9200 - accuracy: 0.6804 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9168 - accuracy: 0.6809 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,346\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,644,554\n",
      "_________________________________________________________________\n",
      "> layers=16, train=0.665, test=0.470\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.1306 - accuracy: 0.6579 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9222 - accuracy: 0.6778 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9098 - accuracy: 0.6815 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9100 - accuracy: 0.6832 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9008 - accuracy: 0.6839 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8973 - accuracy: 0.6853 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.8973 - accuracy: 0.6856 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8993 - accuracy: 0.6842 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8966 - accuracy: 0.6838 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.8913 - accuracy: 0.6860 - 6s/epoch - 8ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8916 - accuracy: 0.6855 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.8926 - accuracy: 0.6862 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8923 - accuracy: 0.6854 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8882 - accuracy: 0.6870 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8895 - accuracy: 0.6862 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.8867 - accuracy: 0.6874 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.8857 - accuracy: 0.6880 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8884 - accuracy: 0.6859 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.8870 - accuracy: 0.6880 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.8873 - accuracy: 0.6873 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8864 - accuracy: 0.6868 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8851 - accuracy: 0.6895 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.8874 - accuracy: 0.6890 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8845 - accuracy: 0.6881 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8837 - accuracy: 0.6896 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,776,138\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,710,346\n",
      "_________________________________________________________________\n",
      "> layers=17, train=0.697, test=0.485\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.2943 - accuracy: 0.6443 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9267 - accuracy: 0.6801 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9105 - accuracy: 0.6818 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9157 - accuracy: 0.6837 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9070 - accuracy: 0.6856 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9062 - accuracy: 0.6841 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9009 - accuracy: 0.6869 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9044 - accuracy: 0.6830 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9030 - accuracy: 0.6834 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9007 - accuracy: 0.6844 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8974 - accuracy: 0.6863 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9045 - accuracy: 0.6823 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9027 - accuracy: 0.6862 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8977 - accuracy: 0.6858 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8984 - accuracy: 0.6866 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.8953 - accuracy: 0.6864 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.8947 - accuracy: 0.6859 - 6s/epoch - 8ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9002 - accuracy: 0.6855 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 0.8958 - accuracy: 0.6864 - 7s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.8955 - accuracy: 0.6865 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8930 - accuracy: 0.6868 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8960 - accuracy: 0.6851 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.8934 - accuracy: 0.6868 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8912 - accuracy: 0.6875 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8953 - accuracy: 0.6866 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,841,930\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,776,138\n",
      "_________________________________________________________________\n",
      "> layers=18, train=0.673, test=0.473\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.4929 - accuracy: 0.6488 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9536 - accuracy: 0.6739 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9337 - accuracy: 0.6790 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9296 - accuracy: 0.6793 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9348 - accuracy: 0.6789 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9167 - accuracy: 0.6822 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9196 - accuracy: 0.6812 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9241 - accuracy: 0.6825 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9174 - accuracy: 0.6823 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9121 - accuracy: 0.6841 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9241 - accuracy: 0.6819 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9110 - accuracy: 0.6856 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9136 - accuracy: 0.6842 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9197 - accuracy: 0.6826 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9151 - accuracy: 0.6833 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9099 - accuracy: 0.6832 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9218 - accuracy: 0.6818 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9091 - accuracy: 0.6849 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9073 - accuracy: 0.6850 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9098 - accuracy: 0.6854 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9161 - accuracy: 0.6839 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9170 - accuracy: 0.6838 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9059 - accuracy: 0.6856 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 0.9115 - accuracy: 0.6848 - 6s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 7s - loss: 0.9046 - accuracy: 0.6847 - 7s/epoch - 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,722\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,841,930\n",
      "_________________________________________________________________\n",
      "> layers=19, train=0.689, test=0.480\n",
      "Epoch 1/25\n",
      "782/782 - 8s - loss: 1.3405 - accuracy: 0.6406 - 8s/epoch - 10ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.0480 - accuracy: 0.6596 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 1.0171 - accuracy: 0.6621 - 7s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9942 - accuracy: 0.6652 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9905 - accuracy: 0.6694 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.0008 - accuracy: 0.6670 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9909 - accuracy: 0.6690 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9895 - accuracy: 0.6679 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9975 - accuracy: 0.6673 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9956 - accuracy: 0.6684 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 1.0001 - accuracy: 0.6689 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9743 - accuracy: 0.6717 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9753 - accuracy: 0.6723 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9922 - accuracy: 0.6672 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9717 - accuracy: 0.6738 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9792 - accuracy: 0.6694 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 0.9759 - accuracy: 0.6723 - 7s/epoch - 9ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9603 - accuracy: 0.6739 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9795 - accuracy: 0.6716 - 6s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9841 - accuracy: 0.6678 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9935 - accuracy: 0.6686 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9918 - accuracy: 0.6702 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9836 - accuracy: 0.6720 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9662 - accuracy: 0.6716 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9855 - accuracy: 0.6716 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,973,514\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,907,722\n",
      "_________________________________________________________________\n",
      "> layers=20, train=0.668, test=0.467\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "testX = testX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=32*32*3, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "# evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    " _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    " _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    " return train_acc, test_acc\n",
    "# Add one new layer and re-train only the new layer\n",
    "# Add one new layer and re-train only the new layer while keeping the last layer frozen\n",
    "def add_layer(model, trainX, trainy):\n",
    "    output_layer = model.layers[-1]\n",
    "    model.pop()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Set the last layer to be non-trainable\n",
    "    output_layer.trainable = False\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    # Compile the model before training\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the new layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add layers and evaluate the updated model\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    add_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 8s - loss: 2.1258 - accuracy: 0.2362 - 8s/epoch - 10ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.9756 - accuracy: 0.3097 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.9152 - accuracy: 0.3344 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.8780 - accuracy: 0.3490 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.8514 - accuracy: 0.3580 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.8301 - accuracy: 0.3671 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.8121 - accuracy: 0.3732 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 1.7968 - accuracy: 0.3792 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.7831 - accuracy: 0.3852 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.7704 - accuracy: 0.3893 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 1.7590 - accuracy: 0.3921 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 1.7484 - accuracy: 0.3949 - 4s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 1.7384 - accuracy: 0.4000 - 4s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 1.7287 - accuracy: 0.4027 - 4s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 1.7198 - accuracy: 0.4048 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 1.7116 - accuracy: 0.4090 - 4s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 1.7037 - accuracy: 0.4111 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 1.6959 - accuracy: 0.4160 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 1.6888 - accuracy: 0.4159 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 1.6816 - accuracy: 0.4194 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 1.6746 - accuracy: 0.4207 - 4s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 1.6680 - accuracy: 0.4238 - 4s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 1.6616 - accuracy: 0.4252 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.6556 - accuracy: 0.4269 - 4s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 1.6495 - accuracy: 0.4295 - 5s/epoch - 6ms/step\n",
      "> layers=2, train=0.431, test=0.422\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 2.2304 - accuracy: 0.1750 - 4s/epoch - 4ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 2.0227 - accuracy: 0.2635 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.9398 - accuracy: 0.2982 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.8917 - accuracy: 0.3185 - 3s/epoch - 3ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.8590 - accuracy: 0.3336 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 1.8347 - accuracy: 0.3439 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 1.8152 - accuracy: 0.3510 - 3s/epoch - 3ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.7998 - accuracy: 0.3574 - 3s/epoch - 3ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.7868 - accuracy: 0.3631 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.7757 - accuracy: 0.3678 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 1.7660 - accuracy: 0.3715 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.7574 - accuracy: 0.3755 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 1.7497 - accuracy: 0.3796 - 3s/epoch - 3ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.7426 - accuracy: 0.3815 - 3s/epoch - 3ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 1.7360 - accuracy: 0.3849 - 3s/epoch - 3ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 1.7300 - accuracy: 0.3862 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 1.7246 - accuracy: 0.3889 - 3s/epoch - 3ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 1.7194 - accuracy: 0.3915 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 1.7145 - accuracy: 0.3926 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 1.7101 - accuracy: 0.3942 - 3s/epoch - 3ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 1.7057 - accuracy: 0.3964 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 1.7017 - accuracy: 0.3977 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 1.6978 - accuracy: 0.3989 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 3s - loss: 1.6941 - accuracy: 0.3999 - 3s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 1.6906 - accuracy: 0.4019 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 789,258\n",
      "_________________________________________________________________\n",
      "> layers=3, train=0.402, test=0.396\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 2.1652 - accuracy: 0.1904 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.9405 - accuracy: 0.2855 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.8657 - accuracy: 0.3215 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.8232 - accuracy: 0.3428 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.7949 - accuracy: 0.3546 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.7746 - accuracy: 0.3627 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.7587 - accuracy: 0.3712 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.7458 - accuracy: 0.3767 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.7349 - accuracy: 0.3803 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.7260 - accuracy: 0.3853 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 1.7177 - accuracy: 0.3880 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.7107 - accuracy: 0.3911 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 1.7042 - accuracy: 0.3933 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.6987 - accuracy: 0.3964 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 1.6934 - accuracy: 0.3984 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 1.6889 - accuracy: 0.3998 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 1.6844 - accuracy: 0.4019 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 1.6805 - accuracy: 0.4031 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 1.6768 - accuracy: 0.4054 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 1.6733 - accuracy: 0.4068 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 1.6702 - accuracy: 0.4071 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 1.6671 - accuracy: 0.4086 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 1.6643 - accuracy: 0.4101 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 3s - loss: 1.6614 - accuracy: 0.4110 - 3s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 1.6589 - accuracy: 0.4118 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 920,842\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 855,050\n",
      "_________________________________________________________________\n",
      "> layers=4, train=0.413, test=0.408\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 2.0700 - accuracy: 0.2497 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.8494 - accuracy: 0.3299 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.7931 - accuracy: 0.3546 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.7635 - accuracy: 0.3641 - 3s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.7442 - accuracy: 0.3725 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 1.7303 - accuracy: 0.3785 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 1.7194 - accuracy: 0.3829 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.7105 - accuracy: 0.3859 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.7033 - accuracy: 0.3887 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.6971 - accuracy: 0.3925 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 1.6915 - accuracy: 0.3942 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.6866 - accuracy: 0.3963 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 1.6823 - accuracy: 0.3988 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.6783 - accuracy: 0.4000 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 1.6748 - accuracy: 0.4028 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 1.6715 - accuracy: 0.4035 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 1.6685 - accuracy: 0.4043 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 1.6656 - accuracy: 0.4050 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 1.6627 - accuracy: 0.4074 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 1.6603 - accuracy: 0.4082 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 1.6579 - accuracy: 0.4097 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 1.6557 - accuracy: 0.4107 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 1.6535 - accuracy: 0.4120 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.6513 - accuracy: 0.4128 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 1.6493 - accuracy: 0.4124 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,634\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 920,842\n",
      "_________________________________________________________________\n",
      "> layers=5, train=0.413, test=0.406\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 2.0670 - accuracy: 0.2480 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.8386 - accuracy: 0.3338 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.7824 - accuracy: 0.3603 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.7509 - accuracy: 0.3727 - 3s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.7304 - accuracy: 0.3817 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.7154 - accuracy: 0.3870 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.7040 - accuracy: 0.3922 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.6951 - accuracy: 0.3954 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.6878 - accuracy: 0.3983 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.6817 - accuracy: 0.4002 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.6766 - accuracy: 0.4032 - 4s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.6721 - accuracy: 0.4044 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 1.6681 - accuracy: 0.4066 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 1.6646 - accuracy: 0.4086 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 1.6615 - accuracy: 0.4098 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 1.6585 - accuracy: 0.4109 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 1.6560 - accuracy: 0.4117 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 1.6533 - accuracy: 0.4132 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 1.6511 - accuracy: 0.4144 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 1.6490 - accuracy: 0.4148 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 1.6469 - accuracy: 0.4154 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 1.6450 - accuracy: 0.4151 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 1.6433 - accuracy: 0.4168 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.6413 - accuracy: 0.4175 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 1.6398 - accuracy: 0.4183 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,052,426\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 986,634\n",
      "_________________________________________________________________\n",
      "> layers=6, train=0.419, test=0.415\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 2.0221 - accuracy: 0.2798 - 9s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.7833 - accuracy: 0.3566 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 1.7391 - accuracy: 0.3732 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 8s - loss: 1.7178 - accuracy: 0.3811 - 8s/epoch - 10ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.7039 - accuracy: 0.3872 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 1.6939 - accuracy: 0.3902 - 8s/epoch - 10ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.6865 - accuracy: 0.3951 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 10s - loss: 1.6804 - accuracy: 0.3967 - 10s/epoch - 13ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.6754 - accuracy: 0.3995 - 9s/epoch - 11ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.6710 - accuracy: 0.4019 - 9s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.6671 - accuracy: 0.4039 - 9s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.6637 - accuracy: 0.4053 - 10s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.6605 - accuracy: 0.4070 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 8s - loss: 1.6577 - accuracy: 0.4071 - 8s/epoch - 10ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.6551 - accuracy: 0.4084 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.6527 - accuracy: 0.4103 - 8s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.6504 - accuracy: 0.4102 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 8s - loss: 1.6483 - accuracy: 0.4115 - 8s/epoch - 10ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 8s - loss: 1.6463 - accuracy: 0.4134 - 8s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 1.6445 - accuracy: 0.4134 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.6427 - accuracy: 0.4137 - 8s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 7s - loss: 1.6411 - accuracy: 0.4151 - 7s/epoch - 9ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 7s - loss: 1.6395 - accuracy: 0.4151 - 7s/epoch - 9ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 1.6378 - accuracy: 0.4172 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.6363 - accuracy: 0.4164 - 8s/epoch - 10ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,218\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,052,426\n",
      "_________________________________________________________________\n",
      "> layers=7, train=0.418, test=0.415\n",
      "Epoch 1/25\n",
      "782/782 - 10s - loss: 1.9921 - accuracy: 0.2663 - 10s/epoch - 13ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.7815 - accuracy: 0.3527 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 1.7380 - accuracy: 0.3731 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 8s - loss: 1.7169 - accuracy: 0.3821 - 8s/epoch - 10ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.7035 - accuracy: 0.3878 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 1.6936 - accuracy: 0.3929 - 8s/epoch - 10ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.6858 - accuracy: 0.3961 - 8s/epoch - 11ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 1.6793 - accuracy: 0.3990 - 8s/epoch - 10ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 1.6741 - accuracy: 0.4003 - 8s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 8s - loss: 1.6693 - accuracy: 0.4021 - 8s/epoch - 10ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 8s - loss: 1.6653 - accuracy: 0.4039 - 8s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.6615 - accuracy: 0.4061 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.6584 - accuracy: 0.4070 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 8s - loss: 1.6556 - accuracy: 0.4083 - 8s/epoch - 10ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.6530 - accuracy: 0.4097 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.6502 - accuracy: 0.4118 - 9s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.6482 - accuracy: 0.4120 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 8s - loss: 1.6461 - accuracy: 0.4127 - 8s/epoch - 10ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 8s - loss: 1.6441 - accuracy: 0.4134 - 8s/epoch - 10ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 8s - loss: 1.6424 - accuracy: 0.4141 - 8s/epoch - 10ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.6405 - accuracy: 0.4153 - 8s/epoch - 10ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 8s - loss: 1.6389 - accuracy: 0.4160 - 8s/epoch - 10ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 8s - loss: 1.6372 - accuracy: 0.4163 - 8s/epoch - 10ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 8s - loss: 1.6358 - accuracy: 0.4177 - 8s/epoch - 10ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.6344 - accuracy: 0.4173 - 8s/epoch - 11ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,010\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,118,218\n",
      "_________________________________________________________________\n",
      "> layers=8, train=0.418, test=0.413\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 2.0348 - accuracy: 0.2651 - 9s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.7797 - accuracy: 0.3544 - 9s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 1.7301 - accuracy: 0.3771 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 8s - loss: 1.7065 - accuracy: 0.3872 - 8s/epoch - 10ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.6922 - accuracy: 0.3921 - 8s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.6823 - accuracy: 0.3968 - 9s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.6751 - accuracy: 0.3991 - 8s/epoch - 11ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 1.6693 - accuracy: 0.4015 - 8s/epoch - 11ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 1.6648 - accuracy: 0.4037 - 8s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 8s - loss: 1.6607 - accuracy: 0.4049 - 8s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.6573 - accuracy: 0.4062 - 9s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.6538 - accuracy: 0.4080 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.6510 - accuracy: 0.4083 - 8s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 8s - loss: 1.6486 - accuracy: 0.4090 - 8s/epoch - 10ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.6463 - accuracy: 0.4104 - 9s/epoch - 11ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.6442 - accuracy: 0.4118 - 8s/epoch - 10ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.6422 - accuracy: 0.4118 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 8s - loss: 1.6404 - accuracy: 0.4131 - 8s/epoch - 10ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.6388 - accuracy: 0.4137 - 9s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 8s - loss: 1.6372 - accuracy: 0.4138 - 8s/epoch - 11ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.6357 - accuracy: 0.4144 - 9s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.6342 - accuracy: 0.4163 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.6326 - accuracy: 0.4160 - 9s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 8s - loss: 1.6315 - accuracy: 0.4167 - 8s/epoch - 11ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.6302 - accuracy: 0.4164 - 8s/epoch - 10ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,249,802\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,184,010\n",
      "_________________________________________________________________\n",
      "> layers=9, train=0.417, test=0.411\n",
      "Epoch 1/25\n",
      "782/782 - 13s - loss: 2.0416 - accuracy: 0.2530 - 13s/epoch - 16ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 11s - loss: 1.7818 - accuracy: 0.3570 - 11s/epoch - 14ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 11s - loss: 1.7274 - accuracy: 0.3771 - 11s/epoch - 14ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.7039 - accuracy: 0.3860 - 9s/epoch - 11ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.6906 - accuracy: 0.3913 - 9s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.6821 - accuracy: 0.3944 - 9s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 10s - loss: 1.6754 - accuracy: 0.3983 - 10s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.6702 - accuracy: 0.3995 - 9s/epoch - 11ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 10s - loss: 1.6661 - accuracy: 0.4021 - 10s/epoch - 13ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.6624 - accuracy: 0.4032 - 9s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 12s - loss: 1.6592 - accuracy: 0.4041 - 12s/epoch - 15ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.6563 - accuracy: 0.4056 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 10s - loss: 1.6536 - accuracy: 0.4084 - 10s/epoch - 13ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.6510 - accuracy: 0.4074 - 9s/epoch - 11ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.6489 - accuracy: 0.4090 - 9s/epoch - 11ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.6467 - accuracy: 0.4091 - 8s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.6448 - accuracy: 0.4113 - 9s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 10s - loss: 1.6430 - accuracy: 0.4113 - 10s/epoch - 13ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.6414 - accuracy: 0.4116 - 9s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 10s - loss: 1.6397 - accuracy: 0.4122 - 10s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.6382 - accuracy: 0.4136 - 10s/epoch - 13ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.6368 - accuracy: 0.4140 - 11s/epoch - 15ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 11s - loss: 1.6353 - accuracy: 0.4146 - 11s/epoch - 14ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 10s - loss: 1.6340 - accuracy: 0.4153 - 10s/epoch - 13ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 10s - loss: 1.6328 - accuracy: 0.4164 - 10s/epoch - 13ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315,594\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,249,802\n",
      "_________________________________________________________________\n",
      "> layers=10, train=0.417, test=0.412\n",
      "Epoch 1/25\n",
      "782/782 - 11s - loss: 1.9943 - accuracy: 0.2681 - 11s/epoch - 14ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.7801 - accuracy: 0.3493 - 10s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.7274 - accuracy: 0.3762 - 9s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.7015 - accuracy: 0.3871 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.6868 - accuracy: 0.3947 - 9s/epoch - 12ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.6768 - accuracy: 0.3981 - 10s/epoch - 12ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.6700 - accuracy: 0.4011 - 9s/epoch - 11ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.6649 - accuracy: 0.4025 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.6606 - accuracy: 0.4042 - 9s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.6574 - accuracy: 0.4055 - 9s/epoch - 12ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.6547 - accuracy: 0.4062 - 9s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 9s - loss: 1.6522 - accuracy: 0.4082 - 9s/epoch - 11ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.6501 - accuracy: 0.4083 - 9s/epoch - 12ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.6480 - accuracy: 0.4094 - 9s/epoch - 12ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.6461 - accuracy: 0.4101 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.6444 - accuracy: 0.4111 - 9s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.6430 - accuracy: 0.4115 - 9s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.6414 - accuracy: 0.4129 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.6398 - accuracy: 0.4140 - 9s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 10s - loss: 1.6387 - accuracy: 0.4135 - 10s/epoch - 13ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 11s - loss: 1.6372 - accuracy: 0.4137 - 11s/epoch - 14ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.6361 - accuracy: 0.4141 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 11s - loss: 1.6352 - accuracy: 0.4138 - 11s/epoch - 14ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 11s - loss: 1.6341 - accuracy: 0.4157 - 11s/epoch - 14ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 11s - loss: 1.6331 - accuracy: 0.4163 - 11s/epoch - 14ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,381,386\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,315,594\n",
      "_________________________________________________________________\n",
      "> layers=11, train=0.415, test=0.411\n",
      "Epoch 1/25\n",
      "782/782 - 11s - loss: 2.0024 - accuracy: 0.2723 - 11s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.7637 - accuracy: 0.3626 - 9s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.7190 - accuracy: 0.3842 - 9s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 10s - loss: 1.6983 - accuracy: 0.3924 - 10s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 10s - loss: 1.6859 - accuracy: 0.3981 - 10s/epoch - 13ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.6776 - accuracy: 0.4008 - 10s/epoch - 13ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 11s - loss: 1.6715 - accuracy: 0.4030 - 11s/epoch - 14ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 11s - loss: 1.6668 - accuracy: 0.4051 - 11s/epoch - 15ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.6630 - accuracy: 0.4073 - 11s/epoch - 15ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.6598 - accuracy: 0.4082 - 11s/epoch - 14ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 10s - loss: 1.6570 - accuracy: 0.4082 - 10s/epoch - 13ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.6545 - accuracy: 0.4099 - 10s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.6523 - accuracy: 0.4100 - 9s/epoch - 12ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.6503 - accuracy: 0.4107 - 9s/epoch - 12ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 10s - loss: 1.6485 - accuracy: 0.4121 - 10s/epoch - 13ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.6469 - accuracy: 0.4129 - 9s/epoch - 12ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.6450 - accuracy: 0.4128 - 9s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.6436 - accuracy: 0.4137 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 10s - loss: 1.6420 - accuracy: 0.4130 - 10s/epoch - 13ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.6409 - accuracy: 0.4152 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.6393 - accuracy: 0.4147 - 9s/epoch - 12ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 10s - loss: 1.6382 - accuracy: 0.4163 - 10s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 10s - loss: 1.6369 - accuracy: 0.4160 - 10s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.6357 - accuracy: 0.4174 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.6349 - accuracy: 0.4164 - 9s/epoch - 12ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,447,178\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,381,386\n",
      "_________________________________________________________________\n",
      "> layers=12, train=0.417, test=0.408\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.9214 - accuracy: 0.3034 - 12s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.7501 - accuracy: 0.3718 - 10s/epoch - 13ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.7152 - accuracy: 0.3846 - 10s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.6979 - accuracy: 0.3920 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 10s - loss: 1.6863 - accuracy: 0.3961 - 10s/epoch - 12ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.6785 - accuracy: 0.3983 - 10s/epoch - 13ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 10s - loss: 1.6727 - accuracy: 0.4015 - 10s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.6683 - accuracy: 0.4017 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 10s - loss: 1.6642 - accuracy: 0.4043 - 10s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 10s - loss: 1.6610 - accuracy: 0.4038 - 10s/epoch - 13ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.6580 - accuracy: 0.4066 - 9s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 9s - loss: 1.6559 - accuracy: 0.4077 - 9s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 10s - loss: 1.6537 - accuracy: 0.4072 - 10s/epoch - 13ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 10s - loss: 1.6514 - accuracy: 0.4093 - 10s/epoch - 13ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.6494 - accuracy: 0.4092 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.6480 - accuracy: 0.4099 - 9s/epoch - 12ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 10s - loss: 1.6462 - accuracy: 0.4117 - 10s/epoch - 13ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.6448 - accuracy: 0.4117 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.6432 - accuracy: 0.4125 - 9s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.6417 - accuracy: 0.4125 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.6407 - accuracy: 0.4121 - 10s/epoch - 13ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.6396 - accuracy: 0.4134 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 10s - loss: 1.6384 - accuracy: 0.4127 - 10s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 10s - loss: 1.6373 - accuracy: 0.4128 - 10s/epoch - 13ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 10s - loss: 1.6360 - accuracy: 0.4146 - 10s/epoch - 12ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,512,970\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,447,178\n",
      "_________________________________________________________________\n",
      "> layers=13, train=0.414, test=0.408\n",
      "Epoch 1/25\n",
      "782/782 - 11s - loss: 1.9837 - accuracy: 0.2796 - 11s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.7503 - accuracy: 0.3682 - 10s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.7066 - accuracy: 0.3862 - 10s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 10s - loss: 1.6878 - accuracy: 0.3937 - 10s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 10s - loss: 1.6774 - accuracy: 0.3985 - 10s/epoch - 13ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.6707 - accuracy: 0.4017 - 10s/epoch - 13ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 10s - loss: 1.6659 - accuracy: 0.4027 - 10s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 10s - loss: 1.6619 - accuracy: 0.4058 - 10s/epoch - 13ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 10s - loss: 1.6593 - accuracy: 0.4059 - 10s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 10s - loss: 1.6568 - accuracy: 0.4077 - 10s/epoch - 12ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 10s - loss: 1.6546 - accuracy: 0.4076 - 10s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.6527 - accuracy: 0.4088 - 10s/epoch - 13ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 10s - loss: 1.6509 - accuracy: 0.4098 - 10s/epoch - 13ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 11s - loss: 1.6496 - accuracy: 0.4104 - 11s/epoch - 15ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.6480 - accuracy: 0.4106 - 11s/epoch - 15ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.6468 - accuracy: 0.4099 - 11s/epoch - 14ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 11s - loss: 1.6455 - accuracy: 0.4117 - 11s/epoch - 14ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 10s - loss: 1.6441 - accuracy: 0.4125 - 10s/epoch - 13ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 10s - loss: 1.6432 - accuracy: 0.4119 - 10s/epoch - 13ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 10s - loss: 1.6418 - accuracy: 0.4131 - 10s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.6411 - accuracy: 0.4131 - 10s/epoch - 12ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 10s - loss: 1.6398 - accuracy: 0.4135 - 10s/epoch - 13ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 10s - loss: 1.6392 - accuracy: 0.4138 - 10s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 10s - loss: 1.6380 - accuracy: 0.4141 - 10s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 10s - loss: 1.6375 - accuracy: 0.4141 - 10s/epoch - 12ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,578,762\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,512,970\n",
      "_________________________________________________________________\n",
      "> layers=14, train=0.414, test=0.412\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.9955 - accuracy: 0.2819 - 12s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.7471 - accuracy: 0.3751 - 10s/epoch - 13ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.7021 - accuracy: 0.3920 - 10s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.6833 - accuracy: 0.3967 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.6732 - accuracy: 0.3989 - 9s/epoch - 12ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.6666 - accuracy: 0.4007 - 10s/epoch - 13ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.6619 - accuracy: 0.4036 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.6585 - accuracy: 0.4047 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.6555 - accuracy: 0.4057 - 9s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 10s - loss: 1.6533 - accuracy: 0.4055 - 10s/epoch - 13ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.6510 - accuracy: 0.4077 - 9s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 9s - loss: 1.6492 - accuracy: 0.4072 - 9s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.6475 - accuracy: 0.4085 - 9s/epoch - 12ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 10s - loss: 1.6464 - accuracy: 0.4082 - 10s/epoch - 12ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.6452 - accuracy: 0.4091 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.6440 - accuracy: 0.4105 - 9s/epoch - 12ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 10s - loss: 1.6427 - accuracy: 0.4096 - 10s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 10s - loss: 1.6419 - accuracy: 0.4122 - 10s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.6408 - accuracy: 0.4114 - 9s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.6401 - accuracy: 0.4116 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.6391 - accuracy: 0.4123 - 10s/epoch - 13ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.6383 - accuracy: 0.4125 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.6377 - accuracy: 0.4139 - 9s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.6372 - accuracy: 0.4134 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 10s - loss: 1.6363 - accuracy: 0.4139 - 10s/epoch - 13ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,554\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,578,762\n",
      "_________________________________________________________________\n",
      "> layers=15, train=0.414, test=0.406\n",
      "Epoch 1/25\n",
      "782/782 - 13s - loss: 2.0115 - accuracy: 0.2725 - 13s/epoch - 16ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 12s - loss: 1.7543 - accuracy: 0.3716 - 12s/epoch - 15ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 11s - loss: 1.7102 - accuracy: 0.3876 - 11s/epoch - 14ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 11s - loss: 1.6912 - accuracy: 0.3935 - 11s/epoch - 14ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 11s - loss: 1.6806 - accuracy: 0.3977 - 11s/epoch - 14ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 11s - loss: 1.6738 - accuracy: 0.4006 - 11s/epoch - 14ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 11s - loss: 1.6687 - accuracy: 0.4025 - 11s/epoch - 14ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 12s - loss: 1.6652 - accuracy: 0.4034 - 12s/epoch - 15ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.6622 - accuracy: 0.4043 - 11s/epoch - 14ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.6594 - accuracy: 0.4062 - 11s/epoch - 14ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 11s - loss: 1.6570 - accuracy: 0.4076 - 11s/epoch - 15ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.6553 - accuracy: 0.4082 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 11s - loss: 1.6536 - accuracy: 0.4076 - 11s/epoch - 13ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 12s - loss: 1.6519 - accuracy: 0.4084 - 12s/epoch - 15ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.6504 - accuracy: 0.4092 - 11s/epoch - 14ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.6489 - accuracy: 0.4104 - 11s/epoch - 14ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 11s - loss: 1.6477 - accuracy: 0.4096 - 11s/epoch - 14ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 13s - loss: 1.6466 - accuracy: 0.4106 - 13s/epoch - 17ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 13s - loss: 1.6454 - accuracy: 0.4102 - 13s/epoch - 17ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 13s - loss: 1.6443 - accuracy: 0.4110 - 13s/epoch - 17ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 12s - loss: 1.6436 - accuracy: 0.4114 - 12s/epoch - 16ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 12s - loss: 1.6425 - accuracy: 0.4123 - 12s/epoch - 15ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 12s - loss: 1.6418 - accuracy: 0.4131 - 12s/epoch - 15ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 11s - loss: 1.6409 - accuracy: 0.4138 - 11s/epoch - 15ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 12s - loss: 1.6402 - accuracy: 0.4135 - 12s/epoch - 15ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,346\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,644,554\n",
      "_________________________________________________________________\n",
      "> layers=16, train=0.414, test=0.410\n",
      "Epoch 1/25\n",
      "782/782 - 14s - loss: 1.9882 - accuracy: 0.2953 - 14s/epoch - 17ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 11s - loss: 1.7432 - accuracy: 0.3794 - 11s/epoch - 15ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 11s - loss: 1.7018 - accuracy: 0.3908 - 11s/epoch - 14ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 11s - loss: 1.6845 - accuracy: 0.3963 - 11s/epoch - 14ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 12s - loss: 1.6750 - accuracy: 0.3986 - 12s/epoch - 15ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 11s - loss: 1.6687 - accuracy: 0.4019 - 11s/epoch - 14ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 11s - loss: 1.6643 - accuracy: 0.4025 - 11s/epoch - 13ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 11s - loss: 1.6608 - accuracy: 0.4046 - 11s/epoch - 14ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.6582 - accuracy: 0.4061 - 11s/epoch - 14ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.6558 - accuracy: 0.4072 - 11s/epoch - 14ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 11s - loss: 1.6537 - accuracy: 0.4068 - 11s/epoch - 14ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.6521 - accuracy: 0.4089 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 10s - loss: 1.6507 - accuracy: 0.4084 - 10s/epoch - 13ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 10s - loss: 1.6492 - accuracy: 0.4107 - 10s/epoch - 13ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.6478 - accuracy: 0.4102 - 11s/epoch - 14ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.6468 - accuracy: 0.4101 - 11s/epoch - 13ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 11s - loss: 1.6459 - accuracy: 0.4109 - 11s/epoch - 14ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 11s - loss: 1.6447 - accuracy: 0.4111 - 11s/epoch - 14ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 11s - loss: 1.6438 - accuracy: 0.4130 - 11s/epoch - 14ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 11s - loss: 1.6428 - accuracy: 0.4127 - 11s/epoch - 14ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 11s - loss: 1.6419 - accuracy: 0.4134 - 11s/epoch - 14ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.6412 - accuracy: 0.4129 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 11s - loss: 1.6403 - accuracy: 0.4137 - 11s/epoch - 14ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 11s - loss: 1.6399 - accuracy: 0.4133 - 11s/epoch - 14ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 11s - loss: 1.6392 - accuracy: 0.4138 - 11s/epoch - 15ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,776,138\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,710,346\n",
      "_________________________________________________________________\n",
      "> layers=17, train=0.415, test=0.408\n",
      "Epoch 1/25\n",
      "782/782 - 13s - loss: 1.9996 - accuracy: 0.2862 - 13s/epoch - 17ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 11s - loss: 1.7451 - accuracy: 0.3834 - 11s/epoch - 14ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 11s - loss: 1.6992 - accuracy: 0.3939 - 11s/epoch - 14ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 12s - loss: 1.6820 - accuracy: 0.3988 - 12s/epoch - 15ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 11s - loss: 1.6732 - accuracy: 0.4011 - 11s/epoch - 14ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 11s - loss: 1.6674 - accuracy: 0.4022 - 11s/epoch - 14ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 12s - loss: 1.6636 - accuracy: 0.4029 - 12s/epoch - 15ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 11s - loss: 1.6606 - accuracy: 0.4037 - 11s/epoch - 14ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.6583 - accuracy: 0.4049 - 11s/epoch - 14ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.6562 - accuracy: 0.4048 - 11s/epoch - 15ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 12s - loss: 1.6547 - accuracy: 0.4053 - 12s/epoch - 15ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.6530 - accuracy: 0.4057 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 11s - loss: 1.6519 - accuracy: 0.4054 - 11s/epoch - 14ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 12s - loss: 1.6507 - accuracy: 0.4071 - 12s/epoch - 15ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.6495 - accuracy: 0.4062 - 11s/epoch - 14ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 12s - loss: 1.6485 - accuracy: 0.4078 - 12s/epoch - 15ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 13s - loss: 1.6477 - accuracy: 0.4073 - 13s/epoch - 17ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 13s - loss: 1.6469 - accuracy: 0.4089 - 13s/epoch - 16ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 13s - loss: 1.6456 - accuracy: 0.4094 - 13s/epoch - 16ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 13s - loss: 1.6451 - accuracy: 0.4082 - 13s/epoch - 17ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 11s - loss: 1.6442 - accuracy: 0.4095 - 11s/epoch - 14ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.6436 - accuracy: 0.4093 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 12s - loss: 1.6428 - accuracy: 0.4100 - 12s/epoch - 15ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 11s - loss: 1.6422 - accuracy: 0.4085 - 11s/epoch - 14ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 11s - loss: 1.6414 - accuracy: 0.4108 - 11s/epoch - 14ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,841,930\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,776,138\n",
      "_________________________________________________________________\n",
      "> layers=18, train=0.410, test=0.406\n",
      "Epoch 1/25\n",
      "782/782 - 14s - loss: 2.0206 - accuracy: 0.2538 - 14s/epoch - 18ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 13s - loss: 1.7552 - accuracy: 0.3609 - 13s/epoch - 17ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 13s - loss: 1.7149 - accuracy: 0.3816 - 13s/epoch - 16ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 13s - loss: 1.6972 - accuracy: 0.3892 - 13s/epoch - 17ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 13s - loss: 1.6870 - accuracy: 0.3936 - 13s/epoch - 16ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 13s - loss: 1.6797 - accuracy: 0.3967 - 13s/epoch - 16ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 13s - loss: 1.6742 - accuracy: 0.4000 - 13s/epoch - 17ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 13s - loss: 1.6699 - accuracy: 0.4010 - 13s/epoch - 16ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 12s - loss: 1.6663 - accuracy: 0.4025 - 12s/epoch - 16ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 14s - loss: 1.6636 - accuracy: 0.4034 - 14s/epoch - 18ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 13s - loss: 1.6611 - accuracy: 0.4046 - 13s/epoch - 16ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 13s - loss: 1.6590 - accuracy: 0.4050 - 13s/epoch - 17ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 13s - loss: 1.6574 - accuracy: 0.4065 - 13s/epoch - 16ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 13s - loss: 1.6559 - accuracy: 0.4068 - 13s/epoch - 16ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 13s - loss: 1.6543 - accuracy: 0.4080 - 13s/epoch - 17ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 13s - loss: 1.6530 - accuracy: 0.4089 - 13s/epoch - 16ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 13s - loss: 1.6519 - accuracy: 0.4086 - 13s/epoch - 17ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 14s - loss: 1.6508 - accuracy: 0.4086 - 14s/epoch - 18ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 13s - loss: 1.6499 - accuracy: 0.4091 - 13s/epoch - 17ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 13s - loss: 1.6490 - accuracy: 0.4093 - 13s/epoch - 16ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 13s - loss: 1.6481 - accuracy: 0.4106 - 13s/epoch - 17ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 13s - loss: 1.6474 - accuracy: 0.4105 - 13s/epoch - 16ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 15s - loss: 1.6463 - accuracy: 0.4104 - 15s/epoch - 20ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 13s - loss: 1.6457 - accuracy: 0.4099 - 13s/epoch - 16ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 13s - loss: 1.6450 - accuracy: 0.4100 - 13s/epoch - 16ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,722\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,841,930\n",
      "_________________________________________________________________\n",
      "> layers=19, train=0.411, test=0.405\n",
      "Epoch 1/25\n",
      "782/782 - 15s - loss: 1.9524 - accuracy: 0.3213 - 15s/epoch - 19ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 13s - loss: 1.7348 - accuracy: 0.3792 - 13s/epoch - 17ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 13s - loss: 1.7041 - accuracy: 0.3894 - 13s/epoch - 16ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 13s - loss: 1.6898 - accuracy: 0.3947 - 13s/epoch - 17ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 13s - loss: 1.6810 - accuracy: 0.3975 - 13s/epoch - 16ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 12s - loss: 1.6756 - accuracy: 0.3996 - 12s/epoch - 16ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 13s - loss: 1.6717 - accuracy: 0.4008 - 13s/epoch - 17ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 12s - loss: 1.6682 - accuracy: 0.4017 - 12s/epoch - 16ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 14s - loss: 1.6656 - accuracy: 0.4023 - 14s/epoch - 18ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 15s - loss: 1.6636 - accuracy: 0.4042 - 15s/epoch - 20ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 14s - loss: 1.6617 - accuracy: 0.4040 - 14s/epoch - 18ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 15s - loss: 1.6600 - accuracy: 0.4055 - 15s/epoch - 19ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 13s - loss: 1.6585 - accuracy: 0.4058 - 13s/epoch - 17ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 13s - loss: 1.6571 - accuracy: 0.4062 - 13s/epoch - 16ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 13s - loss: 1.6557 - accuracy: 0.4065 - 13s/epoch - 17ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 12s - loss: 1.6545 - accuracy: 0.4081 - 12s/epoch - 16ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 13s - loss: 1.6533 - accuracy: 0.4072 - 13s/epoch - 16ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 13s - loss: 1.6520 - accuracy: 0.4083 - 13s/epoch - 17ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 13s - loss: 1.6510 - accuracy: 0.4092 - 13s/epoch - 16ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 13s - loss: 1.6504 - accuracy: 0.4099 - 13s/epoch - 17ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 13s - loss: 1.6493 - accuracy: 0.4082 - 13s/epoch - 17ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 13s - loss: 1.6486 - accuracy: 0.4107 - 13s/epoch - 16ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 13s - loss: 1.6476 - accuracy: 0.4108 - 13s/epoch - 17ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 13s - loss: 1.6468 - accuracy: 0.4107 - 13s/epoch - 16ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 13s - loss: 1.6463 - accuracy: 0.4109 - 13s/epoch - 17ms/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,973,514\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,907,722\n",
      "_________________________________________________________________\n",
      "> layers=20, train=0.411, test=0.407\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "testX = testX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=32*32*3, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = SGD(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "# evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    " _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    " _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    " return train_acc, test_acc\n",
    "# Add one new layer and re-train only the new layer\n",
    "# Add one new layer and re-train only the new layer while keeping the last layer frozen\n",
    "def add_layer(model, trainX, trainy):\n",
    "    output_layer = model.layers[-1]\n",
    "    model.pop()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Set the last layer to be non-trainable\n",
    "    output_layer.trainable = False\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    # Compile the model before training\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the new layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add layers and evaluate the updated model\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    add_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 8s - loss: 2.5335 - accuracy: 0.2261 - 8s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.9460 - accuracy: 0.2792 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 1.9240 - accuracy: 0.2830 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 1.9234 - accuracy: 0.2771 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 1.9151 - accuracy: 0.2752 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.9136 - accuracy: 0.2720 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.9165 - accuracy: 0.2670 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 1.9107 - accuracy: 0.2744 - 7s/epoch - 9ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 7s - loss: 1.9196 - accuracy: 0.2690 - 7s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 7s - loss: 1.9184 - accuracy: 0.2676 - 7s/epoch - 9ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 7s - loss: 1.9563 - accuracy: 0.2411 - 7s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.9217 - accuracy: 0.2639 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.9169 - accuracy: 0.2644 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 1.9117 - accuracy: 0.2667 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.9024 - accuracy: 0.2760 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.9017 - accuracy: 0.2748 - 8s/epoch - 10ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.9066 - accuracy: 0.2700 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 1.9009 - accuracy: 0.2740 - 7s/epoch - 10ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 1.9002 - accuracy: 0.2734 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 1.9015 - accuracy: 0.2721 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.9019 - accuracy: 0.2733 - 8s/epoch - 10ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 8s - loss: 1.9019 - accuracy: 0.2700 - 8s/epoch - 10ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 8s - loss: 1.9099 - accuracy: 0.2667 - 8s/epoch - 10ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 8s - loss: 1.8914 - accuracy: 0.2757 - 8s/epoch - 10ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.8956 - accuracy: 0.2753 - 8s/epoch - 10ms/step\n",
      "> layers=2, train=0.291, test=0.276\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.8934 - accuracy: 0.2979 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.8215 - accuracy: 0.3087 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 1.8184 - accuracy: 0.3085 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 1.8161 - accuracy: 0.3087 - 6s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 1.8145 - accuracy: 0.3124 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.8140 - accuracy: 0.3107 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.8134 - accuracy: 0.3098 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 1.8126 - accuracy: 0.3117 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.8122 - accuracy: 0.3119 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.8117 - accuracy: 0.3129 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 1.8115 - accuracy: 0.3135 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 1.8119 - accuracy: 0.3135 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.8099 - accuracy: 0.3128 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 1.8093 - accuracy: 0.3135 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 1.8097 - accuracy: 0.3132 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 1.8093 - accuracy: 0.3145 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 1.8091 - accuracy: 0.3146 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 1.8085 - accuracy: 0.3132 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 1.8085 - accuracy: 0.3143 - 6s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 1.8090 - accuracy: 0.3137 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 1.8081 - accuracy: 0.3141 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 1.8085 - accuracy: 0.3137 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 1.8070 - accuracy: 0.3142 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 1.8074 - accuracy: 0.3128 - 6s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 1.8068 - accuracy: 0.3145 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 789,258\n",
      "_________________________________________________________________\n",
      "> layers=3, train=0.316, test=0.301\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.8735 - accuracy: 0.2966 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 1.8367 - accuracy: 0.3011 - 7s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 1.8256 - accuracy: 0.3067 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 1.8264 - accuracy: 0.3066 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 1.8233 - accuracy: 0.3084 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.8216 - accuracy: 0.3095 - 7s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 1.8170 - accuracy: 0.3111 - 6s/epoch - 8ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 1.8138 - accuracy: 0.3116 - 7s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 1.8138 - accuracy: 0.3123 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 1.8125 - accuracy: 0.3124 - 6s/epoch - 8ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 1.8124 - accuracy: 0.3130 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 1.8113 - accuracy: 0.3130 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 1.8113 - accuracy: 0.3125 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 1.8099 - accuracy: 0.3129 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 1.8098 - accuracy: 0.3129 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 1.8090 - accuracy: 0.3141 - 7s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 1.8085 - accuracy: 0.3139 - 6s/epoch - 8ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 1.8074 - accuracy: 0.3159 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 1.8058 - accuracy: 0.3156 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 1.8070 - accuracy: 0.3156 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 1.8070 - accuracy: 0.3139 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 1.8057 - accuracy: 0.3157 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 1.8059 - accuracy: 0.3165 - 6s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 1.8053 - accuracy: 0.3167 - 6s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 1.8064 - accuracy: 0.3166 - 6s/epoch - 8ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 920,842\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 855,050\n",
      "_________________________________________________________________\n",
      "> layers=4, train=0.315, test=0.297\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.8494 - accuracy: 0.3015 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.8148 - accuracy: 0.3124 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 1.8109 - accuracy: 0.3115 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 1.8103 - accuracy: 0.3135 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 1.8073 - accuracy: 0.3144 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.8072 - accuracy: 0.3141 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.8077 - accuracy: 0.3120 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 1.8059 - accuracy: 0.3151 - 7s/epoch - 9ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 7s - loss: 1.8054 - accuracy: 0.3148 - 7s/epoch - 9ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 7s - loss: 1.8055 - accuracy: 0.3153 - 7s/epoch - 9ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 7s - loss: 1.8045 - accuracy: 0.3144 - 7s/epoch - 9ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.8041 - accuracy: 0.3156 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 7s - loss: 1.8028 - accuracy: 0.3159 - 7s/epoch - 9ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 1.8032 - accuracy: 0.3154 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.8027 - accuracy: 0.3158 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.8023 - accuracy: 0.3174 - 9s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.8020 - accuracy: 0.3173 - 9s/epoch - 11ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.8017 - accuracy: 0.3174 - 9s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8025 - accuracy: 0.3153 - 9s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 8s - loss: 1.8020 - accuracy: 0.3173 - 8s/epoch - 10ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.8014 - accuracy: 0.3180 - 8s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 7s - loss: 1.8013 - accuracy: 0.3180 - 7s/epoch - 9ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 7s - loss: 1.8001 - accuracy: 0.3168 - 7s/epoch - 9ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 1.8010 - accuracy: 0.3173 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 7s - loss: 1.8005 - accuracy: 0.3189 - 7s/epoch - 9ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,634\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 920,842\n",
      "_________________________________________________________________\n",
      "> layers=5, train=0.321, test=0.303\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.8436 - accuracy: 0.3024 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 1.8177 - accuracy: 0.3094 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 1.8141 - accuracy: 0.3110 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 8s - loss: 1.8145 - accuracy: 0.3116 - 8s/epoch - 10ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.8098 - accuracy: 0.3152 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.8100 - accuracy: 0.3143 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.8099 - accuracy: 0.3145 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 1.8066 - accuracy: 0.3141 - 8s/epoch - 10ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 1.8057 - accuracy: 0.3166 - 8s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 7s - loss: 1.8055 - accuracy: 0.3160 - 7s/epoch - 9ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 7s - loss: 1.8040 - accuracy: 0.3156 - 7s/epoch - 9ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.8059 - accuracy: 0.3156 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.8046 - accuracy: 0.3139 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 1.8046 - accuracy: 0.3155 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 7s - loss: 1.8031 - accuracy: 0.3168 - 7s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 1.8029 - accuracy: 0.3160 - 7s/epoch - 9ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.8021 - accuracy: 0.3155 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 1.8033 - accuracy: 0.3175 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 1.8019 - accuracy: 0.3170 - 7s/epoch - 10ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 8s - loss: 1.8017 - accuracy: 0.3165 - 8s/epoch - 10ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.8019 - accuracy: 0.3171 - 9s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.8017 - accuracy: 0.3162 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.8022 - accuracy: 0.3176 - 9s/epoch - 11ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.8020 - accuracy: 0.3197 - 9s/epoch - 11ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.8001 - accuracy: 0.3184 - 9s/epoch - 12ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,052,426\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 986,634\n",
      "_________________________________________________________________\n",
      "> layers=6, train=0.321, test=0.299\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.8583 - accuracy: 0.3011 - 12s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.8167 - accuracy: 0.3086 - 10s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.8143 - accuracy: 0.3113 - 9s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.8138 - accuracy: 0.3128 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.8089 - accuracy: 0.3123 - 8s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.8088 - accuracy: 0.3140 - 9s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.8096 - accuracy: 0.3133 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 1.8071 - accuracy: 0.3151 - 8s/epoch - 10ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 1.8058 - accuracy: 0.3161 - 8s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8061 - accuracy: 0.3144 - 9s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 8s - loss: 1.8055 - accuracy: 0.3145 - 8s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.8021 - accuracy: 0.3156 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.8046 - accuracy: 0.3147 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.8028 - accuracy: 0.3178 - 9s/epoch - 11ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.8039 - accuracy: 0.3156 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.8023 - accuracy: 0.3169 - 8s/epoch - 10ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.8022 - accuracy: 0.3168 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 8s - loss: 1.8019 - accuracy: 0.3153 - 8s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 8s - loss: 1.8008 - accuracy: 0.3168 - 8s/epoch - 10ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 8s - loss: 1.8004 - accuracy: 0.3154 - 8s/epoch - 10ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.8003 - accuracy: 0.3179 - 8s/epoch - 10ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 8s - loss: 1.8006 - accuracy: 0.3170 - 8s/epoch - 10ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 8s - loss: 1.8006 - accuracy: 0.3173 - 8s/epoch - 11ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 8s - loss: 1.7993 - accuracy: 0.3175 - 8s/epoch - 10ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.7999 - accuracy: 0.3175 - 8s/epoch - 10ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,218\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,052,426\n",
      "_________________________________________________________________\n",
      "> layers=7, train=0.322, test=0.304\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.8475 - accuracy: 0.3037 - 9s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.8168 - accuracy: 0.3099 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 1.8087 - accuracy: 0.3150 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.8113 - accuracy: 0.3138 - 9s/epoch - 11ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.8089 - accuracy: 0.3139 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 1.8107 - accuracy: 0.3132 - 8s/epoch - 10ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 1.8107 - accuracy: 0.3144 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 1.8053 - accuracy: 0.3152 - 8s/epoch - 11ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 1.8053 - accuracy: 0.3172 - 8s/epoch - 11ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 8s - loss: 1.8036 - accuracy: 0.3161 - 8s/epoch - 10ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 8s - loss: 1.8036 - accuracy: 0.3178 - 8s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.8052 - accuracy: 0.3140 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.8044 - accuracy: 0.3167 - 8s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 8s - loss: 1.8055 - accuracy: 0.3164 - 8s/epoch - 10ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.8022 - accuracy: 0.3156 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.8018 - accuracy: 0.3168 - 8s/epoch - 10ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.8013 - accuracy: 0.3164 - 8s/epoch - 11ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.8028 - accuracy: 0.3158 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 8s - loss: 1.8022 - accuracy: 0.3152 - 8s/epoch - 10ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 8s - loss: 1.7994 - accuracy: 0.3188 - 8s/epoch - 10ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.7995 - accuracy: 0.3171 - 8s/epoch - 10ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 8s - loss: 1.7995 - accuracy: 0.3190 - 8s/epoch - 11ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 8s - loss: 1.7999 - accuracy: 0.3177 - 8s/epoch - 11ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 10s - loss: 1.7985 - accuracy: 0.3186 - 10s/epoch - 13ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.7993 - accuracy: 0.3186 - 9s/epoch - 11ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,010\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,118,218\n",
      "_________________________________________________________________\n",
      "> layers=8, train=0.314, test=0.296\n",
      "Epoch 1/25\n",
      "782/782 - 10s - loss: 1.8425 - accuracy: 0.3004 - 10s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.8200 - accuracy: 0.3090 - 9s/epoch - 11ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.8114 - accuracy: 0.3134 - 9s/epoch - 11ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 8s - loss: 1.8097 - accuracy: 0.3109 - 8s/epoch - 10ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.8077 - accuracy: 0.3131 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 1.8070 - accuracy: 0.3132 - 8s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.8079 - accuracy: 0.3125 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 1.8048 - accuracy: 0.3156 - 8s/epoch - 10ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 1.8042 - accuracy: 0.3160 - 8s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 8s - loss: 1.8034 - accuracy: 0.3169 - 8s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8057 - accuracy: 0.3153 - 9s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 1.8011 - accuracy: 0.3171 - 8s/epoch - 11ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.8009 - accuracy: 0.3175 - 8s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 8s - loss: 1.8038 - accuracy: 0.3152 - 8s/epoch - 11ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.8003 - accuracy: 0.3169 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 1.8013 - accuracy: 0.3153 - 8s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 1.8016 - accuracy: 0.3162 - 8s/epoch - 11ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.8008 - accuracy: 0.3195 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 10s - loss: 1.7998 - accuracy: 0.3182 - 10s/epoch - 13ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.8027 - accuracy: 0.3162 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.8012 - accuracy: 0.3169 - 9s/epoch - 12ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.7998 - accuracy: 0.3166 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.7987 - accuracy: 0.3194 - 9s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 8s - loss: 1.8012 - accuracy: 0.3170 - 8s/epoch - 11ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.7990 - accuracy: 0.3167 - 8s/epoch - 11ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,249,802\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,184,010\n",
      "_________________________________________________________________\n",
      "> layers=9, train=0.313, test=0.298\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.8517 - accuracy: 0.2981 - 9s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.8188 - accuracy: 0.3103 - 8s/epoch - 11ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.8109 - accuracy: 0.3121 - 9s/epoch - 11ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.8085 - accuracy: 0.3140 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.8070 - accuracy: 0.3148 - 9s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.8072 - accuracy: 0.3140 - 9s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.8037 - accuracy: 0.3162 - 9s/epoch - 11ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.8042 - accuracy: 0.3143 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.8023 - accuracy: 0.3180 - 9s/epoch - 11ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8057 - accuracy: 0.3125 - 9s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8010 - accuracy: 0.3168 - 9s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 9s - loss: 1.8056 - accuracy: 0.3144 - 9s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 1.8035 - accuracy: 0.3172 - 8s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.8029 - accuracy: 0.3180 - 9s/epoch - 11ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 1.8010 - accuracy: 0.3166 - 8s/epoch - 11ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.8002 - accuracy: 0.3170 - 9s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.8000 - accuracy: 0.3197 - 9s/epoch - 11ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.7982 - accuracy: 0.3191 - 9s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8004 - accuracy: 0.3175 - 9s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.7988 - accuracy: 0.3191 - 9s/epoch - 11ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 1.7995 - accuracy: 0.3185 - 8s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 8s - loss: 1.7995 - accuracy: 0.3182 - 8s/epoch - 11ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 8s - loss: 1.7988 - accuracy: 0.3184 - 8s/epoch - 11ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.7964 - accuracy: 0.3197 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.7953 - accuracy: 0.3178 - 9s/epoch - 11ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315,594\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,249,802\n",
      "_________________________________________________________________\n",
      "> layers=10, train=0.325, test=0.301\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.8781 - accuracy: 0.2909 - 12s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.8138 - accuracy: 0.3091 - 9s/epoch - 11ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.8111 - accuracy: 0.3143 - 9s/epoch - 11ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 10s - loss: 1.8065 - accuracy: 0.3144 - 10s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.8038 - accuracy: 0.3154 - 9s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.8060 - accuracy: 0.3166 - 9s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.8035 - accuracy: 0.3145 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.8073 - accuracy: 0.3141 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.8051 - accuracy: 0.3160 - 9s/epoch - 11ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8032 - accuracy: 0.3185 - 9s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8030 - accuracy: 0.3171 - 9s/epoch - 11ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.8021 - accuracy: 0.3177 - 10s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.8015 - accuracy: 0.3157 - 9s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.7974 - accuracy: 0.3202 - 9s/epoch - 11ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.7970 - accuracy: 0.3182 - 9s/epoch - 11ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.7999 - accuracy: 0.3163 - 9s/epoch - 12ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.7978 - accuracy: 0.3173 - 9s/epoch - 11ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.7975 - accuracy: 0.3179 - 9s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8008 - accuracy: 0.3163 - 9s/epoch - 11ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.7988 - accuracy: 0.3186 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.7967 - accuracy: 0.3196 - 9s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.7966 - accuracy: 0.3197 - 9s/epoch - 11ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.8009 - accuracy: 0.3186 - 9s/epoch - 11ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.7951 - accuracy: 0.3200 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.7968 - accuracy: 0.3210 - 9s/epoch - 11ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,381,386\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,315,594\n",
      "_________________________________________________________________\n",
      "> layers=11, train=0.327, test=0.298\n",
      "Epoch 1/25\n",
      "782/782 - 11s - loss: 1.8610 - accuracy: 0.2949 - 11s/epoch - 14ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.8184 - accuracy: 0.3094 - 9s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.8215 - accuracy: 0.3104 - 9s/epoch - 11ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 12s - loss: 1.8141 - accuracy: 0.3132 - 12s/epoch - 15ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 11s - loss: 1.8138 - accuracy: 0.3128 - 11s/epoch - 14ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.8078 - accuracy: 0.3141 - 10s/epoch - 13ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 10s - loss: 1.8075 - accuracy: 0.3170 - 10s/epoch - 13ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 10s - loss: 1.8070 - accuracy: 0.3136 - 10s/epoch - 13ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.8051 - accuracy: 0.3167 - 9s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8022 - accuracy: 0.3195 - 9s/epoch - 12ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8076 - accuracy: 0.3151 - 9s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.8024 - accuracy: 0.3169 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.8052 - accuracy: 0.3149 - 9s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 10s - loss: 1.8054 - accuracy: 0.3167 - 10s/epoch - 13ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 10s - loss: 1.8045 - accuracy: 0.3166 - 10s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.7994 - accuracy: 0.3178 - 11s/epoch - 14ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 10s - loss: 1.8022 - accuracy: 0.3145 - 10s/epoch - 13ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 10s - loss: 1.8026 - accuracy: 0.3172 - 10s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 10s - loss: 1.8031 - accuracy: 0.3166 - 10s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.8017 - accuracy: 0.3174 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.8011 - accuracy: 0.3184 - 9s/epoch - 12ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.8075 - accuracy: 0.3173 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.8050 - accuracy: 0.3163 - 9s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.8015 - accuracy: 0.3179 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.8066 - accuracy: 0.3151 - 9s/epoch - 11ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,447,178\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,381,386\n",
      "_________________________________________________________________\n",
      "> layers=12, train=0.319, test=0.297\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.8653 - accuracy: 0.2977 - 12s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.8182 - accuracy: 0.3101 - 10s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.8118 - accuracy: 0.3108 - 10s/epoch - 13ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.8082 - accuracy: 0.3154 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.8083 - accuracy: 0.3164 - 9s/epoch - 12ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.8038 - accuracy: 0.3174 - 10s/epoch - 13ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.8047 - accuracy: 0.3171 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.8053 - accuracy: 0.3148 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.8091 - accuracy: 0.3142 - 9s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 10s - loss: 1.8052 - accuracy: 0.3143 - 10s/epoch - 13ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8018 - accuracy: 0.3160 - 9s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 9s - loss: 1.8017 - accuracy: 0.3174 - 9s/epoch - 12ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.8007 - accuracy: 0.3172 - 9s/epoch - 12ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 10s - loss: 1.8068 - accuracy: 0.3170 - 10s/epoch - 13ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.8066 - accuracy: 0.3172 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.8075 - accuracy: 0.3169 - 9s/epoch - 12ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 10s - loss: 1.8020 - accuracy: 0.3172 - 10s/epoch - 13ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.8012 - accuracy: 0.3187 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8002 - accuracy: 0.3170 - 9s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.7984 - accuracy: 0.3209 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.8060 - accuracy: 0.3176 - 10s/epoch - 13ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.8005 - accuracy: 0.3193 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.8015 - accuracy: 0.3170 - 9s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.7992 - accuracy: 0.3184 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 10s - loss: 1.8011 - accuracy: 0.3183 - 10s/epoch - 13ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,512,970\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,447,178\n",
      "_________________________________________________________________\n",
      "> layers=13, train=0.326, test=0.298\n",
      "Epoch 1/25\n",
      "782/782 - 11s - loss: 1.8863 - accuracy: 0.2914 - 11s/epoch - 14ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.8187 - accuracy: 0.3082 - 10s/epoch - 13ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.8159 - accuracy: 0.3110 - 10s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.8107 - accuracy: 0.3155 - 9s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 10s - loss: 1.8117 - accuracy: 0.3140 - 10s/epoch - 13ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.8113 - accuracy: 0.3154 - 9s/epoch - 12ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.8106 - accuracy: 0.3140 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.8140 - accuracy: 0.3141 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 10s - loss: 1.8080 - accuracy: 0.3180 - 10s/epoch - 13ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8076 - accuracy: 0.3144 - 9s/epoch - 12ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8144 - accuracy: 0.3153 - 9s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.8082 - accuracy: 0.3169 - 10s/epoch - 13ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 11s - loss: 1.8070 - accuracy: 0.3166 - 11s/epoch - 14ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 11s - loss: 1.8052 - accuracy: 0.3189 - 11s/epoch - 14ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.8038 - accuracy: 0.3173 - 11s/epoch - 14ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.8080 - accuracy: 0.3159 - 11s/epoch - 14ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.8090 - accuracy: 0.3151 - 9s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.8097 - accuracy: 0.3151 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8039 - accuracy: 0.3171 - 9s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 10s - loss: 1.8007 - accuracy: 0.3184 - 10s/epoch - 13ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.8058 - accuracy: 0.3139 - 9s/epoch - 12ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.8032 - accuracy: 0.3176 - 9s/epoch - 12ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 10s - loss: 1.8052 - accuracy: 0.3168 - 10s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 10s - loss: 1.8051 - accuracy: 0.3176 - 10s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.8039 - accuracy: 0.3152 - 9s/epoch - 12ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,578,762\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,512,970\n",
      "_________________________________________________________________\n",
      "> layers=14, train=0.308, test=0.287\n",
      "Epoch 1/25\n",
      "782/782 - 11s - loss: 1.8805 - accuracy: 0.2904 - 11s/epoch - 14ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.8232 - accuracy: 0.3082 - 10s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.8215 - accuracy: 0.3088 - 10s/epoch - 13ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 10s - loss: 1.8240 - accuracy: 0.3080 - 10s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.8195 - accuracy: 0.3135 - 9s/epoch - 12ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.8120 - accuracy: 0.3143 - 10s/epoch - 12ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 10s - loss: 1.8151 - accuracy: 0.3122 - 10s/epoch - 13ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.8159 - accuracy: 0.3123 - 9s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.8114 - accuracy: 0.3164 - 9s/epoch - 12ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 10s - loss: 1.8079 - accuracy: 0.3153 - 10s/epoch - 12ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 12s - loss: 1.8172 - accuracy: 0.3122 - 12s/epoch - 15ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.8100 - accuracy: 0.3164 - 10s/epoch - 13ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 10s - loss: 1.8071 - accuracy: 0.3158 - 10s/epoch - 13ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 10s - loss: 1.8079 - accuracy: 0.3168 - 10s/epoch - 13ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 10s - loss: 1.8121 - accuracy: 0.3140 - 10s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 10s - loss: 1.8087 - accuracy: 0.3170 - 10s/epoch - 12ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.8100 - accuracy: 0.3156 - 9s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 10s - loss: 1.8084 - accuracy: 0.3155 - 10s/epoch - 13ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8069 - accuracy: 0.3151 - 9s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.8078 - accuracy: 0.3159 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.8079 - accuracy: 0.3144 - 10s/epoch - 12ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 10s - loss: 1.8054 - accuracy: 0.3168 - 10s/epoch - 13ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 10s - loss: 1.8112 - accuracy: 0.3140 - 10s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 10s - loss: 1.8123 - accuracy: 0.3135 - 10s/epoch - 13ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 10s - loss: 1.8058 - accuracy: 0.3172 - 10s/epoch - 13ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,554\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,578,762\n",
      "_________________________________________________________________\n",
      "> layers=15, train=0.312, test=0.285\n",
      "Epoch 1/25\n",
      "782/782 - 13s - loss: 1.9038 - accuracy: 0.2898 - 13s/epoch - 17ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 10s - loss: 1.8340 - accuracy: 0.3044 - 10s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 9s - loss: 1.8280 - accuracy: 0.3089 - 9s/epoch - 12ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 10s - loss: 1.8263 - accuracy: 0.3090 - 10s/epoch - 12ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 10s - loss: 1.8141 - accuracy: 0.3134 - 10s/epoch - 13ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 10s - loss: 1.8168 - accuracy: 0.3116 - 10s/epoch - 12ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 10s - loss: 1.8165 - accuracy: 0.3139 - 10s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 10s - loss: 1.8260 - accuracy: 0.3100 - 10s/epoch - 12ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 10s - loss: 1.8143 - accuracy: 0.3140 - 10s/epoch - 13ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8200 - accuracy: 0.3118 - 9s/epoch - 12ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 10s - loss: 1.8199 - accuracy: 0.3161 - 10s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 10s - loss: 1.8160 - accuracy: 0.3149 - 10s/epoch - 13ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 10s - loss: 1.8157 - accuracy: 0.3152 - 10s/epoch - 12ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.8186 - accuracy: 0.3142 - 9s/epoch - 12ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.8196 - accuracy: 0.3122 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 10s - loss: 1.8107 - accuracy: 0.3183 - 10s/epoch - 13ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.8154 - accuracy: 0.3155 - 9s/epoch - 12ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 10s - loss: 1.8144 - accuracy: 0.3150 - 10s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 11s - loss: 1.8172 - accuracy: 0.3146 - 11s/epoch - 14ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 11s - loss: 1.8102 - accuracy: 0.3159 - 11s/epoch - 14ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 10s - loss: 1.8131 - accuracy: 0.3144 - 10s/epoch - 13ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.8125 - accuracy: 0.3180 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 11s - loss: 1.8150 - accuracy: 0.3123 - 11s/epoch - 14ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.8114 - accuracy: 0.3157 - 9s/epoch - 12ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.8136 - accuracy: 0.3130 - 9s/epoch - 12ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,346\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,644,554\n",
      "_________________________________________________________________\n",
      "> layers=16, train=0.311, test=0.290\n",
      "Epoch 1/25\n",
      "782/782 - 10s - loss: 1.8978 - accuracy: 0.2949 - 10s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 9s - loss: 1.8286 - accuracy: 0.3093 - 9s/epoch - 12ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 10s - loss: 1.8227 - accuracy: 0.3120 - 10s/epoch - 13ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 9s - loss: 1.8230 - accuracy: 0.3115 - 9s/epoch - 11ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 1.8172 - accuracy: 0.3137 - 9s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 9s - loss: 1.8177 - accuracy: 0.3129 - 9s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 9s - loss: 1.8201 - accuracy: 0.3114 - 9s/epoch - 12ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 9s - loss: 1.8190 - accuracy: 0.3130 - 9s/epoch - 11ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 1.8251 - accuracy: 0.3113 - 9s/epoch - 11ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 9s - loss: 1.8135 - accuracy: 0.3145 - 9s/epoch - 11ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 9s - loss: 1.8188 - accuracy: 0.3133 - 9s/epoch - 12ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 9s - loss: 1.8158 - accuracy: 0.3125 - 9s/epoch - 11ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 9s - loss: 1.8226 - accuracy: 0.3121 - 9s/epoch - 11ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 9s - loss: 1.8132 - accuracy: 0.3174 - 9s/epoch - 11ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 9s - loss: 1.8262 - accuracy: 0.3118 - 9s/epoch - 12ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 9s - loss: 1.8168 - accuracy: 0.3137 - 9s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 9s - loss: 1.8171 - accuracy: 0.3146 - 9s/epoch - 11ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 1.8202 - accuracy: 0.3126 - 9s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 9s - loss: 1.8136 - accuracy: 0.3161 - 9s/epoch - 12ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 1.8203 - accuracy: 0.3119 - 9s/epoch - 12ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 9s - loss: 1.8200 - accuracy: 0.3144 - 9s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 9s - loss: 1.8165 - accuracy: 0.3127 - 9s/epoch - 11ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 9s - loss: 1.8200 - accuracy: 0.3136 - 9s/epoch - 12ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.8158 - accuracy: 0.3139 - 9s/epoch - 11ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 9s - loss: 1.8142 - accuracy: 0.3144 - 9s/epoch - 11ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,776,138\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,710,346\n",
      "_________________________________________________________________\n",
      "> layers=17, train=0.317, test=0.294\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.9055 - accuracy: 0.2832 - 12s/epoch - 16ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 11s - loss: 1.8386 - accuracy: 0.3043 - 11s/epoch - 14ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 11s - loss: 1.8304 - accuracy: 0.3064 - 11s/epoch - 14ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 11s - loss: 1.8332 - accuracy: 0.3064 - 11s/epoch - 14ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 11s - loss: 1.8270 - accuracy: 0.3136 - 11s/epoch - 14ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 11s - loss: 1.8280 - accuracy: 0.3105 - 11s/epoch - 14ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 11s - loss: 1.8292 - accuracy: 0.3095 - 11s/epoch - 15ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 11s - loss: 1.8312 - accuracy: 0.3106 - 11s/epoch - 14ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.8297 - accuracy: 0.3112 - 11s/epoch - 14ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.8273 - accuracy: 0.3109 - 11s/epoch - 14ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 11s - loss: 1.8253 - accuracy: 0.3113 - 11s/epoch - 13ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.8263 - accuracy: 0.3101 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 11s - loss: 1.8214 - accuracy: 0.3129 - 11s/epoch - 15ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 11s - loss: 1.8215 - accuracy: 0.3120 - 11s/epoch - 14ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.8235 - accuracy: 0.3146 - 11s/epoch - 14ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.8215 - accuracy: 0.3133 - 11s/epoch - 14ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 11s - loss: 1.8236 - accuracy: 0.3118 - 11s/epoch - 14ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 11s - loss: 1.8189 - accuracy: 0.3152 - 11s/epoch - 14ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 11s - loss: 1.8204 - accuracy: 0.3123 - 11s/epoch - 14ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 11s - loss: 1.8226 - accuracy: 0.3137 - 11s/epoch - 14ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 11s - loss: 1.8269 - accuracy: 0.3112 - 11s/epoch - 14ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.8173 - accuracy: 0.3127 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 12s - loss: 1.8275 - accuracy: 0.3108 - 12s/epoch - 15ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 13s - loss: 1.8184 - accuracy: 0.3113 - 13s/epoch - 17ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 13s - loss: 1.8242 - accuracy: 0.3129 - 13s/epoch - 17ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,841,930\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,776,138\n",
      "_________________________________________________________________\n",
      "> layers=18, train=0.326, test=0.303\n",
      "Epoch 1/25\n",
      "782/782 - 13s - loss: 1.9295 - accuracy: 0.2861 - 13s/epoch - 16ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 11s - loss: 1.8375 - accuracy: 0.3048 - 11s/epoch - 14ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 11s - loss: 1.8441 - accuracy: 0.3058 - 11s/epoch - 14ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 11s - loss: 1.8312 - accuracy: 0.3087 - 11s/epoch - 14ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 12s - loss: 1.8394 - accuracy: 0.3092 - 12s/epoch - 16ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 11s - loss: 1.8289 - accuracy: 0.3125 - 11s/epoch - 14ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 13s - loss: 1.8449 - accuracy: 0.3082 - 13s/epoch - 16ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 12s - loss: 1.8312 - accuracy: 0.3075 - 12s/epoch - 15ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.8291 - accuracy: 0.3115 - 11s/epoch - 14ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.8353 - accuracy: 0.3110 - 11s/epoch - 14ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 11s - loss: 1.8331 - accuracy: 0.3107 - 11s/epoch - 15ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.8422 - accuracy: 0.3086 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 11s - loss: 1.8268 - accuracy: 0.3124 - 11s/epoch - 14ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 11s - loss: 1.8345 - accuracy: 0.3112 - 11s/epoch - 14ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.8261 - accuracy: 0.3152 - 11s/epoch - 13ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 11s - loss: 1.8343 - accuracy: 0.3104 - 11s/epoch - 14ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 11s - loss: 1.8251 - accuracy: 0.3122 - 11s/epoch - 14ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 11s - loss: 1.8384 - accuracy: 0.3090 - 11s/epoch - 14ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 10s - loss: 1.8321 - accuracy: 0.3112 - 10s/epoch - 13ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 10s - loss: 1.8401 - accuracy: 0.3081 - 10s/epoch - 13ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 11s - loss: 1.8285 - accuracy: 0.3113 - 11s/epoch - 14ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.8385 - accuracy: 0.3088 - 11s/epoch - 14ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 11s - loss: 1.8348 - accuracy: 0.3095 - 11s/epoch - 14ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 11s - loss: 1.8343 - accuracy: 0.3135 - 11s/epoch - 14ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 11s - loss: 1.8280 - accuracy: 0.3100 - 11s/epoch - 14ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,722\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,841,930\n",
      "_________________________________________________________________\n",
      "> layers=19, train=0.321, test=0.294\n",
      "Epoch 1/25\n",
      "782/782 - 14s - loss: 1.9084 - accuracy: 0.2885 - 14s/epoch - 18ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 13s - loss: 1.8339 - accuracy: 0.3041 - 13s/epoch - 17ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 13s - loss: 1.8303 - accuracy: 0.3058 - 13s/epoch - 17ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 13s - loss: 1.8274 - accuracy: 0.3077 - 13s/epoch - 16ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 12s - loss: 1.8180 - accuracy: 0.3119 - 12s/epoch - 15ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 12s - loss: 1.8360 - accuracy: 0.3071 - 12s/epoch - 15ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 11s - loss: 1.8291 - accuracy: 0.3093 - 11s/epoch - 14ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 10s - loss: 1.8254 - accuracy: 0.3127 - 10s/epoch - 13ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 11s - loss: 1.8191 - accuracy: 0.3136 - 11s/epoch - 14ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 11s - loss: 1.8294 - accuracy: 0.3091 - 11s/epoch - 15ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 11s - loss: 1.8239 - accuracy: 0.3100 - 11s/epoch - 14ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 11s - loss: 1.8203 - accuracy: 0.3109 - 11s/epoch - 14ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 11s - loss: 1.8261 - accuracy: 0.3117 - 11s/epoch - 15ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 11s - loss: 1.8263 - accuracy: 0.3113 - 11s/epoch - 14ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 11s - loss: 1.8151 - accuracy: 0.3128 - 11s/epoch - 14ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 12s - loss: 1.8239 - accuracy: 0.3133 - 12s/epoch - 15ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 11s - loss: 1.8264 - accuracy: 0.3104 - 11s/epoch - 14ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 11s - loss: 1.8230 - accuracy: 0.3138 - 11s/epoch - 14ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 11s - loss: 1.8224 - accuracy: 0.3136 - 11s/epoch - 15ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 11s - loss: 1.8311 - accuracy: 0.3122 - 11s/epoch - 14ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 11s - loss: 1.8167 - accuracy: 0.3158 - 11s/epoch - 14ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 11s - loss: 1.8240 - accuracy: 0.3129 - 11s/epoch - 15ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 13s - loss: 1.8260 - accuracy: 0.3127 - 13s/epoch - 17ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 13s - loss: 1.8303 - accuracy: 0.3111 - 13s/epoch - 17ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 15s - loss: 1.8193 - accuracy: 0.3133 - 15s/epoch - 19ms/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,973,514\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,907,722\n",
      "_________________________________________________________________\n",
      "> layers=20, train=0.311, test=0.284\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "testX = testX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=32*32*3, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = Adam(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "# evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    " _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    " _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    " return train_acc, test_acc\n",
    "# Add one new layer and re-train only the new layer\n",
    "# Add one new layer and re-train only the new layer while keeping the last layer frozen\n",
    "def add_layer(model, trainX, trainy):\n",
    "    output_layer = model.layers[-1]\n",
    "    model.pop()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Set the last layer to be non-trainable\n",
    "    output_layer.trainable = False\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    # Compile the model before training\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the new layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add layers and evaluate the updated model\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    add_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the base model\n",
    "base_model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(base_model.layers), train_acc, test_acc))\n",
    "scores[len(base_model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Fine-tune the model by adding and training one layer at a time\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    # Add a new layer to the model\n",
    "    base_model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Add a new output layer with the correct number of units\n",
    "    base_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    base_model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with the new layer\n",
    "    base_model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy, testy)\n",
    "    base_model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(base_model.layers), train_acc, test_acc))\n",
    "    scores[len(base_model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.8762 - accuracy: 0.3339 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.6991 - accuracy: 0.4032 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 1.6227 - accuracy: 0.4296 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 1.5702 - accuracy: 0.4478 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 1.5290 - accuracy: 0.4648 - 7s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 1.4935 - accuracy: 0.4783 - 6s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 1.4640 - accuracy: 0.4887 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 1.4374 - accuracy: 0.4964 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 1.4130 - accuracy: 0.5053 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 1.3935 - accuracy: 0.5114 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 1.3719 - accuracy: 0.5212 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 1.3524 - accuracy: 0.5265 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 1.3351 - accuracy: 0.5315 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 1.3182 - accuracy: 0.5392 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 1.3054 - accuracy: 0.5421 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 1.2907 - accuracy: 0.5463 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 1.2753 - accuracy: 0.5521 - 7s/epoch - 8ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 1.2603 - accuracy: 0.5566 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 1.2500 - accuracy: 0.5623 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 1.2364 - accuracy: 0.5673 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 1.2261 - accuracy: 0.5687 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 1.2139 - accuracy: 0.5753 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 1.2046 - accuracy: 0.5781 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 1.1896 - accuracy: 0.5831 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 1.1800 - accuracy: 0.5859 - 6s/epoch - 7ms/step\n",
      "> layers=2, train=0.597, test=0.514\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.4885 - accuracy: 0.4737 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.2265 - accuracy: 0.5669 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.1653 - accuracy: 0.5905 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.1319 - accuracy: 0.6039 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.1103 - accuracy: 0.6120 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 1.0941 - accuracy: 0.6166 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 1.0816 - accuracy: 0.6227 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.0710 - accuracy: 0.6277 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.0620 - accuracy: 0.6297 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.0543 - accuracy: 0.6329 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 1.0472 - accuracy: 0.6340 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.0416 - accuracy: 0.6360 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 1.0357 - accuracy: 0.6376 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.0300 - accuracy: 0.6411 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 1.0253 - accuracy: 0.6423 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 1.0209 - accuracy: 0.6434 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 1.0160 - accuracy: 0.6453 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 1.0129 - accuracy: 0.6469 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 1.0086 - accuracy: 0.6473 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 1.0055 - accuracy: 0.6477 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 1.0011 - accuracy: 0.6511 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9978 - accuracy: 0.6526 - 4s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9949 - accuracy: 0.6538 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 3s - loss: 0.9920 - accuracy: 0.6539 - 3s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9887 - accuracy: 0.6560 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 789,258\n",
      "_________________________________________________________________\n",
      "> layers=3, train=0.659, test=0.543\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.2601 - accuracy: 0.5529 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0802 - accuracy: 0.6200 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0453 - accuracy: 0.6336 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0266 - accuracy: 0.6415 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.0131 - accuracy: 0.6451 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0029 - accuracy: 0.6473 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9943 - accuracy: 0.6505 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 0.9887 - accuracy: 0.6526 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 0.9823 - accuracy: 0.6556 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 0.9759 - accuracy: 0.6587 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9709 - accuracy: 0.6607 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9680 - accuracy: 0.6614 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9621 - accuracy: 0.6637 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.9599 - accuracy: 0.6622 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9551 - accuracy: 0.6654 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9528 - accuracy: 0.6660 - 4s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9496 - accuracy: 0.6678 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9461 - accuracy: 0.6683 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9438 - accuracy: 0.6701 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9423 - accuracy: 0.6698 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9402 - accuracy: 0.6707 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.9371 - accuracy: 0.6719 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9346 - accuracy: 0.6724 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9337 - accuracy: 0.6723 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9305 - accuracy: 0.6742 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 920,842\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 855,050\n",
      "_________________________________________________________________\n",
      "> layers=4, train=0.676, test=0.536\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.1466 - accuracy: 0.5958 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0115 - accuracy: 0.6435 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9862 - accuracy: 0.6545 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9741 - accuracy: 0.6561 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 0.9633 - accuracy: 0.6619 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 0.9581 - accuracy: 0.6644 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 0.9507 - accuracy: 0.6666 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9449 - accuracy: 0.6687 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 0.9416 - accuracy: 0.6700 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 0.9371 - accuracy: 0.6710 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9367 - accuracy: 0.6712 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 0.9330 - accuracy: 0.6709 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 0.9278 - accuracy: 0.6749 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 0.9267 - accuracy: 0.6742 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 0.9254 - accuracy: 0.6754 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9240 - accuracy: 0.6776 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9189 - accuracy: 0.6796 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.9184 - accuracy: 0.6781 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9159 - accuracy: 0.6783 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9143 - accuracy: 0.6799 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9121 - accuracy: 0.6803 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9106 - accuracy: 0.6799 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 0.9089 - accuracy: 0.6800 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 3s - loss: 0.9063 - accuracy: 0.6812 - 3s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 0.9052 - accuracy: 0.6833 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,634\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 920,842\n",
      "_________________________________________________________________\n",
      "> layers=5, train=0.685, test=0.529\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.0781 - accuracy: 0.6175 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 0.9738 - accuracy: 0.6569 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9529 - accuracy: 0.6664 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9410 - accuracy: 0.6693 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9333 - accuracy: 0.6719 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9288 - accuracy: 0.6728 - 4s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9269 - accuracy: 0.6738 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9224 - accuracy: 0.6762 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.9197 - accuracy: 0.6787 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9161 - accuracy: 0.6774 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9144 - accuracy: 0.6792 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9123 - accuracy: 0.6803 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9095 - accuracy: 0.6813 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9077 - accuracy: 0.6818 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9069 - accuracy: 0.6823 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9049 - accuracy: 0.6808 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9042 - accuracy: 0.6845 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9037 - accuracy: 0.6821 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9030 - accuracy: 0.6829 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9010 - accuracy: 0.6853 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8985 - accuracy: 0.6841 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8983 - accuracy: 0.6849 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8983 - accuracy: 0.6847 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8958 - accuracy: 0.6852 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8947 - accuracy: 0.6872 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,052,426\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 986,634\n",
      "_________________________________________________________________\n",
      "> layers=6, train=0.694, test=0.532\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.0370 - accuracy: 0.6393 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.9380 - accuracy: 0.6704 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 0.9276 - accuracy: 0.6761 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9181 - accuracy: 0.6774 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9142 - accuracy: 0.6805 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9123 - accuracy: 0.6796 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9060 - accuracy: 0.6831 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9039 - accuracy: 0.6816 - 6s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9028 - accuracy: 0.6848 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8991 - accuracy: 0.6857 - 4s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 0.8999 - accuracy: 0.6858 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 0.8974 - accuracy: 0.6867 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 0.8970 - accuracy: 0.6845 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 0.8965 - accuracy: 0.6872 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8931 - accuracy: 0.6873 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8932 - accuracy: 0.6860 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 0.8933 - accuracy: 0.6893 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.8922 - accuracy: 0.6862 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8895 - accuracy: 0.6869 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 0.8905 - accuracy: 0.6880 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 0.8902 - accuracy: 0.6889 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.8892 - accuracy: 0.6887 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 0.8869 - accuracy: 0.6883 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8888 - accuracy: 0.6884 - 4s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8860 - accuracy: 0.6883 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,218\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,052,426\n",
      "_________________________________________________________________\n",
      "> layers=7, train=0.686, test=0.525\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.0003 - accuracy: 0.6508 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.9224 - accuracy: 0.6739 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9127 - accuracy: 0.6803 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9081 - accuracy: 0.6819 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9038 - accuracy: 0.6818 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 0.9009 - accuracy: 0.6841 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 0.8964 - accuracy: 0.6870 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.8970 - accuracy: 0.6857 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 0.8944 - accuracy: 0.6848 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8913 - accuracy: 0.6879 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 0.8905 - accuracy: 0.6863 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8903 - accuracy: 0.6871 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 0.8889 - accuracy: 0.6861 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8887 - accuracy: 0.6868 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8857 - accuracy: 0.6893 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 0.8857 - accuracy: 0.6876 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 0.8857 - accuracy: 0.6886 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.8861 - accuracy: 0.6864 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 0.8835 - accuracy: 0.6874 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 0.8825 - accuracy: 0.6901 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8824 - accuracy: 0.6889 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.8819 - accuracy: 0.6891 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.8815 - accuracy: 0.6904 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8811 - accuracy: 0.6916 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8802 - accuracy: 0.6908 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,010\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,118,218\n",
      "_________________________________________________________________\n",
      "> layers=8, train=0.685, test=0.530\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0287 - accuracy: 0.6510 - 7s/epoch - 10ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9322 - accuracy: 0.6738 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9188 - accuracy: 0.6773 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9160 - accuracy: 0.6808 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9128 - accuracy: 0.6808 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9074 - accuracy: 0.6823 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9020 - accuracy: 0.6835 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 0.9006 - accuracy: 0.6822 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 0.8993 - accuracy: 0.6852 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 0.8972 - accuracy: 0.6858 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 0.8967 - accuracy: 0.6876 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 0.8977 - accuracy: 0.6862 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8944 - accuracy: 0.6869 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 0.8936 - accuracy: 0.6864 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 0.8961 - accuracy: 0.6876 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 0.8925 - accuracy: 0.6877 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 0.8937 - accuracy: 0.6867 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.8949 - accuracy: 0.6870 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 0.8911 - accuracy: 0.6867 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 0.8927 - accuracy: 0.6883 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 0.8916 - accuracy: 0.6871 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.8889 - accuracy: 0.6874 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 0.8913 - accuracy: 0.6890 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 3s - loss: 0.8903 - accuracy: 0.6888 - 3s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 0.8906 - accuracy: 0.6859 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,249,802\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,184,010\n",
      "_________________________________________________________________\n",
      "> layers=9, train=0.670, test=0.516\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.0224 - accuracy: 0.6524 - 4s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.9190 - accuracy: 0.6762 - 4s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9127 - accuracy: 0.6805 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9070 - accuracy: 0.6836 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9046 - accuracy: 0.6831 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 0.9046 - accuracy: 0.6831 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 0.9000 - accuracy: 0.6856 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 0.8995 - accuracy: 0.6853 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 0.8989 - accuracy: 0.6852 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8976 - accuracy: 0.6850 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 0.8949 - accuracy: 0.6847 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 0.8922 - accuracy: 0.6869 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 0.8908 - accuracy: 0.6876 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8908 - accuracy: 0.6860 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8896 - accuracy: 0.6885 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8890 - accuracy: 0.6870 - 4s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 0.8875 - accuracy: 0.6880 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.8883 - accuracy: 0.6886 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8889 - accuracy: 0.6884 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8869 - accuracy: 0.6888 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8870 - accuracy: 0.6889 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8884 - accuracy: 0.6888 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 0.8852 - accuracy: 0.6903 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 3s - loss: 0.8837 - accuracy: 0.6891 - 3s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 0.8843 - accuracy: 0.6894 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315,594\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,249,802\n",
      "_________________________________________________________________\n",
      "> layers=10, train=0.657, test=0.510\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.0123 - accuracy: 0.6566 - 5s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 0.9191 - accuracy: 0.6786 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9172 - accuracy: 0.6789 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9086 - accuracy: 0.6827 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 0.9028 - accuracy: 0.6828 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.8989 - accuracy: 0.6841 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8970 - accuracy: 0.6853 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 0.8953 - accuracy: 0.6846 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8927 - accuracy: 0.6858 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8920 - accuracy: 0.6877 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.8924 - accuracy: 0.6871 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8922 - accuracy: 0.6879 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8904 - accuracy: 0.6884 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8905 - accuracy: 0.6865 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 0.8885 - accuracy: 0.6879 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8873 - accuracy: 0.6883 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8892 - accuracy: 0.6878 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.8882 - accuracy: 0.6874 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8874 - accuracy: 0.6890 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8853 - accuracy: 0.6899 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8872 - accuracy: 0.6895 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8875 - accuracy: 0.6889 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8844 - accuracy: 0.6909 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8860 - accuracy: 0.6896 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8847 - accuracy: 0.6895 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,381,386\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,315,594\n",
      "_________________________________________________________________\n",
      "> layers=11, train=0.683, test=0.523\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 0.9880 - accuracy: 0.6596 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 0.9137 - accuracy: 0.6809 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 0.9053 - accuracy: 0.6813 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.8997 - accuracy: 0.6844 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.8965 - accuracy: 0.6853 - 4s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.8930 - accuracy: 0.6877 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8914 - accuracy: 0.6896 - 4s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8907 - accuracy: 0.6883 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8896 - accuracy: 0.6874 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8881 - accuracy: 0.6881 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 0.8844 - accuracy: 0.6893 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8853 - accuracy: 0.6900 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8854 - accuracy: 0.6889 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8856 - accuracy: 0.6897 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8827 - accuracy: 0.6901 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8842 - accuracy: 0.6888 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8828 - accuracy: 0.6910 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.8815 - accuracy: 0.6908 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 0.8806 - accuracy: 0.6903 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8808 - accuracy: 0.6915 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8794 - accuracy: 0.6906 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8785 - accuracy: 0.6918 - 4s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8807 - accuracy: 0.6916 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8814 - accuracy: 0.6890 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8796 - accuracy: 0.6911 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,447,178\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,381,386\n",
      "_________________________________________________________________\n",
      "> layers=12, train=0.678, test=0.521\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 0.9806 - accuracy: 0.6601 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 0.9132 - accuracy: 0.6805 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9070 - accuracy: 0.6828 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9003 - accuracy: 0.6837 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8953 - accuracy: 0.6851 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.8906 - accuracy: 0.6870 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8897 - accuracy: 0.6871 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.8883 - accuracy: 0.6884 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8873 - accuracy: 0.6881 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.8853 - accuracy: 0.6910 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.8840 - accuracy: 0.6899 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8860 - accuracy: 0.6879 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8833 - accuracy: 0.6888 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8850 - accuracy: 0.6905 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8840 - accuracy: 0.6900 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8840 - accuracy: 0.6891 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8813 - accuracy: 0.6906 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.8817 - accuracy: 0.6914 - 4s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 0.8797 - accuracy: 0.6900 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 0.8796 - accuracy: 0.6916 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 0.8800 - accuracy: 0.6916 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.8795 - accuracy: 0.6923 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8795 - accuracy: 0.6917 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8791 - accuracy: 0.6915 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8768 - accuracy: 0.6911 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,512,970\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,447,178\n",
      "_________________________________________________________________\n",
      "> layers=13, train=0.691, test=0.526\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 0.9706 - accuracy: 0.6668 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 0.9020 - accuracy: 0.6838 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.8985 - accuracy: 0.6852 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.8960 - accuracy: 0.6868 - 4s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 0.8951 - accuracy: 0.6853 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.8913 - accuracy: 0.6875 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8892 - accuracy: 0.6895 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8896 - accuracy: 0.6879 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8884 - accuracy: 0.6894 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8844 - accuracy: 0.6905 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.8851 - accuracy: 0.6899 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8840 - accuracy: 0.6894 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8839 - accuracy: 0.6911 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8821 - accuracy: 0.6911 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8835 - accuracy: 0.6894 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.8813 - accuracy: 0.6904 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.8807 - accuracy: 0.6915 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8807 - accuracy: 0.6916 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8809 - accuracy: 0.6901 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8801 - accuracy: 0.6908 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8788 - accuracy: 0.6914 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8808 - accuracy: 0.6914 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.8782 - accuracy: 0.6919 - 6s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8780 - accuracy: 0.6928 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8784 - accuracy: 0.6908 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,578,762\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,512,970\n",
      "_________________________________________________________________\n",
      "> layers=14, train=0.658, test=0.507\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 0.9752 - accuracy: 0.6649 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9050 - accuracy: 0.6837 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.8969 - accuracy: 0.6865 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.8955 - accuracy: 0.6883 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.8953 - accuracy: 0.6861 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.8925 - accuracy: 0.6872 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.8942 - accuracy: 0.6867 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8897 - accuracy: 0.6880 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8884 - accuracy: 0.6897 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.8869 - accuracy: 0.6891 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8893 - accuracy: 0.6886 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.8869 - accuracy: 0.6890 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8879 - accuracy: 0.6896 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8853 - accuracy: 0.6903 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8830 - accuracy: 0.6906 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.8856 - accuracy: 0.6885 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8824 - accuracy: 0.6898 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.8823 - accuracy: 0.6897 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8820 - accuracy: 0.6889 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8803 - accuracy: 0.6907 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8807 - accuracy: 0.6908 - 4s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8784 - accuracy: 0.6919 - 4s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8792 - accuracy: 0.6909 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.8799 - accuracy: 0.6910 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8793 - accuracy: 0.6913 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,554\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,578,762\n",
      "_________________________________________________________________\n",
      "> layers=15, train=0.695, test=0.533\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 0.9567 - accuracy: 0.6657 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9064 - accuracy: 0.6833 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9009 - accuracy: 0.6828 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.8975 - accuracy: 0.6851 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8947 - accuracy: 0.6877 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8935 - accuracy: 0.6874 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.8913 - accuracy: 0.6855 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.8895 - accuracy: 0.6882 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.8899 - accuracy: 0.6874 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.8881 - accuracy: 0.6891 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.8869 - accuracy: 0.6887 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.8832 - accuracy: 0.6902 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.8861 - accuracy: 0.6882 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.8837 - accuracy: 0.6904 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8840 - accuracy: 0.6901 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.8828 - accuracy: 0.6906 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.8826 - accuracy: 0.6908 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.8821 - accuracy: 0.6900 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.8813 - accuracy: 0.6904 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8783 - accuracy: 0.6912 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8787 - accuracy: 0.6902 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8771 - accuracy: 0.6926 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.8773 - accuracy: 0.6919 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8764 - accuracy: 0.6930 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8779 - accuracy: 0.6910 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,346\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,644,554\n",
      "_________________________________________________________________\n",
      "> layers=16, train=0.652, test=0.508\n",
      "Epoch 1/25\n",
      "782/782 - 8s - loss: 0.9682 - accuracy: 0.6645 - 8s/epoch - 10ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9074 - accuracy: 0.6836 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.8997 - accuracy: 0.6850 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.8928 - accuracy: 0.6878 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8940 - accuracy: 0.6869 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.8898 - accuracy: 0.6882 - 6s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 0.8873 - accuracy: 0.6902 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.8873 - accuracy: 0.6894 - 6s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.8865 - accuracy: 0.6892 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.8876 - accuracy: 0.6897 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 7s - loss: 0.8838 - accuracy: 0.6914 - 7s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.8846 - accuracy: 0.6903 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.8836 - accuracy: 0.6919 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 0.8820 - accuracy: 0.6896 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 0.8838 - accuracy: 0.6915 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 0.8804 - accuracy: 0.6925 - 7s/epoch - 9ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.8796 - accuracy: 0.6900 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 0.8795 - accuracy: 0.6918 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.8789 - accuracy: 0.6919 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.8785 - accuracy: 0.6920 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8797 - accuracy: 0.6915 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8774 - accuracy: 0.6912 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.8780 - accuracy: 0.6933 - 6s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8750 - accuracy: 0.6917 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8746 - accuracy: 0.6925 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,776,138\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,710,346\n",
      "_________________________________________________________________\n",
      "> layers=17, train=0.685, test=0.527\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 0.9787 - accuracy: 0.6660 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9027 - accuracy: 0.6844 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.8986 - accuracy: 0.6847 - 4s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.8951 - accuracy: 0.6874 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8928 - accuracy: 0.6882 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8915 - accuracy: 0.6873 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.8901 - accuracy: 0.6880 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8904 - accuracy: 0.6881 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8882 - accuracy: 0.6883 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.8876 - accuracy: 0.6889 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8877 - accuracy: 0.6898 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.8883 - accuracy: 0.6881 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8848 - accuracy: 0.6905 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8870 - accuracy: 0.6901 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.8829 - accuracy: 0.6890 - 4s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.8847 - accuracy: 0.6891 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.8856 - accuracy: 0.6900 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.8856 - accuracy: 0.6882 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.8860 - accuracy: 0.6895 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 0.8825 - accuracy: 0.6918 - 7s/epoch - 10ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 7s - loss: 0.8857 - accuracy: 0.6895 - 7s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8830 - accuracy: 0.6909 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.8827 - accuracy: 0.6894 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 0.8829 - accuracy: 0.6913 - 6s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.8824 - accuracy: 0.6915 - 6s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,841,930\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,776,138\n",
      "_________________________________________________________________\n",
      "> layers=18, train=0.695, test=0.530\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 0.9685 - accuracy: 0.6685 - 7s/epoch - 10ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 0.8998 - accuracy: 0.6847 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.8966 - accuracy: 0.6866 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.8958 - accuracy: 0.6864 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8949 - accuracy: 0.6865 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8915 - accuracy: 0.6866 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.8881 - accuracy: 0.6886 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8874 - accuracy: 0.6901 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8899 - accuracy: 0.6883 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.8880 - accuracy: 0.6877 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8886 - accuracy: 0.6895 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.8858 - accuracy: 0.6885 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8854 - accuracy: 0.6890 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.8860 - accuracy: 0.6889 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8883 - accuracy: 0.6901 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 0.8889 - accuracy: 0.6896 - 7s/epoch - 9ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.8879 - accuracy: 0.6889 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8868 - accuracy: 0.6885 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.8843 - accuracy: 0.6892 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.8857 - accuracy: 0.6908 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.8853 - accuracy: 0.6899 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.8833 - accuracy: 0.6908 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.8844 - accuracy: 0.6919 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8824 - accuracy: 0.6912 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.8850 - accuracy: 0.6909 - 4s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,722\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,841,930\n",
      "_________________________________________________________________\n",
      "> layers=19, train=0.686, test=0.525\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 0.9859 - accuracy: 0.6612 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9123 - accuracy: 0.6801 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 0.9085 - accuracy: 0.6835 - 4s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9011 - accuracy: 0.6841 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.8997 - accuracy: 0.6860 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.8958 - accuracy: 0.6885 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.8954 - accuracy: 0.6854 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.8939 - accuracy: 0.6873 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.8935 - accuracy: 0.6859 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.8896 - accuracy: 0.6876 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.8923 - accuracy: 0.6880 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.8904 - accuracy: 0.6871 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.8895 - accuracy: 0.6894 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.8842 - accuracy: 0.6905 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.8850 - accuracy: 0.6895 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.8825 - accuracy: 0.6900 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.8820 - accuracy: 0.6896 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.8818 - accuracy: 0.6916 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.8814 - accuracy: 0.6910 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.8815 - accuracy: 0.6914 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.8828 - accuracy: 0.6900 - 4s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.8798 - accuracy: 0.6915 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.8785 - accuracy: 0.6927 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.8772 - accuracy: 0.6931 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.8779 - accuracy: 0.6934 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,973,514\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,907,722\n",
      "_________________________________________________________________\n",
      "> layers=20, train=0.689, test=0.527\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "testX = testX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=32*32*3, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "# evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    " _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    " _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    " return train_acc, test_acc\n",
    "# Add one new layer and re-train only the new layer\n",
    "# Add one new layer and re-train only the new layer while keeping the last layer frozen\n",
    "def add_layer(model, trainX, trainy):\n",
    "    output_layer = model.layers[-1]\n",
    "    model.pop()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Set the last layer to be non-trainable\n",
    "    output_layer.trainable = False\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    # Compile the model before training\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the new layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add layers and evaluate the updated model\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    add_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.8809 - accuracy: 0.3293 - 9s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 1.7042 - accuracy: 0.4011 - 7s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.6266 - accuracy: 0.4313 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.5729 - accuracy: 0.4457 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 1.5324 - accuracy: 0.4641 - 6s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.4959 - accuracy: 0.4764 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.4673 - accuracy: 0.4856 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 1.4402 - accuracy: 0.4963 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.4155 - accuracy: 0.5025 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.3960 - accuracy: 0.5106 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 1.3768 - accuracy: 0.5179 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 1.3583 - accuracy: 0.5241 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.3419 - accuracy: 0.5306 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 1.3269 - accuracy: 0.5347 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 1.3132 - accuracy: 0.5388 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 1.2987 - accuracy: 0.5456 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 1.2835 - accuracy: 0.5514 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 1.2707 - accuracy: 0.5522 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 1.2575 - accuracy: 0.5602 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 1.2446 - accuracy: 0.5628 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 7s - loss: 1.2353 - accuracy: 0.5691 - 7s/epoch - 9ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 1.2224 - accuracy: 0.5720 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 1.2139 - accuracy: 0.5738 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 1.2013 - accuracy: 0.5783 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 1.1896 - accuracy: 0.5831 - 6s/epoch - 8ms/step\n",
      "> layers=2, train=0.548, test=0.485\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.5119 - accuracy: 0.4643 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 1.2529 - accuracy: 0.5574 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.1879 - accuracy: 0.5820 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.1530 - accuracy: 0.5961 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.1300 - accuracy: 0.6037 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.1123 - accuracy: 0.6112 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.0997 - accuracy: 0.6146 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0883 - accuracy: 0.6193 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.0795 - accuracy: 0.6213 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 1.0645 - accuracy: 0.6276 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.0605 - accuracy: 0.6294 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 1.0571 - accuracy: 0.6300 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.0534 - accuracy: 0.6314 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 1.0506 - accuracy: 0.6326 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 7s - loss: 1.0475 - accuracy: 0.6336 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 1.0448 - accuracy: 0.6353 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 1.0418 - accuracy: 0.6352 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 1.0398 - accuracy: 0.6363 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 1.0373 - accuracy: 0.6375 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 1.0306 - accuracy: 0.6399 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 1.0296 - accuracy: 0.6400 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 1.0282 - accuracy: 0.6400 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 1.0274 - accuracy: 0.6400 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.0262 - accuracy: 0.6411 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 1.0250 - accuracy: 0.6414 - lr: 0.0025 - 4s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 789,258\n",
      "_________________________________________________________________\n",
      "> layers=3, train=0.644, test=0.541\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.3093 - accuracy: 0.5327 - lr: 0.0100 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.1251 - accuracy: 0.6041 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.0910 - accuracy: 0.6162 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.0733 - accuracy: 0.6226 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.0613 - accuracy: 0.6264 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 1.0530 - accuracy: 0.6305 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 1.0445 - accuracy: 0.6322 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.0383 - accuracy: 0.6340 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.0321 - accuracy: 0.6373 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 1.0182 - accuracy: 0.6428 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.0149 - accuracy: 0.6427 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 1.0130 - accuracy: 0.6445 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 1.0109 - accuracy: 0.6446 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.0089 - accuracy: 0.6461 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 1.0070 - accuracy: 0.6468 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 1.0053 - accuracy: 0.6470 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 1.0043 - accuracy: 0.6471 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 1.0021 - accuracy: 0.6475 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 1.0012 - accuracy: 0.6483 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 0.9941 - accuracy: 0.6516 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9923 - accuracy: 0.6521 - lr: 0.0025 - 4s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.9915 - accuracy: 0.6519 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9911 - accuracy: 0.6512 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9905 - accuracy: 0.6519 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 0.9895 - accuracy: 0.6518 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 920,842\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 855,050\n",
      "_________________________________________________________________\n",
      "> layers=4, train=0.654, test=0.544\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.2112 - accuracy: 0.5741 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0736 - accuracy: 0.6212 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.0524 - accuracy: 0.6295 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.0415 - accuracy: 0.6355 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.0338 - accuracy: 0.6388 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0285 - accuracy: 0.6380 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.0247 - accuracy: 0.6405 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.0195 - accuracy: 0.6403 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0175 - accuracy: 0.6423 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9975 - accuracy: 0.6493 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 0.9948 - accuracy: 0.6515 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 0.9929 - accuracy: 0.6505 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 0.9917 - accuracy: 0.6514 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.9905 - accuracy: 0.6501 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9890 - accuracy: 0.6517 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9881 - accuracy: 0.6522 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9863 - accuracy: 0.6538 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 3s - loss: 0.9856 - accuracy: 0.6523 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 0.9852 - accuracy: 0.6531 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9763 - accuracy: 0.6577 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9752 - accuracy: 0.6577 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.9748 - accuracy: 0.6568 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9742 - accuracy: 0.6581 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9737 - accuracy: 0.6584 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9736 - accuracy: 0.6579 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,634\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 920,842\n",
      "_________________________________________________________________\n",
      "> layers=5, train=0.659, test=0.538\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.1598 - accuracy: 0.5941 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0503 - accuracy: 0.6312 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0369 - accuracy: 0.6379 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.0289 - accuracy: 0.6382 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.0203 - accuracy: 0.6425 - lr: 0.0100 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0184 - accuracy: 0.6418 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.0139 - accuracy: 0.6448 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0116 - accuracy: 0.6445 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0119 - accuracy: 0.6439 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 0.9869 - accuracy: 0.6530 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9852 - accuracy: 0.6534 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9837 - accuracy: 0.6539 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9833 - accuracy: 0.6554 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 0.9815 - accuracy: 0.6552 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9807 - accuracy: 0.6545 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9812 - accuracy: 0.6554 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 0.9799 - accuracy: 0.6544 - lr: 0.0050 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9787 - accuracy: 0.6557 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9789 - accuracy: 0.6554 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9682 - accuracy: 0.6593 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 0.9678 - accuracy: 0.6585 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 0.9673 - accuracy: 0.6591 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9669 - accuracy: 0.6598 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9669 - accuracy: 0.6608 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 0.9658 - accuracy: 0.6598 - lr: 0.0025 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,052,426\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 986,634\n",
      "_________________________________________________________________\n",
      "> layers=6, train=0.657, test=0.537\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.1767 - accuracy: 0.5955 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0510 - accuracy: 0.6312 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0390 - accuracy: 0.6358 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.0338 - accuracy: 0.6368 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.0266 - accuracy: 0.6389 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0199 - accuracy: 0.6418 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.0197 - accuracy: 0.6409 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0138 - accuracy: 0.6432 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0122 - accuracy: 0.6449 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9848 - accuracy: 0.6535 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9830 - accuracy: 0.6540 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9814 - accuracy: 0.6543 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9812 - accuracy: 0.6549 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.9801 - accuracy: 0.6561 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9796 - accuracy: 0.6553 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9800 - accuracy: 0.6548 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9794 - accuracy: 0.6528 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9781 - accuracy: 0.6560 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9779 - accuracy: 0.6552 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9675 - accuracy: 0.6596 - lr: 0.0025 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9674 - accuracy: 0.6596 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9668 - accuracy: 0.6602 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9660 - accuracy: 0.6593 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9656 - accuracy: 0.6603 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9664 - accuracy: 0.6589 - lr: 0.0025 - 4s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,218\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,052,426\n",
      "_________________________________________________________________\n",
      "> layers=7, train=0.662, test=0.539\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.1308 - accuracy: 0.6045 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0372 - accuracy: 0.6350 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0258 - accuracy: 0.6402 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0174 - accuracy: 0.6419 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.0141 - accuracy: 0.6451 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.0100 - accuracy: 0.6447 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.0056 - accuracy: 0.6465 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0047 - accuracy: 0.6460 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0039 - accuracy: 0.6469 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9779 - accuracy: 0.6533 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9775 - accuracy: 0.6551 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9764 - accuracy: 0.6564 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9759 - accuracy: 0.6582 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9759 - accuracy: 0.6559 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9762 - accuracy: 0.6570 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9748 - accuracy: 0.6546 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9758 - accuracy: 0.6548 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9744 - accuracy: 0.6572 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9746 - accuracy: 0.6558 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9637 - accuracy: 0.6610 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9636 - accuracy: 0.6604 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9629 - accuracy: 0.6607 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9638 - accuracy: 0.6610 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9631 - accuracy: 0.6614 - lr: 0.0025 - 4s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9625 - accuracy: 0.6615 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,010\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,118,218\n",
      "_________________________________________________________________\n",
      "> layers=8, train=0.659, test=0.538\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.1148 - accuracy: 0.6096 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0340 - accuracy: 0.6391 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0185 - accuracy: 0.6416 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0181 - accuracy: 0.6411 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.0113 - accuracy: 0.6440 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0052 - accuracy: 0.6480 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.0056 - accuracy: 0.6469 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0034 - accuracy: 0.6486 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.0023 - accuracy: 0.6459 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9766 - accuracy: 0.6556 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9765 - accuracy: 0.6574 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9762 - accuracy: 0.6560 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 7s - loss: 0.9759 - accuracy: 0.6555 - lr: 0.0050 - 7s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9744 - accuracy: 0.6574 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9754 - accuracy: 0.6563 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9747 - accuracy: 0.6575 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9744 - accuracy: 0.6561 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9744 - accuracy: 0.6572 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9732 - accuracy: 0.6563 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9632 - accuracy: 0.6601 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9635 - accuracy: 0.6608 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9624 - accuracy: 0.6607 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9625 - accuracy: 0.6608 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9627 - accuracy: 0.6610 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9618 - accuracy: 0.6595 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,249,802\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,184,010\n",
      "_________________________________________________________________\n",
      "> layers=9, train=0.661, test=0.534\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.1146 - accuracy: 0.6175 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0219 - accuracy: 0.6409 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0138 - accuracy: 0.6458 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.0064 - accuracy: 0.6452 - lr: 0.0100 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.0063 - accuracy: 0.6452 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 1.0050 - accuracy: 0.6455 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 1.0041 - accuracy: 0.6468 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0038 - accuracy: 0.6463 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0003 - accuracy: 0.6490 - lr: 0.0100 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9753 - accuracy: 0.6561 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9743 - accuracy: 0.6565 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9751 - accuracy: 0.6568 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9757 - accuracy: 0.6566 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.9741 - accuracy: 0.6564 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9726 - accuracy: 0.6576 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9745 - accuracy: 0.6555 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9735 - accuracy: 0.6573 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 0.9734 - accuracy: 0.6586 - lr: 0.0050 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9735 - accuracy: 0.6573 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9626 - accuracy: 0.6617 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9625 - accuracy: 0.6600 - lr: 0.0025 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9622 - accuracy: 0.6608 - lr: 0.0025 - 4s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9621 - accuracy: 0.6620 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9622 - accuracy: 0.6612 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9622 - accuracy: 0.6603 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315,594\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,249,802\n",
      "_________________________________________________________________\n",
      "> layers=10, train=0.660, test=0.535\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0963 - accuracy: 0.6216 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.0170 - accuracy: 0.6429 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 1.0119 - accuracy: 0.6436 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0065 - accuracy: 0.6470 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 1.0012 - accuracy: 0.6450 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9967 - accuracy: 0.6489 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9956 - accuracy: 0.6504 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9962 - accuracy: 0.6478 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9939 - accuracy: 0.6503 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9729 - accuracy: 0.6573 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9714 - accuracy: 0.6569 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9711 - accuracy: 0.6576 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9700 - accuracy: 0.6571 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9714 - accuracy: 0.6560 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9704 - accuracy: 0.6582 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9710 - accuracy: 0.6571 - lr: 0.0050 - 4s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9704 - accuracy: 0.6587 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9689 - accuracy: 0.6592 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9696 - accuracy: 0.6579 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9609 - accuracy: 0.6618 - lr: 0.0025 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9600 - accuracy: 0.6609 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9602 - accuracy: 0.6609 - lr: 0.0025 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9603 - accuracy: 0.6611 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9599 - accuracy: 0.6596 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9597 - accuracy: 0.6629 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,381,386\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,315,594\n",
      "_________________________________________________________________\n",
      "> layers=11, train=0.659, test=0.530\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0802 - accuracy: 0.6238 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.0173 - accuracy: 0.6417 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0076 - accuracy: 0.6465 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0043 - accuracy: 0.6463 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9988 - accuracy: 0.6479 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9953 - accuracy: 0.6494 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9912 - accuracy: 0.6506 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9900 - accuracy: 0.6518 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9889 - accuracy: 0.6508 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9700 - accuracy: 0.6587 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9704 - accuracy: 0.6587 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9683 - accuracy: 0.6589 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9685 - accuracy: 0.6584 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9677 - accuracy: 0.6586 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9676 - accuracy: 0.6584 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9682 - accuracy: 0.6593 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9673 - accuracy: 0.6575 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9670 - accuracy: 0.6586 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9674 - accuracy: 0.6585 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9590 - accuracy: 0.6618 - lr: 0.0025 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9594 - accuracy: 0.6615 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9589 - accuracy: 0.6622 - lr: 0.0025 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9590 - accuracy: 0.6616 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9588 - accuracy: 0.6621 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9582 - accuracy: 0.6605 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,447,178\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,381,386\n",
      "_________________________________________________________________\n",
      "> layers=12, train=0.662, test=0.535\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0663 - accuracy: 0.6282 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0029 - accuracy: 0.6468 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9934 - accuracy: 0.6509 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9908 - accuracy: 0.6525 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9910 - accuracy: 0.6487 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9867 - accuracy: 0.6513 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9842 - accuracy: 0.6530 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9830 - accuracy: 0.6526 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9805 - accuracy: 0.6543 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9670 - accuracy: 0.6600 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9657 - accuracy: 0.6588 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9652 - accuracy: 0.6599 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9647 - accuracy: 0.6596 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9648 - accuracy: 0.6583 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9649 - accuracy: 0.6595 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9644 - accuracy: 0.6586 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9650 - accuracy: 0.6596 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9631 - accuracy: 0.6614 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9634 - accuracy: 0.6597 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9569 - accuracy: 0.6618 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9564 - accuracy: 0.6621 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 7s - loss: 0.9566 - accuracy: 0.6614 - lr: 0.0025 - 7s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 7s - loss: 0.9562 - accuracy: 0.6617 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9565 - accuracy: 0.6623 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9562 - accuracy: 0.6623 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,512,970\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,447,178\n",
      "_________________________________________________________________\n",
      "> layers=13, train=0.662, test=0.537\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0541 - accuracy: 0.6315 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9966 - accuracy: 0.6491 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9907 - accuracy: 0.6517 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9877 - accuracy: 0.6520 - lr: 0.0100 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9853 - accuracy: 0.6526 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9856 - accuracy: 0.6538 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9840 - accuracy: 0.6520 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9816 - accuracy: 0.6527 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9808 - accuracy: 0.6543 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9657 - accuracy: 0.6596 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9643 - accuracy: 0.6606 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9643 - accuracy: 0.6595 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9634 - accuracy: 0.6603 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9636 - accuracy: 0.6609 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9635 - accuracy: 0.6588 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9631 - accuracy: 0.6600 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9635 - accuracy: 0.6598 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9637 - accuracy: 0.6604 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9629 - accuracy: 0.6605 - lr: 0.0050 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9556 - accuracy: 0.6621 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9552 - accuracy: 0.6626 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9547 - accuracy: 0.6615 - lr: 0.0025 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9551 - accuracy: 0.6631 - lr: 0.0025 - 6s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9547 - accuracy: 0.6629 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9547 - accuracy: 0.6627 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,578,762\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,512,970\n",
      "_________________________________________________________________\n",
      "> layers=14, train=0.663, test=0.538\n",
      "Epoch 1/25\n",
      "782/782 - 12s - loss: 1.0650 - accuracy: 0.6320 - lr: 0.0100 - 12s/epoch - 15ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9965 - accuracy: 0.6484 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9903 - accuracy: 0.6517 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9864 - accuracy: 0.6533 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9854 - accuracy: 0.6517 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9844 - accuracy: 0.6531 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9805 - accuracy: 0.6542 - lr: 0.0100 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9812 - accuracy: 0.6534 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9807 - accuracy: 0.6548 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9640 - accuracy: 0.6594 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9638 - accuracy: 0.6607 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9629 - accuracy: 0.6593 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9628 - accuracy: 0.6611 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9632 - accuracy: 0.6599 - lr: 0.0050 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9630 - accuracy: 0.6610 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9627 - accuracy: 0.6608 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 0.9626 - accuracy: 0.6626 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9631 - accuracy: 0.6609 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9617 - accuracy: 0.6584 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9551 - accuracy: 0.6625 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9554 - accuracy: 0.6614 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9551 - accuracy: 0.6633 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9549 - accuracy: 0.6624 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 0.9552 - accuracy: 0.6636 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9545 - accuracy: 0.6628 - lr: 0.0025 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,554\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,578,762\n",
      "_________________________________________________________________\n",
      "> layers=15, train=0.664, test=0.536\n",
      "Epoch 1/25\n",
      "782/782 - 8s - loss: 1.0413 - accuracy: 0.6355 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9900 - accuracy: 0.6513 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9863 - accuracy: 0.6529 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 0.9823 - accuracy: 0.6547 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 0.9811 - accuracy: 0.6559 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 0.9794 - accuracy: 0.6546 - lr: 0.0100 - 8s/epoch - 11ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 0.9788 - accuracy: 0.6555 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 8s - loss: 0.9783 - accuracy: 0.6554 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 8s - loss: 0.9765 - accuracy: 0.6562 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 8s - loss: 0.9626 - accuracy: 0.6601 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 8s - loss: 0.9624 - accuracy: 0.6603 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 0.9626 - accuracy: 0.6597 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 0.9617 - accuracy: 0.6598 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 8s - loss: 0.9611 - accuracy: 0.6626 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 8s - loss: 0.9612 - accuracy: 0.6612 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 8s - loss: 0.9608 - accuracy: 0.6605 - lr: 0.0050 - 8s/epoch - 11ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 0.9603 - accuracy: 0.6612 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 8s - loss: 0.9606 - accuracy: 0.6607 - lr: 0.0050 - 8s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 8s - loss: 0.9601 - accuracy: 0.6620 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 9s - loss: 0.9540 - accuracy: 0.6624 - lr: 0.0025 - 9s/epoch - 11ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 8s - loss: 0.9540 - accuracy: 0.6627 - lr: 0.0025 - 8s/epoch - 11ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 8s - loss: 0.9538 - accuracy: 0.6638 - lr: 0.0025 - 8s/epoch - 10ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 7s - loss: 0.9538 - accuracy: 0.6627 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 8s - loss: 0.9531 - accuracy: 0.6635 - lr: 0.0025 - 8s/epoch - 10ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 0.9537 - accuracy: 0.6632 - lr: 0.0025 - 8s/epoch - 10ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,346\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,644,554\n",
      "_________________________________________________________________\n",
      "> layers=16, train=0.663, test=0.536\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.0659 - accuracy: 0.6279 - lr: 0.0100 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 0.9989 - accuracy: 0.6502 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 0.9919 - accuracy: 0.6510 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 0.9862 - accuracy: 0.6540 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 0.9842 - accuracy: 0.6534 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9844 - accuracy: 0.6555 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9825 - accuracy: 0.6540 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9814 - accuracy: 0.6538 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 9s - loss: 0.9803 - accuracy: 0.6560 - lr: 0.0100 - 9s/epoch - 11ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 7s - loss: 0.9634 - accuracy: 0.6605 - lr: 0.0050 - 7s/epoch - 10ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9629 - accuracy: 0.6609 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9618 - accuracy: 0.6621 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9616 - accuracy: 0.6611 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9624 - accuracy: 0.6603 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9624 - accuracy: 0.6614 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 0.9612 - accuracy: 0.6606 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 0.9617 - accuracy: 0.6626 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 0.9610 - accuracy: 0.6599 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9603 - accuracy: 0.6603 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 0.9539 - accuracy: 0.6626 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 7s - loss: 0.9537 - accuracy: 0.6628 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 7s - loss: 0.9536 - accuracy: 0.6629 - lr: 0.0025 - 7s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 8s - loss: 0.9537 - accuracy: 0.6627 - lr: 0.0025 - 8s/epoch - 10ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9536 - accuracy: 0.6625 - lr: 0.0025 - 7s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9540 - accuracy: 0.6629 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,776,138\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,710,346\n",
      "_________________________________________________________________\n",
      "> layers=17, train=0.664, test=0.534\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.0570 - accuracy: 0.6296 - lr: 0.0100 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 1.0027 - accuracy: 0.6481 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 8s - loss: 0.9950 - accuracy: 0.6504 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 0.9925 - accuracy: 0.6514 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 0.9902 - accuracy: 0.6531 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9881 - accuracy: 0.6524 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9852 - accuracy: 0.6537 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9821 - accuracy: 0.6548 - lr: 0.0100 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9802 - accuracy: 0.6564 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9650 - accuracy: 0.6601 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 7s - loss: 0.9638 - accuracy: 0.6607 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9635 - accuracy: 0.6589 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9633 - accuracy: 0.6597 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9627 - accuracy: 0.6607 - lr: 0.0050 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9631 - accuracy: 0.6602 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9625 - accuracy: 0.6621 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 0.9627 - accuracy: 0.6610 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 0.9633 - accuracy: 0.6608 - lr: 0.0050 - 9s/epoch - 12ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 0.9633 - accuracy: 0.6605 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 0.9546 - accuracy: 0.6634 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9548 - accuracy: 0.6626 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9546 - accuracy: 0.6635 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9539 - accuracy: 0.6630 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9543 - accuracy: 0.6631 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 7s - loss: 0.9542 - accuracy: 0.6645 - lr: 0.0025 - 7s/epoch - 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,841,930\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,776,138\n",
      "_________________________________________________________________\n",
      "> layers=18, train=0.665, test=0.537\n",
      "Epoch 1/25\n",
      "782/782 - 10s - loss: 1.0644 - accuracy: 0.6319 - lr: 0.0100 - 10s/epoch - 13ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 8s - loss: 0.9972 - accuracy: 0.6511 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 0.9911 - accuracy: 0.6532 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 8s - loss: 0.9880 - accuracy: 0.6527 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 9s - loss: 0.9859 - accuracy: 0.6538 - lr: 0.0100 - 9s/epoch - 11ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 0.9878 - accuracy: 0.6551 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 7s - loss: 0.9848 - accuracy: 0.6540 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 0.9820 - accuracy: 0.6540 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 7s - loss: 0.9812 - accuracy: 0.6545 - lr: 0.0100 - 7s/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 7s - loss: 0.9637 - accuracy: 0.6611 - lr: 0.0050 - 7s/epoch - 8ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 8s - loss: 0.9642 - accuracy: 0.6602 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 0.9633 - accuracy: 0.6594 - lr: 0.0050 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 7s - loss: 0.9625 - accuracy: 0.6602 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 0.9632 - accuracy: 0.6607 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 7s - loss: 0.9631 - accuracy: 0.6608 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9631 - accuracy: 0.6614 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 0.9622 - accuracy: 0.6600 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 0.9622 - accuracy: 0.6617 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9613 - accuracy: 0.6616 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9543 - accuracy: 0.6631 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9538 - accuracy: 0.6638 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9543 - accuracy: 0.6632 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9544 - accuracy: 0.6629 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9538 - accuracy: 0.6632 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9546 - accuracy: 0.6636 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,722\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,841,930\n",
      "_________________________________________________________________\n",
      "> layers=19, train=0.665, test=0.534\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.0618 - accuracy: 0.6316 - lr: 0.0100 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 1.0004 - accuracy: 0.6495 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9964 - accuracy: 0.6510 - lr: 0.0100 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 7s - loss: 0.9932 - accuracy: 0.6517 - lr: 0.0100 - 7s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 0.9895 - accuracy: 0.6529 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 8s - loss: 0.9871 - accuracy: 0.6519 - lr: 0.0100 - 8s/epoch - 10ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 7s - loss: 0.9853 - accuracy: 0.6530 - lr: 0.0100 - 7s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 0.9836 - accuracy: 0.6553 - lr: 0.0100 - 7s/epoch - 9ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 7s - loss: 0.9820 - accuracy: 0.6546 - lr: 0.0100 - 7s/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9650 - accuracy: 0.6605 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 7s - loss: 0.9623 - accuracy: 0.6604 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 7s - loss: 0.9633 - accuracy: 0.6597 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9632 - accuracy: 0.6600 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9631 - accuracy: 0.6617 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9637 - accuracy: 0.6613 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9626 - accuracy: 0.6598 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9621 - accuracy: 0.6618 - lr: 0.0050 - 6s/epoch - 8ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 0.9622 - accuracy: 0.6601 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 0.9636 - accuracy: 0.6607 - lr: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9547 - accuracy: 0.6642 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9545 - accuracy: 0.6630 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9548 - accuracy: 0.6635 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9541 - accuracy: 0.6639 - lr: 0.0025 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9542 - accuracy: 0.6627 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 7s - loss: 0.9537 - accuracy: 0.6640 - lr: 0.0025 - 7s/epoch - 9ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,973,514\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,907,722\n",
      "_________________________________________________________________\n",
      "> layers=20, train=0.664, test=0.536\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "testX = testX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=32*32*3, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Learning rate scheduling function\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lr\n",
    "\n",
    "# Evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    return train_acc, test_acc\n",
    "\n",
    "# Add one new layer and re-train only the new layer\n",
    "def add_layer(model, trainX, trainy):\n",
    "    output_layer = model.layers[-1]\n",
    "    model.pop()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Set the last layer to be non-trainable\n",
    "    output_layer.trainable = False\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    # Compile the model before training\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the new layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2, callbacks=[lr_scheduler])\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add layers and evaluate the updated model\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    add_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 0.9669 - accuracy: 0.6582 - 9s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9664 - accuracy: 0.6596 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9664 - accuracy: 0.6587 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9663 - accuracy: 0.6586 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 7s - loss: 0.9679 - accuracy: 0.6575 - 7s/epoch - 9ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 0.9686 - accuracy: 0.6593 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9665 - accuracy: 0.6593 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9663 - accuracy: 0.6587 - 6s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9665 - accuracy: 0.6592 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9667 - accuracy: 0.6586 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9667 - accuracy: 0.6588 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9661 - accuracy: 0.6600 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9675 - accuracy: 0.6595 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9672 - accuracy: 0.6581 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9657 - accuracy: 0.6601 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9662 - accuracy: 0.6607 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9661 - accuracy: 0.6601 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9664 - accuracy: 0.6585 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9655 - accuracy: 0.6595 - 6s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9667 - accuracy: 0.6590 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9662 - accuracy: 0.6591 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9655 - accuracy: 0.6599 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9668 - accuracy: 0.6594 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9649 - accuracy: 0.6593 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9657 - accuracy: 0.6596 - 6s/epoch - 8ms/step\n",
      "> layers=20, train=0.651, test=0.528\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unfreeze_softmax_layer(model):\n",
    "    # Freeze all layers except the last one (softmax layer)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "\n",
    "    # Compile the model with only the softmax layer trainable\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the softmax layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "unfreeze_softmax_layer(model)\n",
    "\n",
    "# Evaluate the model after unfreezing the softmax layer\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[19] = (train_acc, test_acc)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 10s - loss: 0.9643 - accuracy: 0.6607 - 10s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 0.9658 - accuracy: 0.6598 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9656 - accuracy: 0.6596 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9654 - accuracy: 0.6588 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9642 - accuracy: 0.6605 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9666 - accuracy: 0.6592 - 6s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9658 - accuracy: 0.6583 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9659 - accuracy: 0.6592 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9654 - accuracy: 0.6582 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9659 - accuracy: 0.6592 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9657 - accuracy: 0.6573 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9650 - accuracy: 0.6586 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9668 - accuracy: 0.6577 - 6s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9671 - accuracy: 0.6572 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9659 - accuracy: 0.6586 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9666 - accuracy: 0.6583 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9659 - accuracy: 0.6594 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9643 - accuracy: 0.6599 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9656 - accuracy: 0.6591 - 6s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9657 - accuracy: 0.6603 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9639 - accuracy: 0.6603 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9657 - accuracy: 0.6593 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9670 - accuracy: 0.6584 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9655 - accuracy: 0.6588 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9648 - accuracy: 0.6598 - 5s/epoch - 6ms/step\n",
      "> layers=20, train=0.660, test=0.535\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unfreeze_softmax_layer(model):\n",
    "    # Freeze all layers except the last one (softmax layer)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "\n",
    "    # Compile the model with only the softmax layer trainable\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the softmax layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "unfreeze_softmax_layer(model)\n",
    "\n",
    "# Evaluate the model after unfreezing the softmax layer\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[19] = (train_acc, test_acc)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 - 6s - loss: 0.9644 - accuracy: 0.6597 - 6s/epoch - 8ms/step\n",
      "Epoch 2/50\n",
      "782/782 - 6s - loss: 0.9646 - accuracy: 0.6594 - 6s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "782/782 - 6s - loss: 0.9639 - accuracy: 0.6603 - 6s/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "782/782 - 5s - loss: 0.9642 - accuracy: 0.6587 - 5s/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "782/782 - 5s - loss: 0.9643 - accuracy: 0.6594 - 5s/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "782/782 - 5s - loss: 0.9649 - accuracy: 0.6598 - 5s/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "782/782 - 6s - loss: 0.9642 - accuracy: 0.6590 - 6s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "782/782 - 5s - loss: 0.9632 - accuracy: 0.6609 - 5s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "782/782 - 5s - loss: 0.9654 - accuracy: 0.6592 - 5s/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "782/782 - 5s - loss: 0.9640 - accuracy: 0.6594 - 5s/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "782/782 - 5s - loss: 0.9637 - accuracy: 0.6596 - 5s/epoch - 6ms/step\n",
      "Epoch 12/50\n",
      "782/782 - 5s - loss: 0.9636 - accuracy: 0.6603 - 5s/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "782/782 - 5s - loss: 0.9655 - accuracy: 0.6590 - 5s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "782/782 - 6s - loss: 0.9647 - accuracy: 0.6597 - 6s/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "782/782 - 5s - loss: 0.9632 - accuracy: 0.6595 - 5s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "782/782 - 5s - loss: 0.9637 - accuracy: 0.6607 - 5s/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "782/782 - 5s - loss: 0.9636 - accuracy: 0.6598 - 5s/epoch - 6ms/step\n",
      "Epoch 18/50\n",
      "782/782 - 5s - loss: 0.9635 - accuracy: 0.6604 - 5s/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "782/782 - 5s - loss: 0.9643 - accuracy: 0.6588 - 5s/epoch - 6ms/step\n",
      "Epoch 20/50\n",
      "782/782 - 5s - loss: 0.9648 - accuracy: 0.6586 - 5s/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "782/782 - 6s - loss: 0.9628 - accuracy: 0.6596 - 6s/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "782/782 - 6s - loss: 0.9648 - accuracy: 0.6596 - 6s/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "782/782 - 5s - loss: 0.9640 - accuracy: 0.6606 - 5s/epoch - 6ms/step\n",
      "Epoch 24/50\n",
      "782/782 - 6s - loss: 0.9646 - accuracy: 0.6588 - 6s/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "782/782 - 7s - loss: 0.9644 - accuracy: 0.6604 - 7s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "782/782 - 7s - loss: 0.9649 - accuracy: 0.6593 - 7s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "782/782 - 8s - loss: 0.9651 - accuracy: 0.6595 - 8s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "782/782 - 8s - loss: 0.9645 - accuracy: 0.6607 - 8s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "782/782 - 7s - loss: 0.9648 - accuracy: 0.6600 - 7s/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "782/782 - 8s - loss: 0.9638 - accuracy: 0.6597 - 8s/epoch - 10ms/step\n",
      "Epoch 31/50\n",
      "782/782 - 7s - loss: 0.9629 - accuracy: 0.6602 - 7s/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "782/782 - 5s - loss: 0.9636 - accuracy: 0.6604 - 5s/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "782/782 - 5s - loss: 0.9640 - accuracy: 0.6614 - 5s/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "782/782 - 6s - loss: 0.9635 - accuracy: 0.6580 - 6s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "782/782 - 6s - loss: 0.9637 - accuracy: 0.6604 - 6s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "782/782 - 5s - loss: 0.9634 - accuracy: 0.6594 - 5s/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "782/782 - 5s - loss: 0.9637 - accuracy: 0.6613 - 5s/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "782/782 - 5s - loss: 0.9633 - accuracy: 0.6606 - 5s/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "782/782 - 8s - loss: 0.9639 - accuracy: 0.6586 - 8s/epoch - 10ms/step\n",
      "Epoch 40/50\n",
      "782/782 - 7s - loss: 0.9653 - accuracy: 0.6596 - 7s/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "782/782 - 6s - loss: 0.9628 - accuracy: 0.6617 - 6s/epoch - 8ms/step\n",
      "Epoch 42/50\n",
      "782/782 - 6s - loss: 0.9637 - accuracy: 0.6617 - 6s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "782/782 - 5s - loss: 0.9629 - accuracy: 0.6599 - 5s/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "782/782 - 5s - loss: 0.9630 - accuracy: 0.6601 - 5s/epoch - 6ms/step\n",
      "Epoch 45/50\n",
      "782/782 - 5s - loss: 0.9644 - accuracy: 0.6581 - 5s/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "782/782 - 5s - loss: 0.9644 - accuracy: 0.6596 - 5s/epoch - 6ms/step\n",
      "Epoch 47/50\n",
      "782/782 - 5s - loss: 0.9648 - accuracy: 0.6595 - 5s/epoch - 6ms/step\n",
      "Epoch 48/50\n",
      "782/782 - 7s - loss: 0.9632 - accuracy: 0.6600 - 7s/epoch - 9ms/step\n",
      "Epoch 49/50\n",
      "782/782 - 6s - loss: 0.9644 - accuracy: 0.6594 - 6s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "782/782 - 6s - loss: 0.9637 - accuracy: 0.6605 - 6s/epoch - 7ms/step\n",
      "> layers=20, train=0.663, test=0.533\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unfreeze_softmax_layer(model):\n",
    "    # Freeze all layers except the last one (softmax layer)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "\n",
    "    # Compile the model with only the softmax layer trainable\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the softmax layer\n",
    "    model.fit(trainX, trainy, epochs=50, batch_size=64, verbose=2)\n",
    "\n",
    "unfreeze_softmax_layer(model)\n",
    "\n",
    "# Evaluate the model after unfreezing the softmax layer\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[18] = (train_acc, test_acc)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.8577 - accuracy: 0.3353 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.6804 - accuracy: 0.4012 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 1.6080 - accuracy: 0.4277 - 7s/epoch - 9ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 1.5557 - accuracy: 0.4457 - 6s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 1.5271 - accuracy: 0.4583 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.4920 - accuracy: 0.4672 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 1.4641 - accuracy: 0.4769 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 1.4517 - accuracy: 0.4806 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 1.4310 - accuracy: 0.4903 - 6s/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 1.4139 - accuracy: 0.4973 - 6s/epoch - 8ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 1.4028 - accuracy: 0.4995 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 7s - loss: 1.3932 - accuracy: 0.5053 - 7s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 7s - loss: 1.3803 - accuracy: 0.5062 - 7s/epoch - 9ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 1.3562 - accuracy: 0.5174 - 7s/epoch - 8ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 1.3527 - accuracy: 0.5178 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 7s - loss: 1.3442 - accuracy: 0.5210 - 7s/epoch - 9ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 1.3391 - accuracy: 0.5218 - 6s/epoch - 8ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 1.3458 - accuracy: 0.5207 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 1.3173 - accuracy: 0.5307 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 1.3318 - accuracy: 0.5240 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 7s - loss: 1.3220 - accuracy: 0.5289 - 7s/epoch - 9ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 1.3189 - accuracy: 0.5310 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 1.3167 - accuracy: 0.5306 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 9s - loss: 1.3084 - accuracy: 0.5316 - 9s/epoch - 11ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 8s - loss: 1.2976 - accuracy: 0.5365 - 8s/epoch - 10ms/step\n",
      "> layers=2, train=0.543, test=0.474\n",
      "Epoch 1/25\n",
      "782/782 - 4s - loss: 1.4567 - accuracy: 0.4802 - 4s/epoch - 5ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 3s - loss: 1.2892 - accuracy: 0.5358 - 3s/epoch - 4ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 3s - loss: 1.2537 - accuracy: 0.5492 - 3s/epoch - 4ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 3s - loss: 1.2226 - accuracy: 0.5624 - 3s/epoch - 4ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.2110 - accuracy: 0.5657 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.1965 - accuracy: 0.5743 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 3s - loss: 1.1878 - accuracy: 0.5764 - 3s/epoch - 4ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 3s - loss: 1.1805 - accuracy: 0.5791 - 3s/epoch - 4ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 3s - loss: 1.1724 - accuracy: 0.5824 - 3s/epoch - 4ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 3s - loss: 1.1629 - accuracy: 0.5863 - 3s/epoch - 4ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 3s - loss: 1.1581 - accuracy: 0.5869 - 3s/epoch - 4ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 3s - loss: 1.1517 - accuracy: 0.5898 - 3s/epoch - 4ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 3s - loss: 1.1481 - accuracy: 0.5899 - 3s/epoch - 4ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 3s - loss: 1.1394 - accuracy: 0.5925 - 3s/epoch - 4ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 3s - loss: 1.1383 - accuracy: 0.5951 - 3s/epoch - 4ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 3s - loss: 1.1309 - accuracy: 0.5981 - 3s/epoch - 4ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 3s - loss: 1.1318 - accuracy: 0.5985 - 3s/epoch - 4ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 1.1247 - accuracy: 0.5982 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 3s - loss: 1.1184 - accuracy: 0.6018 - 3s/epoch - 4ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 3s - loss: 1.1127 - accuracy: 0.6027 - 3s/epoch - 4ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 3s - loss: 1.1122 - accuracy: 0.6030 - 3s/epoch - 4ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 3s - loss: 1.1052 - accuracy: 0.6063 - 3s/epoch - 4ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 3s - loss: 1.1032 - accuracy: 0.6082 - 3s/epoch - 4ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.1027 - accuracy: 0.6079 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 3s - loss: 1.0946 - accuracy: 0.6094 - 3s/epoch - 4ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 789,258\n",
      "_________________________________________________________________\n",
      "> layers=3, train=0.619, test=0.509\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.2330 - accuracy: 0.5599 - 5s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.1305 - accuracy: 0.5978 - 4s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.1098 - accuracy: 0.6043 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.1008 - accuracy: 0.6084 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 3s - loss: 1.0897 - accuracy: 0.6132 - 3s/epoch - 4ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 3s - loss: 1.0814 - accuracy: 0.6136 - 3s/epoch - 4ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.0774 - accuracy: 0.6150 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0707 - accuracy: 0.6192 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.0674 - accuracy: 0.6195 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.0648 - accuracy: 0.6204 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 1.0611 - accuracy: 0.6228 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 1.0551 - accuracy: 0.6227 - 4s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 1.0523 - accuracy: 0.6258 - 4s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 1.0510 - accuracy: 0.6244 - 4s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 1.0497 - accuracy: 0.6261 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 1.0448 - accuracy: 0.6260 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 1.0432 - accuracy: 0.6284 - 4s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 1.0392 - accuracy: 0.6292 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 1.0358 - accuracy: 0.6307 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 1.0348 - accuracy: 0.6289 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 1.0315 - accuracy: 0.6300 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 1.0298 - accuracy: 0.6321 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 1.0292 - accuracy: 0.6319 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 1.0267 - accuracy: 0.6332 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 1.0230 - accuracy: 0.6340 - 4s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 920,842\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 855,050\n",
      "_________________________________________________________________\n",
      "> layers=4, train=0.632, test=0.504\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.1479 - accuracy: 0.5938 - 5s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0697 - accuracy: 0.6189 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0574 - accuracy: 0.6225 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.0471 - accuracy: 0.6265 - 4s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.0380 - accuracy: 0.6300 - 4s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0358 - accuracy: 0.6318 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 1.0321 - accuracy: 0.6319 - 4s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 1.0285 - accuracy: 0.6330 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 1.0271 - accuracy: 0.6333 - 4s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 1.0244 - accuracy: 0.6354 - 4s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.0213 - accuracy: 0.6350 - 4s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 1.0188 - accuracy: 0.6345 - 4s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 1.0181 - accuracy: 0.6368 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 1.0149 - accuracy: 0.6382 - 4s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 1.0142 - accuracy: 0.6371 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 1.0114 - accuracy: 0.6396 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 1.0129 - accuracy: 0.6378 - 4s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 4s - loss: 1.0085 - accuracy: 0.6392 - 4s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 1.0096 - accuracy: 0.6393 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 1.0068 - accuracy: 0.6398 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 1.0054 - accuracy: 0.6417 - 4s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 1.0046 - accuracy: 0.6418 - 4s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 1.0021 - accuracy: 0.6404 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9996 - accuracy: 0.6431 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9986 - accuracy: 0.6438 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,634\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 920,842\n",
      "_________________________________________________________________\n",
      "> layers=5, train=0.648, test=0.512\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.1118 - accuracy: 0.6071 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0348 - accuracy: 0.6325 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 1.0279 - accuracy: 0.6333 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0217 - accuracy: 0.6360 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 8s - loss: 1.0170 - accuracy: 0.6370 - 8s/epoch - 10ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 1.0143 - accuracy: 0.6396 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 1.0129 - accuracy: 0.6368 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 1.0103 - accuracy: 0.6389 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 1.0088 - accuracy: 0.6388 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 1.0040 - accuracy: 0.6416 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 1.0044 - accuracy: 0.6404 - 4s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 1.0028 - accuracy: 0.6429 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 1.0014 - accuracy: 0.6430 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 0.9987 - accuracy: 0.6422 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 7s - loss: 0.9992 - accuracy: 0.6434 - 7s/epoch - 9ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9977 - accuracy: 0.6444 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9955 - accuracy: 0.6443 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9956 - accuracy: 0.6433 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9930 - accuracy: 0.6459 - 6s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9922 - accuracy: 0.6438 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9911 - accuracy: 0.6464 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9897 - accuracy: 0.6464 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9899 - accuracy: 0.6464 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9888 - accuracy: 0.6463 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9887 - accuracy: 0.6476 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,052,426\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 986,634\n",
      "_________________________________________________________________\n",
      "> layers=6, train=0.647, test=0.511\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0843 - accuracy: 0.6152 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0223 - accuracy: 0.6373 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0146 - accuracy: 0.6391 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 1.0076 - accuracy: 0.6381 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 1.0047 - accuracy: 0.6410 - 4s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 1.0014 - accuracy: 0.6433 - 4s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9980 - accuracy: 0.6463 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9956 - accuracy: 0.6434 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.9965 - accuracy: 0.6434 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9941 - accuracy: 0.6441 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9924 - accuracy: 0.6439 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9933 - accuracy: 0.6455 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 4s - loss: 0.9903 - accuracy: 0.6474 - 4s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 4s - loss: 0.9894 - accuracy: 0.6467 - 4s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9881 - accuracy: 0.6461 - 4s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9871 - accuracy: 0.6484 - 4s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9870 - accuracy: 0.6476 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9866 - accuracy: 0.6477 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9847 - accuracy: 0.6480 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9848 - accuracy: 0.6473 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9832 - accuracy: 0.6474 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9838 - accuracy: 0.6478 - 4s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9831 - accuracy: 0.6483 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9824 - accuracy: 0.6484 - 4s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9806 - accuracy: 0.6497 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,218\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,052,426\n",
      "_________________________________________________________________\n",
      "> layers=7, train=0.649, test=0.511\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0793 - accuracy: 0.6202 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0198 - accuracy: 0.6384 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0063 - accuracy: 0.6414 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 1.0017 - accuracy: 0.6441 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9982 - accuracy: 0.6439 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9950 - accuracy: 0.6448 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9945 - accuracy: 0.6456 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9897 - accuracy: 0.6471 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 7s - loss: 0.9902 - accuracy: 0.6459 - 7s/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9901 - accuracy: 0.6456 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9884 - accuracy: 0.6469 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9877 - accuracy: 0.6487 - 6s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9863 - accuracy: 0.6488 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9852 - accuracy: 0.6482 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9850 - accuracy: 0.6493 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9838 - accuracy: 0.6481 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9834 - accuracy: 0.6476 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9822 - accuracy: 0.6491 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9824 - accuracy: 0.6475 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9825 - accuracy: 0.6496 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 7s - loss: 0.9809 - accuracy: 0.6506 - 7s/epoch - 9ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9804 - accuracy: 0.6498 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9794 - accuracy: 0.6503 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9788 - accuracy: 0.6502 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9788 - accuracy: 0.6490 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,010\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,118,218\n",
      "_________________________________________________________________\n",
      "> layers=8, train=0.646, test=0.510\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0845 - accuracy: 0.6185 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0103 - accuracy: 0.6401 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 1.0044 - accuracy: 0.6427 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 1.0007 - accuracy: 0.6432 - 6s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9968 - accuracy: 0.6423 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9939 - accuracy: 0.6447 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9932 - accuracy: 0.6450 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9893 - accuracy: 0.6460 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9905 - accuracy: 0.6471 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9893 - accuracy: 0.6466 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9856 - accuracy: 0.6471 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9858 - accuracy: 0.6471 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9855 - accuracy: 0.6476 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9834 - accuracy: 0.6487 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9830 - accuracy: 0.6489 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9820 - accuracy: 0.6490 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9820 - accuracy: 0.6483 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9798 - accuracy: 0.6506 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9792 - accuracy: 0.6492 - 6s/epoch - 8ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9798 - accuracy: 0.6491 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9787 - accuracy: 0.6504 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9774 - accuracy: 0.6496 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9793 - accuracy: 0.6494 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9775 - accuracy: 0.6507 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9764 - accuracy: 0.6506 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,249,802\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,184,010\n",
      "_________________________________________________________________\n",
      "> layers=9, train=0.653, test=0.510\n",
      "Epoch 1/25\n",
      "782/782 - 5s - loss: 1.0704 - accuracy: 0.6247 - 5s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 4s - loss: 1.0115 - accuracy: 0.6418 - 4s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 4s - loss: 1.0028 - accuracy: 0.6435 - 4s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9969 - accuracy: 0.6454 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9916 - accuracy: 0.6464 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9899 - accuracy: 0.6464 - 5s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9883 - accuracy: 0.6491 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9874 - accuracy: 0.6469 - 4s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.9861 - accuracy: 0.6478 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9818 - accuracy: 0.6488 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9824 - accuracy: 0.6479 - 4s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9810 - accuracy: 0.6500 - 4s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9804 - accuracy: 0.6493 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9804 - accuracy: 0.6497 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9785 - accuracy: 0.6492 - 4s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 4s - loss: 0.9774 - accuracy: 0.6501 - 4s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 4s - loss: 0.9776 - accuracy: 0.6501 - 4s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9764 - accuracy: 0.6501 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9766 - accuracy: 0.6506 - 4s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9760 - accuracy: 0.6516 - 4s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 4s - loss: 0.9754 - accuracy: 0.6510 - 4s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9739 - accuracy: 0.6504 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9743 - accuracy: 0.6507 - 4s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9729 - accuracy: 0.6516 - 4s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 4s - loss: 0.9732 - accuracy: 0.6500 - 4s/epoch - 5ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315,594\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,249,802\n",
      "_________________________________________________________________\n",
      "> layers=10, train=0.651, test=0.506\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0580 - accuracy: 0.6248 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 1.0042 - accuracy: 0.6424 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9975 - accuracy: 0.6435 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 4s - loss: 0.9891 - accuracy: 0.6461 - 4s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 4s - loss: 0.9871 - accuracy: 0.6465 - 4s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 4s - loss: 0.9857 - accuracy: 0.6497 - 4s/epoch - 6ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 4s - loss: 0.9840 - accuracy: 0.6474 - 4s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 4s - loss: 0.9825 - accuracy: 0.6479 - 4s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9830 - accuracy: 0.6487 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9805 - accuracy: 0.6486 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9792 - accuracy: 0.6493 - 4s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9790 - accuracy: 0.6501 - 4s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9773 - accuracy: 0.6495 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9760 - accuracy: 0.6504 - 5s/epoch - 6ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 4s - loss: 0.9757 - accuracy: 0.6499 - 4s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9769 - accuracy: 0.6500 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9746 - accuracy: 0.6519 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9760 - accuracy: 0.6509 - 6s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 4s - loss: 0.9734 - accuracy: 0.6497 - 4s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 4s - loss: 0.9729 - accuracy: 0.6513 - 4s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9722 - accuracy: 0.6511 - 5s/epoch - 6ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 4s - loss: 0.9714 - accuracy: 0.6502 - 4s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 4s - loss: 0.9724 - accuracy: 0.6505 - 4s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 4s - loss: 0.9701 - accuracy: 0.6518 - 4s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9708 - accuracy: 0.6508 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,381,386\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,315,594\n",
      "_________________________________________________________________\n",
      "> layers=11, train=0.652, test=0.510\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0583 - accuracy: 0.6241 - 6s/epoch - 7ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 1.0038 - accuracy: 0.6428 - 5s/epoch - 6ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9952 - accuracy: 0.6436 - 5s/epoch - 6ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9897 - accuracy: 0.6467 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9871 - accuracy: 0.6456 - 5s/epoch - 6ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9870 - accuracy: 0.6473 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9837 - accuracy: 0.6489 - 5s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9809 - accuracy: 0.6483 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 4s - loss: 0.9820 - accuracy: 0.6506 - 4s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 4s - loss: 0.9797 - accuracy: 0.6501 - 4s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 4s - loss: 0.9803 - accuracy: 0.6488 - 4s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 4s - loss: 0.9784 - accuracy: 0.6485 - 4s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9796 - accuracy: 0.6487 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9774 - accuracy: 0.6494 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9758 - accuracy: 0.6515 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9757 - accuracy: 0.6506 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9763 - accuracy: 0.6495 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9737 - accuracy: 0.6515 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9736 - accuracy: 0.6514 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9723 - accuracy: 0.6506 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9727 - accuracy: 0.6519 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9720 - accuracy: 0.6503 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9713 - accuracy: 0.6512 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 0.9716 - accuracy: 0.6514 - 6s/epoch - 8ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9715 - accuracy: 0.6509 - 6s/epoch - 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,447,178\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,381,386\n",
      "_________________________________________________________________\n",
      "> layers=12, train=0.654, test=0.506\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0473 - accuracy: 0.6307 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9999 - accuracy: 0.6445 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9911 - accuracy: 0.6454 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9864 - accuracy: 0.6485 - 6s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9843 - accuracy: 0.6471 - 6s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9825 - accuracy: 0.6491 - 6s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9804 - accuracy: 0.6504 - 6s/epoch - 8ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9802 - accuracy: 0.6484 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9776 - accuracy: 0.6507 - 5s/epoch - 6ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9767 - accuracy: 0.6487 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9759 - accuracy: 0.6512 - 5s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9763 - accuracy: 0.6493 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9738 - accuracy: 0.6513 - 5s/epoch - 6ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9741 - accuracy: 0.6504 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9735 - accuracy: 0.6513 - 5s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9713 - accuracy: 0.6507 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9718 - accuracy: 0.6502 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9709 - accuracy: 0.6493 - 5s/epoch - 6ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9713 - accuracy: 0.6520 - 5s/epoch - 6ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9684 - accuracy: 0.6516 - 5s/epoch - 6ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9698 - accuracy: 0.6521 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9686 - accuracy: 0.6516 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9691 - accuracy: 0.6518 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9691 - accuracy: 0.6523 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9672 - accuracy: 0.6506 - 5s/epoch - 6ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,512,970\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,447,178\n",
      "_________________________________________________________________\n",
      "> layers=13, train=0.651, test=0.507\n",
      "Epoch 1/25\n",
      "782/782 - 6s - loss: 1.0691 - accuracy: 0.6238 - 6s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9961 - accuracy: 0.6459 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9895 - accuracy: 0.6483 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9871 - accuracy: 0.6456 - 5s/epoch - 6ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9840 - accuracy: 0.6478 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9806 - accuracy: 0.6492 - 6s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9796 - accuracy: 0.6499 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9773 - accuracy: 0.6488 - 5s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9755 - accuracy: 0.6516 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9748 - accuracy: 0.6508 - 5s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9733 - accuracy: 0.6513 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9732 - accuracy: 0.6499 - 5s/epoch - 6ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9721 - accuracy: 0.6510 - 6s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9705 - accuracy: 0.6513 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9717 - accuracy: 0.6512 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9692 - accuracy: 0.6512 - 5s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9691 - accuracy: 0.6535 - 5s/epoch - 6ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9696 - accuracy: 0.6513 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9690 - accuracy: 0.6515 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9675 - accuracy: 0.6526 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9670 - accuracy: 0.6509 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9655 - accuracy: 0.6535 - 5s/epoch - 6ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9660 - accuracy: 0.6529 - 5s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9656 - accuracy: 0.6523 - 5s/epoch - 6ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9654 - accuracy: 0.6536 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,578,762\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,512,970\n",
      "_________________________________________________________________\n",
      "> layers=14, train=0.649, test=0.505\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0524 - accuracy: 0.6292 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 5s - loss: 0.9961 - accuracy: 0.6445 - 5s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 5s - loss: 0.9877 - accuracy: 0.6484 - 5s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 5s - loss: 0.9854 - accuracy: 0.6465 - 5s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 5s - loss: 0.9829 - accuracy: 0.6487 - 5s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9792 - accuracy: 0.6500 - 6s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9783 - accuracy: 0.6482 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 5s - loss: 0.9792 - accuracy: 0.6498 - 5s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 5s - loss: 0.9749 - accuracy: 0.6504 - 5s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9726 - accuracy: 0.6501 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 5s - loss: 0.9734 - accuracy: 0.6497 - 5s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 5s - loss: 0.9723 - accuracy: 0.6519 - 5s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9719 - accuracy: 0.6505 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9696 - accuracy: 0.6518 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9692 - accuracy: 0.6526 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9689 - accuracy: 0.6503 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 5s - loss: 0.9678 - accuracy: 0.6525 - 5s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9674 - accuracy: 0.6525 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9679 - accuracy: 0.6525 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9669 - accuracy: 0.6539 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 7s - loss: 0.9681 - accuracy: 0.6528 - 7s/epoch - 9ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9669 - accuracy: 0.6535 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9656 - accuracy: 0.6520 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9660 - accuracy: 0.6528 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9646 - accuracy: 0.6542 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,554\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,578,762\n",
      "_________________________________________________________________\n",
      "> layers=15, train=0.652, test=0.508\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0649 - accuracy: 0.6273 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 1.0005 - accuracy: 0.6457 - 7s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9931 - accuracy: 0.6467 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9873 - accuracy: 0.6486 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9831 - accuracy: 0.6479 - 6s/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9812 - accuracy: 0.6501 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 5s - loss: 0.9800 - accuracy: 0.6503 - 5s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9775 - accuracy: 0.6492 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9768 - accuracy: 0.6498 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9766 - accuracy: 0.6504 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9746 - accuracy: 0.6492 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9731 - accuracy: 0.6514 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9735 - accuracy: 0.6508 - 6s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 5s - loss: 0.9727 - accuracy: 0.6510 - 5s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 5s - loss: 0.9707 - accuracy: 0.6518 - 5s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9708 - accuracy: 0.6519 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9709 - accuracy: 0.6515 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9688 - accuracy: 0.6528 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 6s - loss: 0.9683 - accuracy: 0.6524 - 6s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9687 - accuracy: 0.6529 - 6s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 5s - loss: 0.9688 - accuracy: 0.6522 - 5s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9676 - accuracy: 0.6517 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9667 - accuracy: 0.6522 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9668 - accuracy: 0.6521 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 7s - loss: 0.9654 - accuracy: 0.6536 - 7s/epoch - 9ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,346\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,644,554\n",
      "_________________________________________________________________\n",
      "> layers=16, train=0.652, test=0.506\n",
      "Epoch 1/25\n",
      "782/782 - 8s - loss: 1.0795 - accuracy: 0.6280 - 8s/epoch - 11ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 0.9930 - accuracy: 0.6450 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 7s - loss: 0.9890 - accuracy: 0.6466 - 7s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9842 - accuracy: 0.6470 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9816 - accuracy: 0.6497 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 5s - loss: 0.9797 - accuracy: 0.6482 - 5s/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9771 - accuracy: 0.6500 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 0.9756 - accuracy: 0.6506 - 7s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9757 - accuracy: 0.6506 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9734 - accuracy: 0.6511 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9724 - accuracy: 0.6508 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9721 - accuracy: 0.6523 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9715 - accuracy: 0.6514 - 6s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9719 - accuracy: 0.6510 - 6s/epoch - 8ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9695 - accuracy: 0.6520 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9683 - accuracy: 0.6524 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9683 - accuracy: 0.6515 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 6s - loss: 0.9695 - accuracy: 0.6529 - 6s/epoch - 8ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 0.9685 - accuracy: 0.6532 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 6s - loss: 0.9671 - accuracy: 0.6521 - 6s/epoch - 8ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9681 - accuracy: 0.6525 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9661 - accuracy: 0.6533 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9663 - accuracy: 0.6528 - 6s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9663 - accuracy: 0.6519 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9665 - accuracy: 0.6515 - 6s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,776,138\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,710,346\n",
      "_________________________________________________________________\n",
      "> layers=17, train=0.656, test=0.506\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0628 - accuracy: 0.6272 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9972 - accuracy: 0.6463 - 6s/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9924 - accuracy: 0.6459 - 6s/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9846 - accuracy: 0.6477 - 6s/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9811 - accuracy: 0.6483 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 7s - loss: 0.9806 - accuracy: 0.6484 - 7s/epoch - 9ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9769 - accuracy: 0.6498 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9761 - accuracy: 0.6497 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9758 - accuracy: 0.6500 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9732 - accuracy: 0.6513 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9727 - accuracy: 0.6522 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 7s - loss: 0.9718 - accuracy: 0.6511 - 7s/epoch - 8ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 6s - loss: 0.9715 - accuracy: 0.6501 - 6s/epoch - 8ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9698 - accuracy: 0.6520 - 6s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9697 - accuracy: 0.6528 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9692 - accuracy: 0.6517 - 6s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 8s - loss: 0.9692 - accuracy: 0.6522 - 8s/epoch - 10ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 9s - loss: 0.9683 - accuracy: 0.6538 - 9s/epoch - 11ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 8s - loss: 0.9682 - accuracy: 0.6525 - 8s/epoch - 10ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 0.9677 - accuracy: 0.6535 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9649 - accuracy: 0.6544 - 6s/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9671 - accuracy: 0.6522 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9658 - accuracy: 0.6527 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 7s - loss: 0.9659 - accuracy: 0.6527 - 7s/epoch - 9ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9654 - accuracy: 0.6524 - 6s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,841,930\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,776,138\n",
      "_________________________________________________________________\n",
      "> layers=18, train=0.653, test=0.505\n",
      "Epoch 1/25\n",
      "782/782 - 9s - loss: 1.0500 - accuracy: 0.6302 - 9s/epoch - 12ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 7s - loss: 0.9927 - accuracy: 0.6451 - 7s/epoch - 9ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9858 - accuracy: 0.6480 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9829 - accuracy: 0.6462 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9805 - accuracy: 0.6503 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9775 - accuracy: 0.6491 - 6s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 8s - loss: 0.9763 - accuracy: 0.6497 - 8s/epoch - 10ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 7s - loss: 0.9766 - accuracy: 0.6513 - 7s/epoch - 8ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9752 - accuracy: 0.6511 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 5s - loss: 0.9726 - accuracy: 0.6516 - 5s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9718 - accuracy: 0.6516 - 6s/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 6s - loss: 0.9718 - accuracy: 0.6525 - 6s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 5s - loss: 0.9708 - accuracy: 0.6520 - 5s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 6s - loss: 0.9688 - accuracy: 0.6526 - 6s/epoch - 8ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9679 - accuracy: 0.6531 - 6s/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 5s - loss: 0.9686 - accuracy: 0.6526 - 5s/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 6s - loss: 0.9679 - accuracy: 0.6537 - 6s/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 5s - loss: 0.9672 - accuracy: 0.6524 - 5s/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 5s - loss: 0.9665 - accuracy: 0.6523 - 5s/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 5s - loss: 0.9669 - accuracy: 0.6540 - 5s/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9656 - accuracy: 0.6542 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 5s - loss: 0.9665 - accuracy: 0.6544 - 5s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 5s - loss: 0.9655 - accuracy: 0.6520 - 5s/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 5s - loss: 0.9652 - accuracy: 0.6528 - 5s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 5s - loss: 0.9650 - accuracy: 0.6533 - 5s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,722\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,841,930\n",
      "_________________________________________________________________\n",
      "> layers=19, train=0.656, test=0.505\n",
      "Epoch 1/25\n",
      "782/782 - 7s - loss: 1.0623 - accuracy: 0.6245 - 7s/epoch - 9ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 6s - loss: 0.9956 - accuracy: 0.6439 - 6s/epoch - 8ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 6s - loss: 0.9903 - accuracy: 0.6472 - 6s/epoch - 8ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 6s - loss: 0.9834 - accuracy: 0.6478 - 6s/epoch - 8ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 6s - loss: 0.9799 - accuracy: 0.6485 - 6s/epoch - 8ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 6s - loss: 0.9784 - accuracy: 0.6496 - 6s/epoch - 8ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 6s - loss: 0.9774 - accuracy: 0.6494 - 6s/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 6s - loss: 0.9747 - accuracy: 0.6509 - 6s/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 6s - loss: 0.9759 - accuracy: 0.6502 - 6s/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 6s - loss: 0.9727 - accuracy: 0.6505 - 6s/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 6s - loss: 0.9719 - accuracy: 0.6521 - 6s/epoch - 8ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 8s - loss: 0.9718 - accuracy: 0.6511 - 8s/epoch - 10ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 8s - loss: 0.9717 - accuracy: 0.6521 - 8s/epoch - 10ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 7s - loss: 0.9689 - accuracy: 0.6516 - 7s/epoch - 9ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 6s - loss: 0.9694 - accuracy: 0.6526 - 6s/epoch - 8ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 6s - loss: 0.9680 - accuracy: 0.6526 - 6s/epoch - 8ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 7s - loss: 0.9686 - accuracy: 0.6527 - 7s/epoch - 9ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 7s - loss: 0.9674 - accuracy: 0.6523 - 7s/epoch - 9ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 7s - loss: 0.9661 - accuracy: 0.6525 - 7s/epoch - 9ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 7s - loss: 0.9661 - accuracy: 0.6535 - 7s/epoch - 9ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 6s - loss: 0.9634 - accuracy: 0.6532 - 6s/epoch - 8ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 6s - loss: 0.9646 - accuracy: 0.6521 - 6s/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 6s - loss: 0.9653 - accuracy: 0.6521 - 6s/epoch - 8ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 6s - loss: 0.9633 - accuracy: 0.6532 - 6s/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 6s - loss: 0.9634 - accuracy: 0.6548 - 6s/epoch - 7ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,973,514\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 1,907,722\n",
      "_________________________________________________________________\n",
      "> layers=20, train=0.655, test=0.505\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "testX = testX.reshape(-1, 32 * 32 * 3) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=32*32*3, activation='tanh', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "# evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    " _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    " _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    " return train_acc, test_acc\n",
    "# Add one new layer and re-train only the new layer\n",
    "# Add one new layer and re-train only the new layer while keeping the last layer frozen\n",
    "def add_layer(model, trainX, trainy):\n",
    "    output_layer = model.layers[-1]\n",
    "    model.pop()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(Dense(256, activation='tanh', kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Set the last layer to be non-trainable\n",
    "    output_layer.trainable = False\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    # Compile the model before training\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "    # Train only the new layer\n",
    "    model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add layers and evaluate the updated model\n",
    "n_layers = 18\n",
    "\n",
    "for i in range(n_layers):\n",
    "    add_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> layers=4, train=0.107, test=0.103\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2d_22\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m n_layers \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# add layer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     add_layer(model, trainX, trainy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39m# evaluate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     train_acc, test_acc \u001b[39m=\u001b[39m evaluate_model(model, trainX, testX, trainy, testy)\n",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 12\u001b[0m line \u001b[0;36madd_layer\u001b[1;34m(model, trainX, trainy)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     layer\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# add a new convolutional layer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m model\u001b[39m.\u001b[39;49madd(Conv2D(\u001b[39m32\u001b[39;49m, (\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m), activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, kernel_initializer\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhe_uniform\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m(\u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m3\u001b[39;49m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m model\u001b[39m.\u001b[39madd(Flatten())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# re-add the output layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py:253\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    251\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 253\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m         )\n\u001b[0;32m    260\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv2d_22\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 10)"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "trainX, testX = trainX / 255.0, testX / 255.0\n",
    "\n",
    "# One-hot encode output variable\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# add one new layer and re-train only the new layer\n",
    "def add_layer(model, trainX, trainy):\n",
    "    # remember the current output layer\n",
    "    output_layer = model.layers[-1]\n",
    "    # remove the output layer\n",
    "    model.pop()\n",
    "    # mark all remaining layers as non-trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add a new convolutional layer\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
    "    model.add(Flatten())\n",
    "    # re-add the output layer\n",
    "    model.add(output_layer)\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=10, verbose=0)  # Using fewer epochs for illustration purposes\n",
    "\n",
    "# get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# add layers and evaluate the updated model\n",
    "n_layers = 3\n",
    "for i in range(n_layers):\n",
    "    # add layer\n",
    "    add_layer(model, trainX, trainy)\n",
    "    # evaluate model\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    # store scores for plotting\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# plot number of added layers vs accuracy\n",
    "pyplot.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "pyplot.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 - 35s - loss: 1.5608 - accuracy: 0.4488 - 35s/epoch - 44ms/step\n",
      "Epoch 2/25\n",
      "782/782 - 34s - loss: 1.2404 - accuracy: 0.5642 - 34s/epoch - 43ms/step\n",
      "Epoch 3/25\n",
      "782/782 - 35s - loss: 1.1205 - accuracy: 0.6072 - 35s/epoch - 45ms/step\n",
      "Epoch 4/25\n",
      "782/782 - 36s - loss: 1.0105 - accuracy: 0.6484 - 36s/epoch - 46ms/step\n",
      "Epoch 5/25\n",
      "782/782 - 36s - loss: 0.9182 - accuracy: 0.6840 - 36s/epoch - 47ms/step\n",
      "Epoch 6/25\n",
      "782/782 - 37s - loss: 0.8452 - accuracy: 0.7127 - 37s/epoch - 47ms/step\n",
      "Epoch 7/25\n",
      "782/782 - 38s - loss: 0.7737 - accuracy: 0.7421 - 38s/epoch - 49ms/step\n",
      "Epoch 8/25\n",
      "782/782 - 38s - loss: 0.7140 - accuracy: 0.7656 - 38s/epoch - 49ms/step\n",
      "Epoch 9/25\n",
      "782/782 - 38s - loss: 0.6564 - accuracy: 0.7924 - 38s/epoch - 48ms/step\n",
      "Epoch 10/25\n",
      "782/782 - 40s - loss: 0.6085 - accuracy: 0.8147 - 40s/epoch - 51ms/step\n",
      "Epoch 11/25\n",
      "782/782 - 42s - loss: 0.5611 - accuracy: 0.8354 - 42s/epoch - 54ms/step\n",
      "Epoch 12/25\n",
      "782/782 - 40s - loss: 0.5236 - accuracy: 0.8525 - 40s/epoch - 51ms/step\n",
      "Epoch 13/25\n",
      "782/782 - 40s - loss: 0.4862 - accuracy: 0.8724 - 40s/epoch - 51ms/step\n",
      "Epoch 14/25\n",
      "782/782 - 39s - loss: 0.4530 - accuracy: 0.8874 - 39s/epoch - 50ms/step\n",
      "Epoch 15/25\n",
      "782/782 - 39s - loss: 0.4228 - accuracy: 0.9026 - 39s/epoch - 49ms/step\n",
      "Epoch 16/25\n",
      "782/782 - 39s - loss: 0.3983 - accuracy: 0.9126 - 39s/epoch - 50ms/step\n",
      "Epoch 17/25\n",
      "782/782 - 38s - loss: 0.3723 - accuracy: 0.9250 - 38s/epoch - 49ms/step\n",
      "Epoch 18/25\n",
      "782/782 - 40s - loss: 0.3513 - accuracy: 0.9342 - 40s/epoch - 51ms/step\n",
      "Epoch 19/25\n",
      "782/782 - 39s - loss: 0.3301 - accuracy: 0.9433 - 39s/epoch - 49ms/step\n",
      "Epoch 20/25\n",
      "782/782 - 38s - loss: 0.3129 - accuracy: 0.9502 - 38s/epoch - 48ms/step\n",
      "Epoch 21/25\n",
      "782/782 - 38s - loss: 0.2950 - accuracy: 0.9570 - 38s/epoch - 48ms/step\n",
      "Epoch 22/25\n",
      "782/782 - 38s - loss: 0.2805 - accuracy: 0.9607 - 38s/epoch - 48ms/step\n",
      "Epoch 23/25\n",
      "782/782 - 38s - loss: 0.2660 - accuracy: 0.9673 - 38s/epoch - 48ms/step\n",
      "Epoch 24/25\n",
      "782/782 - 38s - loss: 0.2518 - accuracy: 0.9720 - 38s/epoch - 49ms/step\n",
      "Epoch 25/25\n",
      "782/782 - 39s - loss: 0.2403 - accuracy: 0.9748 - 39s/epoch - 50ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Evaluate the base model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m train_acc, test_acc \u001b[39m=\u001b[39m evaluate_model(model, trainX, testX, trainy, testy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m> layers=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, train=\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m, test=\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mlayers), train_acc, test_acc))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m scores[\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mlayers)] \u001b[39m=\u001b[39m (train_acc, test_acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "# from keras.optimizers import SGD\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the CIFAR-10 dataset\n",
    "# (trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# # Reshape and normalize the input data\n",
    "# trainX = trainX / 255.0\n",
    "# testX = testX / 255.0\n",
    "\n",
    "# # One-hot encode the target labels\n",
    "# trainy = to_categorical(trainy)\n",
    "# testy = to_categorical(testy)\n",
    "\n",
    "# # Define and fit the base model\n",
    "# def get_base_model(trainX, trainy):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='tanh', kernel_initializer='he_uniform'))\n",
    "#     model.add(Dense(10, activation='softmax', trainable = False))\n",
    "\n",
    "#     opt = SGD(lr=0.01, momentum=0.9)\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#     model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Add convolutional layers and evaluate the updated model\n",
    "# def add_convolutional_layers(model, trainX, trainy, testX, testy, scores):\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "#     history = model.fit(trainX, trainy, epochs=25, batch_size=64, verbose=2)\n",
    "\n",
    "#     # Plot the accuracy for each layer\n",
    "#     plt.plot(history.history['accuracy'], label=f'train (Layer {len(model.layers)})', marker='.')\n",
    "#     plt.plot([len(model.layers)], history.history['accuracy'][-1], marker='o')\n",
    "#     scores[len(model.layers)] = (history.history['accuracy'][-1], model.evaluate(testX, testy, verbose=0)[1])\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# # Get the base model\n",
    "# model = get_base_model(trainX, trainy)\n",
    "\n",
    "# # Evaluate the base model\n",
    "# scores = dict()\n",
    "# train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "# print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "# scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# # Add convolutional layers and evaluate the updated model\n",
    "# n_layers = 3  # You can change this number based on your requirements\n",
    "\n",
    "# for i in range(n_layers):\n",
    "#     add_convolutional_layers(model, trainX, trainy, testX, testy, scores)\n",
    "#     model.summary()\n",
    "#     print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "\n",
    "# # Plot the overall accuracy graph\n",
    "# plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "# plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "# plt.xlabel('Number of Layers')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Reshape the input data to have 4 dimensions\n",
    "trainX = trainX.reshape(-1, 32, 32, 3)\n",
    "testX = testX.reshape(-1, 32, 32, 3)\n",
    "\n",
    "# # Define and fit the base model\n",
    "# def get_base_model(trainX, trainy):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='tanh', kernel_initializer='he_uniform'))\n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#     opt = SGD(lr=0.01, momentum=0.9)\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#     model.fit(trainX, trainy, epochs=1, batch_size=64, verbose=2)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Add one new convolutional layer and re-train only the new layer\n",
    "# def add_conv_layer(base_model, trainX, trainy, scores):\n",
    "#     model = Sequential()\n",
    "#     for layer in base_model.layers[:-1]:  # Exclude the last layer\n",
    "#         model.add(layer)\n",
    "    \n",
    "#     # Flatten the output of the Dense layer\n",
    "#     model.add(Flatten())\n",
    "    \n",
    "#     # Reshape to match the input shape of the Conv2D layer\n",
    "#     model.add(Reshape((16, 16, 2)))  # Adjust the shape based on your needs\n",
    "    \n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='tanh', kernel_initializer='he_uniform'))\n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#     # Compile the model before training\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "#     # Train only the new convolutional layer\n",
    "#     history = model.fit(trainX, trainy, epochs=1, batch_size=64, verbose=2)\n",
    "    \n",
    "#     # Record the accuracy for the added layer\n",
    "#     scores[len(model.layers)] = history.history['accuracy']\n",
    "\n",
    "# # Get the base model\n",
    "# base_model = get_base_model(trainX, trainy)\n",
    "\n",
    "# # Evaluate the base model\n",
    "# scores = dict()\n",
    "# train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy, testy)\n",
    "# print('> layers=%d, train=%.3f, test=%.3f' % (len(base_model.layers), train_acc, test_acc))\n",
    "# scores[len(base_model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# # Add convolutional layers and evaluate the updated model\n",
    "# n_layers = 19  # You can adjust the number of convolutional layers\n",
    "\n",
    "# for i in range(n_layers):\n",
    "#     add_conv_layer(base_model, trainX, trainy, scores)\n",
    "#     train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy, testy)\n",
    "#     base_model.summary()\n",
    "#     print('> layers=%d, train=%.3f, test=%.3f' % (len(base_model.layers), train_acc, test_acc))\n",
    "#     scores[len(base_model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# # Plot accuracies for each layer\n",
    "# for layer, acc in scores.items():\n",
    "#     plt.plot(range(1, 26), acc, label=f'Layer {layer}')\n",
    "\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.4772 - accuracy: 0.4810 - val_loss: 1.3135 - val_accuracy: 0.5362\n",
      "Epoch 2/25\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1932 - accuracy: 0.5865 - val_loss: 1.2820 - val_accuracy: 0.5527\n",
      "Epoch 3/25\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0644 - accuracy: 0.6340 - val_loss: 1.1773 - val_accuracy: 0.5945\n",
      "Epoch 4/25\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.9710 - accuracy: 0.6652 - val_loss: 1.2305 - val_accuracy: 0.5869\n",
      "Epoch 5/25\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.8949 - accuracy: 0.6926 - val_loss: 1.2124 - val_accuracy: 0.5873\n",
      "Epoch 6/25\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8362 - accuracy: 0.7127 - val_loss: 1.2246 - val_accuracy: 0.5911\n",
      "Epoch 7/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7724 - accuracy: 0.7340 - val_loss: 1.2410 - val_accuracy: 0.5955\n",
      "Epoch 8/25\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7170 - accuracy: 0.7561 - val_loss: 1.2353 - val_accuracy: 0.5979\n",
      "Epoch 9/25\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6668 - accuracy: 0.7719 - val_loss: 1.3037 - val_accuracy: 0.5839\n",
      "Epoch 10/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.6217 - accuracy: 0.7885 - val_loss: 1.3562 - val_accuracy: 0.5779\n",
      "Epoch 11/25\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5739 - accuracy: 0.8057 - val_loss: 1.3964 - val_accuracy: 0.5810\n",
      "Epoch 12/25\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5336 - accuracy: 0.8192 - val_loss: 1.4423 - val_accuracy: 0.5771\n",
      "Epoch 13/25\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.5000 - accuracy: 0.8310 - val_loss: 1.4880 - val_accuracy: 0.5782\n",
      "Epoch 14/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.4621 - accuracy: 0.8449 - val_loss: 1.4934 - val_accuracy: 0.5820\n",
      "Epoch 15/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.4295 - accuracy: 0.8563 - val_loss: 1.5917 - val_accuracy: 0.5744\n",
      "Epoch 16/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.4014 - accuracy: 0.8658 - val_loss: 1.6119 - val_accuracy: 0.5795\n",
      "Epoch 17/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.3689 - accuracy: 0.8768 - val_loss: 1.6643 - val_accuracy: 0.5729\n",
      "Epoch 18/25\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.3439 - accuracy: 0.8865 - val_loss: 1.7564 - val_accuracy: 0.5682\n",
      "Epoch 19/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.3182 - accuracy: 0.8956 - val_loss: 1.8258 - val_accuracy: 0.5622\n",
      "Epoch 20/25\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.2933 - accuracy: 0.9046 - val_loss: 1.8659 - val_accuracy: 0.5672\n",
      "Epoch 21/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2712 - accuracy: 0.9119 - val_loss: 1.9302 - val_accuracy: 0.5599\n",
      "Epoch 22/25\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2529 - accuracy: 0.9192 - val_loss: 1.9982 - val_accuracy: 0.5628\n",
      "Epoch 23/25\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2371 - accuracy: 0.9235 - val_loss: 2.0533 - val_accuracy: 0.5649\n",
      "Epoch 24/25\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2168 - accuracy: 0.9320 - val_loss: 2.1189 - val_accuracy: 0.5685\n",
      "Epoch 25/25\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.2081 - accuracy: 0.9338 - val_loss: 2.1869 - val_accuracy: 0.5671\n",
      "\n",
      "Model summary before training added layer 1:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                250890    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,034\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 251,786\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model summary after training added layer 1:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                250890    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,034\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 251,786\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary before training added layer 2:\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 21632)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                216330    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235,722\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 226,474\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary after training added layer 2:\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 21632)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                216330    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235,722\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 226,474\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary before training added layer 3:\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                184330    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 212,970\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 203,722\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary after training added layer 3:\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                184330    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 212,970\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 203,722\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary before training added layer 4:\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 22, 22, 32)        9248      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 15488)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                154890    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192,778\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 183,530\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary after training added layer 4:\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 22, 22, 32)        9248      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 15488)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                154890    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192,778\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 183,530\n",
      "_________________________________________________________________\n",
      "\n",
      "Model summary before training added layer 5:\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 22, 22, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 20, 20, 32)        9248      \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                128010    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,146\n",
      "Trainable params: 9,248\n",
      "Non-trainable params: 165,898\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n",
    "\n",
    "# Build the initial model without Dense and Flatten layers\n",
    "def build_initial_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Function to add a Conv2D layer to the model\n",
    "def add_conv_layer(model):\n",
    "    # Create a new model excluding the last two layers\n",
    "    new_model = models.Sequential()\n",
    "    for layer in model.layers[:-2]:\n",
    "        new_model.add(layer)\n",
    "\n",
    "    # Add a new Conv2D layer\n",
    "    new_model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    # Skip MaxPooling2D to avoid negative dimensions\n",
    "    # new_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Re-add Flatten layer\n",
    "    new_model.add(layers.Flatten())\n",
    "\n",
    "    # Create a new Dense layer with the same configuration\n",
    "    dense_layer = layers.Dense(10, activation='softmax', kernel_initializer=model.layers[-1].kernel_initializer)\n",
    "    \n",
    "    # Add the new Dense layer\n",
    "    new_model.add(dense_layer)\n",
    "\n",
    "    # Freeze the Dense layer\n",
    "    new_model.layers[-1].trainable = False\n",
    "\n",
    "    return new_model\n",
    "\n",
    "# Greedy layerwise training\n",
    "def greedy_layerwise_training(model, num_layers_to_add, train_base=True):\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_layers_to_add):\n",
    "        # Freeze existing layers\n",
    "        for layer in model.layers[:-2]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add a new Conv2D layer and re-add Flatten layer\n",
    "        model = add_conv_layer(model)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Display model summary before and after adding the layer\n",
    "        print(f'\\nModel summary before training added layer {i + 1}:\\n')\n",
    "        model.summary()\n",
    "\n",
    "        if train_base:\n",
    "            # Train the model on the CIFAR-10 dataset for one epoch\n",
    "            history = model.fit(train_images, train_labels, epochs=1, validation_data=(test_images, test_labels), verbose=0)\n",
    "            accuracies.append(history.history['accuracy'][0])\n",
    "\n",
    "        # Unfreeze the last added Conv2D layer for training\n",
    "        model.layers[-2].trainable = True\n",
    "\n",
    "        # Train only the newly added Conv2D layer and collect accuracy\n",
    "        history = model.fit(train_images, train_labels, epochs=25, batch_size=32, validation_data=(test_images, test_labels), verbose=0)\n",
    "        accuracies.append(history.history['accuracy'][0])\n",
    "\n",
    "        # Display model summary after training the added layer\n",
    "        print(f'\\nModel summary after training added layer {i + 1}:\\n')\n",
    "        model.summary()\n",
    "\n",
    "    return model, accuracies\n",
    "\n",
    "# Build the initial model\n",
    "initial_model = build_initial_model()\n",
    "\n",
    "# Train the base model\n",
    "initial_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "initial_model.fit(train_images, train_labels, epochs=25, batch_size=32, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Perform greedy layerwise training by adding convolutional layers\n",
    "final_model, layer_accuracies = greedy_layerwise_training(initial_model, num_layers_to_add=14, train_base=False)\n",
    "\n",
    "# Evaluate the final model\n",
    "test_loss, test_acc = final_model.evaluate(test_images, test_labels)\n",
    "print(f'\\nFinal model test accuracy: {test_acc}')\n",
    "\n",
    "# Save accuracies to a file\n",
    "with open('layer_accuracies.txt', 'w') as file:\n",
    "    for accuracy in layer_accuracies:\n",
    "        file.write(f'{accuracy}\\n')\n",
    "\n",
    "# Plot accuracy vs layer added\n",
    "plt.plot(range(1, len(layer_accuracies) + 1), layer_accuracies, marker='o')\n",
    "plt.title('Accuracy vs Layers Added')\n",
    "plt.xlabel('Number of Layers Added')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 45s 28ms/step - loss: 1.4881 - accuracy: 0.4783 - val_loss: 1.3039 - val_accuracy: 0.5419\n",
      "\n",
      "Model summary before training added layer 1:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                250890    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,034\n",
      "Trainable params: 260,138\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 16\u001b[0m line \u001b[0;36m<cell line: 88>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m initial_model\u001b[39m.\u001b[39mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, validation_data\u001b[39m=\u001b[39m(test_images, test_labels))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Perform greedy layerwise training by adding convolutional layers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m final_model, layer_accuracies \u001b[39m=\u001b[39m greedy_layerwise_training(initial_model, num_layers_to_add\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, train_base\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Evaluate the final model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m final_model\u001b[39m.\u001b[39mevaluate(test_images, test_labels)\n",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 16\u001b[0m line \u001b[0;36mgreedy_layerwise_training\u001b[1;34m(model, num_layers_to_add, train_base)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# Train only the newly added Conv2D layer and collect accuracy\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(test_images, test_labels), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m accuracies\u001b[39m.\u001b[39mappend(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Display model summary after training the added layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n",
    "\n",
    "# Build the initial model without Dense and Flatten layers\n",
    "def build_initial_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Function to add a Conv2D layer to the model\n",
    "def add_conv_layer(model):\n",
    "    # Create a new model excluding the last two layers\n",
    "    new_model = models.Sequential()\n",
    "    for layer in model.layers[:-2]:\n",
    "        new_model.add(layer)\n",
    "\n",
    "    # Add a new Conv2D layer\n",
    "    new_model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    # Skip MaxPooling2D to avoid negative dimensions\n",
    "    # new_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Re-add Flatten layer\n",
    "    new_model.add(layers.Flatten())\n",
    "\n",
    "    # Create a new Dense layer with the same configuration\n",
    "    dense_layer = layers.Dense(10, activation='softmax', kernel_initializer=model.layers[-1].kernel_initializer)\n",
    "    \n",
    "    # Add the new Dense layer\n",
    "    new_model.add(dense_layer)\n",
    "\n",
    "    # Freeze the Dense layer\n",
    "    new_model.layers[-1].trainable = False\n",
    "\n",
    "    return new_model\n",
    "\n",
    "# Greedy layerwise training\n",
    "def greedy_layerwise_training(model, num_layers_to_add, train_base=True):\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_layers_to_add):\n",
    "        # Freeze existing layers\n",
    "        for layer in model.layers[:-2]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add a new Conv2D layer and re-add Flatten layer\n",
    "        model = add_conv_layer(model)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        if train_base:\n",
    "            # Train the model on the CIFAR-10 dataset for one epoch\n",
    "            history = model.fit(train_images, train_labels, epochs=25, validation_data=(test_images, test_labels), verbose=1)\n",
    "            accuracies.append(history.history['accuracy'][-1])\n",
    "\n",
    "        # Unfreeze the last added Conv2D layer for training\n",
    "        model.layers[-2].trainable = True\n",
    "\n",
    "        # Train only the newly added Conv2D layer and collect accuracy\n",
    "        history = model.fit(train_images, train_labels, epochs=25, validation_data=(test_images, test_labels), verbose=1)\n",
    "        accuracies.append(history.history['accuracy'][-1])\n",
    "\n",
    "        # Display model summary after training the added layer\n",
    "        print(f'\\nModel summary after training added layer {i + 1}:\\n')\n",
    "        model.summary()\n",
    "\n",
    "    return model, accuracies\n",
    "\n",
    "# Build the initial model\n",
    "initial_model = build_initial_model()\n",
    "\n",
    "# Train the base model\n",
    "initial_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "initial_model.fit(train_images, train_labels, epochs=25, validation_data=(test_images, test_labels), verbose=1)\n",
    "\n",
    "# Perform greedy layerwise training by adding convolutional layers\n",
    "final_model, layer_accuracies = greedy_layerwise_training(initial_model, num_layers_to_add=15, train_base=False)\n",
    "\n",
    "# Evaluate the final model\n",
    "test_loss, test_acc = final_model.evaluate(test_images, test_labels)\n",
    "print(f'\\nFinal model test accuracy: {test_acc}')\n",
    "\n",
    "# Save accuracies to a file\n",
    "with open('layer_accuracies.txt', 'w') as file:\n",
    "    for accuracy in layer_accuracies:\n",
    "        file.write(f'{accuracy}\\n')\n",
    "\n",
    "# Plot accuracy vs layer added\n",
    "plt.plot(range(1, len(layer_accuracies) + 1), layer_accuracies, marker='o')\n",
    "plt.title('Accuracy vs Layers Added')\n",
    "plt.xlabel('Number of Layers Added')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Prathav\\AppData\\Local\\Temp\\ipykernel_11304\\1438579424.py\", line 61, in <cell line: 61>\n      train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy_int, testy_int)\n    File \"C:\\Users\\Prathav\\AppData\\Local\\Temp\\ipykernel_11304\\1438579424.py\", line 35, in evaluate_model\n      model.fit(trainX, trainy, epochs=25, verbose=0)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32768,10] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_36616]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Evaluate the base model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Evaluate the base model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m train_acc, test_acc \u001b[39m=\u001b[39m evaluate_model(base_model, trainX, testX, trainy_int, testy_int)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m> layers=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, train=\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m, test=\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(base_model\u001b[39m.\u001b[39mlayers), train_acc, test_acc))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m scores[\u001b[39mlen\u001b[39m(base_model\u001b[39m.\u001b[39mlayers)] \u001b[39m=\u001b[39m (train_acc, test_acc)\n",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 17\u001b[0m line \u001b[0;36mevaluate_model\u001b[1;34m(model, trainX, testX, trainy, testy)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m trainy \u001b[39m=\u001b[39m trainy\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))  \u001b[39m# Flatten to a 1D array if needed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m testy \u001b[39m=\u001b[39m testy\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))  \u001b[39m# Flatten to a 1D array if needed\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, trainy, epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Ensure trainy and testy are in integer format\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m _, train_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(trainX, trainy, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Prathav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Prathav\\AppData\\Local\\Temp\\ipykernel_11304\\1438579424.py\", line 61, in <cell line: 61>\n      train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy_int, testy_int)\n    File \"C:\\Users\\Prathav\\AppData\\Local\\Temp\\ipykernel_11304\\1438579424.py\", line 35, in evaluate_model\n      model.fit(trainX, trainy, epochs=25, verbose=0)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32768,10] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_36616]"
     ]
    }
   ],
   "source": [
    "from keras.layers import Reshape, Dense, Conv2D, Flatten, concatenate\n",
    "trainy_int = np.argmax(trainy, axis=1)\n",
    "testy_int = np.argmax(testy, axis=1)\n",
    "# Function to add a new Conv2D layer to the model\n",
    "def add_conv_layer(model):\n",
    "    # Define the new Conv2D layer\n",
    "    new_conv_layer = Conv2D(32, (3, 3), activation='relu', padding='same')(model.layers[0].output)\n",
    "    \n",
    "    # Flatten the output of the existing model\n",
    "    flat = Flatten()(model.layers[-1].output)\n",
    "    \n",
    "    # Combine the new Conv2D layer with the existing layers\n",
    "    combined = concatenate([Reshape((32, 32, 1))(new_conv_layer), Reshape((32, 32, 1))(flat)])\n",
    "    \n",
    "    # Add a Dense layer\n",
    "    x_dense = Dense(128, activation='relu')(combined)\n",
    "    \n",
    "    # Output layer\n",
    "    x_output = Dense(10, activation='softmax')(x_dense)\n",
    "    \n",
    "    # Create a new model\n",
    "    new_model = Model(inputs=model.inputs, outputs=x_output)\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    "    # Assuming trainy and testy are in integer format (not one-hot encoded)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Ensure trainy and testy are in integer format\n",
    "    trainy = trainy.reshape((-1,))  # Flatten to a 1D array if needed\n",
    "    testy = testy.reshape((-1,))  # Flatten to a 1D array if needed\n",
    "    \n",
    "    model.fit(trainX, trainy, epochs=25, verbose=0)\n",
    "    \n",
    "    # Ensure trainy and testy are in integer format\n",
    "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    \n",
    "    return train_acc, test_acc\n",
    "\n",
    "\n",
    "def create_base_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Now you can call this function in your code\n",
    "base_model = create_base_model(trainX.shape[1:])\n",
    "\n",
    "# Initialize the base model\n",
    "base_model = create_base_model(trainX.shape[1:])\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "# Evaluate the base model\n",
    "# Evaluate the base model\n",
    "train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy_int, testy_int)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(base_model.layers), train_acc, test_acc))\n",
    "scores[len(base_model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add Conv2D layers and evaluate\n",
    "n_layers = 19\n",
    "for i in range(n_layers):\n",
    "    base_model = add_conv_layer(base_model)\n",
    "    train_acc, test_acc = evaluate_model(base_model, trainX, testX, trainy_int, testy_int)\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(base_model.layers), train_acc, test_acc))\n",
    "    scores[len(base_model.layers)] = (train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainX_gray: (50000, 32, 32, 3)\n",
      "Shape of trainy: (50000, 10)\n",
      "Number of samples in trainX_gray: 50000\n",
      "Number of samples in trainy: 50000\n"
     ]
    }
   ],
   "source": [
    "# Reshape to (None, 32, 32, 1) for grayscale images\n",
    "trainX_gray = trainX.reshape(-1, 32, 32, 3)\n",
    "testX_gray = testX.reshape(-1, 32, 32, 3)\n",
    "\n",
    "print(\"Shape of trainX_gray:\", trainX_gray.shape)\n",
    "print(\"Shape of trainy:\", trainy.shape)\n",
    "print(\"Number of samples in trainX_gray:\", trainX_gray.shape[0])\n",
    "print(\"Number of samples in trainy:\", trainy.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 34s - loss: 1.5584 - accuracy: 0.4482 - 34s/epoch - 44ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [10], output_shape = [16, 16, 1]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n   inputs=tf.Tensor(shape=(None, 10), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 19\u001b[0m line \u001b[0;36m<cell line: 76>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m n_layers \u001b[39m=\u001b[39m \u001b[39m19\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     model \u001b[39m=\u001b[39m add_conv_layer(model, trainX, trainy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     train_acc, test_acc \u001b[39m=\u001b[39m evaluate_model(model, trainX, testX, trainy, testy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 19\u001b[0m line \u001b[0;36madd_conv_layer\u001b[1;34m(model, trainX, trainy)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m x_output \u001b[39m=\u001b[39m Dense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(x_dense)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Combine the new Conv2D layer with the existing layers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m combined \u001b[39m=\u001b[39m concatenate([model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39moutput, Reshape((\u001b[39m16\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39m1\u001b[39;49m))(x_output)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m x \u001b[39m=\u001b[39m Flatten()(combined)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m Dense(\u001b[39m256\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m, kernel_initializer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhe_uniform\u001b[39m\u001b[39m'\u001b[39m)(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\layers\\reshaping\\reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    116\u001b[0m     output_shape[unknown] \u001b[39m=\u001b[39m original \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m known\n\u001b[0;32m    117\u001b[0m \u001b[39melif\u001b[39;00m original \u001b[39m!=\u001b[39m known:\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m output_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [10], output_shape = [16, 16, 1]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n   inputs=tf.Tensor(shape=(None, 10), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model using the Functional API\n",
    "def get_base_model(trainX, trainy):\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='tanh', kernel_initializer='he_uniform')(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=1, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def add_conv_layer(model, trainX, trainy):\n",
    "    # Add Conv2D layer\n",
    "    x_conv = Conv2D(32, (3, 3), activation='relu', padding='same')(model.layers[-4].output)\n",
    "    x_pool = MaxPooling2D((2, 2))(x_conv)\n",
    "    x_flat = Flatten()(x_pool)\n",
    "    x_dense = Dense(256, activation='tanh', kernel_initializer='he_uniform')(x_flat)\n",
    "    x_output = Dense(10, activation='softmax')(x_dense)\n",
    "\n",
    "    # Combine the new Conv2D layer with the existing layers\n",
    "    combined = concatenate([model.layers[0].output, Reshape((16, 16, 1))(x_output)])\n",
    "    x = Flatten()(combined)\n",
    "    x = Dense(256, activation='tanh', kernel_initializer='he_uniform')(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # Create the final model with the combined layers\n",
    "    final_model = Model(inputs=model.input, outputs=x)\n",
    "\n",
    "    # Compile the model before training\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    final_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    # Train the final model\n",
    "    final_model.fit(trainX, trainy, epochs=1, batch_size=64, verbose=2)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "\n",
    "\n",
    "# Add convolutional layers and evaluate the updated model\n",
    "n_layers = 19\n",
    "\n",
    "for i in range(n_layers):\n",
    "    model = add_conv_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Plotting the results\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathav\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 50s - loss: 1.6308 - accuracy: 0.4214 - 50s/epoch - 64ms/step\n",
      "> layers=5, train=0.522, test=0.505\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_2\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [32, 32, 3], output_shape = [32, 32, 1]\n\nCall arguments received by layer \"reshape_2\" (type Reshape):\n   inputs=tf.Tensor(shape=(None, 32, 32, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 15\u001b[0m line \u001b[0;36m<cell line: 81>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m n_layers \u001b[39m=\u001b[39m \u001b[39m19\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     model \u001b[39m=\u001b[39m add_conv_layer(model, trainX, trainy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     train_acc, test_acc \u001b[39m=\u001b[39m evaluate_model(model, trainX, testX, trainy, testy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;32mc:\\Users\\Prathav\\Downloads\\Untitled-1.ipynb Cell 15\u001b[0m line \u001b[0;36madd_conv_layer\u001b[1;34m(model, trainX, trainy)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m x_output \u001b[39m=\u001b[39m Dense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(x_dense)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Combine the new Conv2D layer with the existing layers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m reshaped_input \u001b[39m=\u001b[39m Reshape((\u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m1\u001b[39;49m))(model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49moutput)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m reshaped_output \u001b[39m=\u001b[39m Reshape((\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m1\u001b[39m))(x_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Prathav/Downloads/Untitled-1.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m combined \u001b[39m=\u001b[39m concatenate([reshaped_input, reshaped_output])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\layers\\reshaping\\reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    116\u001b[0m     output_shape[unknown] \u001b[39m=\u001b[39m original \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m known\n\u001b[0;32m    117\u001b[0m \u001b[39melif\u001b[39;00m original \u001b[39m!=\u001b[39m known:\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m output_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_2\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [32, 32, 3], output_shape = [32, 32, 1]\n\nCall arguments received by layer \"reshape_2\" (type Reshape):\n   inputs=tf.Tensor(shape=(None, 32, 32, 3), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, concatenate\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "# Define and fit the base model using the Functional API\n",
    "def get_base_model(trainX, trainy):\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='tanh', kernel_initializer='he_uniform')(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(trainX, trainy, epochs=1, batch_size=64, verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "from keras.layers import Reshape\n",
    "\n",
    "def add_conv_layer(model, trainX, trainy):\n",
    "    # Add Conv2D layer\n",
    "    x_conv = Conv2D(32, (3, 3), activation='relu', padding='same')(model.layers[0].output)\n",
    "    x_flat = Flatten()(x_conv)\n",
    "    x_dense = Dense(256, activation='tanh', kernel_initializer='he_uniform')(x_flat)\n",
    "    x_output = Dense(10, activation='softmax')(x_dense)\n",
    "\n",
    "    # Combine the new Conv2D layer with the existing layers\n",
    "    reshaped_input = Reshape((32, 32, 1))(model.layers[0].output)\n",
    "    reshaped_output = Reshape((32, 32, 1))(x_output)\n",
    "    combined = concatenate([reshaped_input, reshaped_output])\n",
    "    x = Flatten()(combined)\n",
    "    x = Dense(256, activation='tanh', kernel_initializer='he_uniform')(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # Create the final model with the combined layers\n",
    "    final_model = Model(inputs=model.input, outputs=x)\n",
    "\n",
    "    # Compile the model before training\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    final_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    # Train the final model\n",
    "    final_model.fit(trainX, trainy, epochs=1, batch_size=64, verbose=2)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "\n",
    "# Evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Add convolutional layers without MaxPooling2D and evaluate the updated model\n",
    "n_layers = 19\n",
    "\n",
    "for i in range(n_layers):\n",
    "    model = add_conv_layer(model, trainX, trainy)\n",
    "    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "    model.summary()\n",
    "    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "\n",
    "# Plotting the results\n",
    "plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
